title,source,url,json_content

How to download model_index.json from civitai?,Stack Overflow,N/A,"{""tags"": [""stable-diffusion"", ""image-generation""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78976666, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1726121331, ""creation_date"": 1726121331, ""body"": ""I&#39;m new to this area,\nI have installed stable diffusion and then downloaded a model from civitai.com\nbut when i execute, i&#39;m getting error .\nWhich is expected but I can&#39;t find a  any model i click download, it download the .safetensor only (for example https://civitai.com/models/352581/vixons-pony-styles)\n\n"", ""excerpt"": ""Which is expected but I can&#39;t find a model_index.json any model i click download, it download the .<span class=\""highlight\"">safetensor</span> only (for example https://civitai.com/models/352581/vixons-pony-styles)\nfrom diffusers import &hellip; "", ""title"": ""How to download model_index.json from civitai?""}"

Quantize and fine-tune Llama 3.1 8B for Ollama,Stack Overflow,N/A,"{""tags"": [""huggingface-transformers"", ""quantization"", ""fine-tuning"", ""ollama"", ""llama3.1""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78922383, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1724835324, ""creation_date"": 1724835324, ""body"": ""I want to fine-tune locally the Meta&#39;s Llama 3.1 8B Instruct model with custom data and then save it in a format compatible with Ollama for further inference. As I do everything locally and don&#39;t have unlimited resources (24 GB VRAM), I need to quantize the model with the requirement that the safetensors should be in either in F16, F32 or BF16 dtype otherwise Ollama cannot parse them.\nI implemented the following code to fetch the base model from HF hub and fine tune it:\n\nAmong the things I tried are:\n\nConverting the model to GGUF format with llama.cpp but it crashes with the error ;\nDifferent values for  but it always saved the model with U8 tensors;\nRemoving the BitsAndBytes config and just put a  (Same with 4bits) in the  call, but then I have OOM errors on the GPU;\nManually reformat the tensors before merging or before saving the model with the code below:\n\n\nIt seems Ollama doesn&#39;t handle BitsAndBytes quantization according to their maintainers. However I&#39;m curious about how that could matter if the tensors are saved with the correct F16/F32/BF16 dtype.\nSo apart from BitsAndBytes, do you recommend any other standard framework that could work with Ollama requirements ?\n"", ""excerpt"": ""I want to fine-tune locally the Meta&#39;s Llama 3.1 8B Instruct model with custom data and then save it in a format compatible with Ollama for further inference. As I do everything locally and don&#39;t have &hellip; "", ""title"": ""Quantize and fine-tune Llama 3.1 8B for Ollama""}"

How to quantize safetensors model and save it to GGUF with less then q8_0 quntization?,Stack Overflow,N/A,"{""tags"": [""large-language-model"", ""quantization""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78842047, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1723602721, ""creation_date"": 1723011038, ""body"": ""I&#39;m developing LLM agents using llama.cpp as inference engine. Sometimes I want to use models in safetensors format and there is a python script (https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) to convert.\nScript is awesome, but minimum number size is 8 bit (q8_0). Is there any other script or repo with other quantization formats?\n"", ""excerpt"": ""I&#39;m developing LLM agents using llama.cpp as inference engine. Sometimes I want to use models in safetensors format and there is a python script (https://github.com/ggerganov/llama.cpp/blob/master/con &hellip; "", ""title"": ""How to quantize safetensors model and save it to GGUF with less then q8_0 quntization?""}"

"Output images of my Stable Diffusion Fine tuning unchanged same? No difference after training. (Python, Diffusers)",Stack Overflow,N/A,"{""tags"": [""python"", ""stable-diffusion"", ""fine-tuning"", ""image-generation"", ""diffusers""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78866361, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1723553049, ""creation_date"": 1723553049, ""body"": ""i am currently trying to finetune sd1.5 with python and i dont get any errors but my output test images are exactly the same (pixelcomparisson online with img2go), also i compared the .safetensors from the unet cmd: &quot;fc /b file1 file2&quot; and no changes were detected.\nmy idea for the dummy data was to just use the same image several times with the same description as a proof of concept. I then use exactly the same prompt to generate test images, hoping to see some resembelence to my dummy input image\nThe test image is just a 2seconds paint drawing of me and the description is &quot;ProofOfConceptForDummyData&quot; for each, just like the prompt that i use after training for the test iamge.\nAlso additionally one epoche takes 32min on my 4070GPU with 12GB. i know that the batch size ist just at 1 but increasing results in an error that the gpu memory is not enough.\n\nonly use a small number of images (less than 10) with a high learning rate to get fast results as proof of concept\ni tried to increase the learning rate to a very high value to get changes in images very fast so i dont need to train all night, but the results were unchanged\ni tried running it for 10 epochs, also without any luck\nExpected to see just any changes as a Proof of concept so i can continue with actual training.\n(changes in the model &amp; changes within the images, which should be the result of of model changes)\ncurrently i don&#39;t care about overfitting since i first want to get just any results as Proof of concept\n"", ""excerpt"": ""i am currently trying to finetune sd1.5 with python and i dont get any errors but my output test images are exactly the same (pixelcomparisson online with img2go), also i compared the .safetensors fro &hellip; "", ""title"": ""Output images of my Stable Diffusion Fine tuning unchanged same? No difference after training. (Python, Diffusers)""}"

Failure running Apple MLX lora.py on 13B llms,Stack Overflow,N/A,"{""tags"": [""large-language-model""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 78860722, ""is_answered"": false, ""question_id"": 78248012, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1723452265, ""creation_date"": 1723452265, ""body"": ""The config files (in the same folder as the model) need to be in sync with the model files. I encountered something similar to your error when my model and config files went out of sync. Clearing out the cache (just delete the files in .\\mlx-lm folder) helped bring back the 2 in sync. Coming to the question on why they are working for smaller models, here is speculation from my side based on my observations.\n\nLarge models are split into multiple files unlike smaller models\nWhen you (say)quantise, in case of a smaller model the model simply gets overwritten. In case of larger models the older files are retained due to a naming convention. So you have old as well as new model weights.\nThese additional files are caught by the glob.glob (find all model*.safetensor command) in one of the wrapper code. This additional data causes unpredictable behaviour (e.g. loading extra weights thru&#39; the model.load_weights(list(weights.items()))\nKeep a track of the models in the ./mlx-lm folder very closely and you will be able to understand the exact issue.\n\n"", ""excerpt"": ""These additional files are caught by the glob.glob (find all model*.<span class=\""highlight\"">safetensor</span> command) in one of the wrapper code. &hellip; "", ""title"": ""Failure running Apple MLX lora.py on 13B llms""}"

Writing a formated binary file from a Pandas Dataframe,Stack Overflow,N/A,"{""tags"": [""python"", ""numpy"", ""pandas"", ""binaryfiles""], ""question_score"": 10, ""is_accepted"": false, ""answer_id"": 51106891, ""is_answered"": false, ""question_id"": 26348095, ""item_type"": ""answer"", ""score"": 10, ""last_activity_date"": 1722714735, ""creation_date"": 1530293797, ""body"": ""Pandas now offers a wide variety of formats:\n\nFor small to medium sized files, I prefer CSV, as properly-formatted CSV can store arbitrary string data, is human readable, and is as dirt-simple as any format can be while achieving the previous two goals.\nUnfortunately, for more complex problems, the choice is harder.\nIf I were on Amazon AWS, I would consider using parquet.  However, I do not have any experience with this format.\nI can no longer favor the pickle format.  Although the pickle format claims long term stability, it allows arbitrary code execution. And even if no code is actually stored in the pickle, ALL pickles execute code just to unpickle them, which is what lead the folks at huggingface to recommend first pickle-tools for scanning pickles and then safetensors as an alternative to pickles.  But for saving and loading pandas tables, safetensors is not an option, as they are designed for large multidimensional arrays of floating-point values (&quot;tensors&quot;) not for tabular data.\nI do not recommend using .  is best for quick file storage where you do not expect the file to be used on a different machine where the data may have a different endianness (big-/little-endian).\nI no longer favor the HDF5 format. It has serious risks for long-term archival since it is fairly complex. It has a 150 page specification, and only one 300,000 line C implementation.\nIf you have advice on stable, secure, binary formats for saving your own pandas data, please share them!  In the meantime, I think CSV for small data and zipped CSVs to save disk space or network bandwidth when needed may be the best way to go.  I personally try to avoid any pandas format that requires more than plain-text to store, if possible.\n"", ""excerpt"": ""Pandas now offers a wide variety of formats:\nFormat Type Data Description     Reader         Writer\ntext        CSV                  read_csv       to_csv\ntext        JSON                 read_json    &hellip; "", ""title"": ""Writing a formated binary file from a Pandas Dataframe""}"

load a model from checkpoint folder in pyTorch,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""checkpointing""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 78795465, ""is_answered"": false, ""question_id"": 78795407, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1721944722, ""creation_date"": 1721942193, ""body"": ""Have you tried the  flag? Setting to false will prevent the model from being saved as a .safetensors file.\nHere&#39;s a base example:-\n\nIn your case this would be\n\nEdit:- Who the HF model class inherits from determines the serializer used when its set to off.  uses pickle to deserialize. The  uses  so you should be able to load it in with the above method.\n\n\n\nHF Model Parent/Donor Class\n.save_pretrained(safe_serialization=?)\nserializer\nNative Pytorch Deserialisation Support\n\n\n\n\nPretrainedModel\noff\npickle\nY\n\n\nTFPretrainedModel\noff\nh5\nN\n\n\nFlaxPretrainedModel\noff\nmsgpack\nN\n\n\n\nsave_pretrained()\nPeft model donor\n"", ""excerpt"": ""Have you tried the safe_serialization flag? Setting to false will prevent the model from being saved as a .safetensors file.\nHere&#39;s a base example:-\nfrom transformers import AutoModel\n\nmodel = AutoMod &hellip; "", ""title"": ""load a model from checkpoint folder in pyTorch""}"

safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge,Stack Overflow,N/A,"{""tags"": [""python"", ""large-language-model"", ""peft""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 77630319, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1721813652, ""creation_date"": 1702102068, ""body"": ""When fine-tuning a Large Language model (Qwen), there is no .bin ending model files but only .safetensors. The following error occurs when importing this pre-trained model. The specific code and error information are as follows.\ncode:\n\nerror information:\n\nI would like to know how to solve this problem.\n"", ""excerpt"": ""When fine-tuning a Large Language model (Qwen), there is no .bin ending model files but only .safetensors. The following error occurs when importing this pre-trained model. The specific code and error &hellip; "", ""title"": ""safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge""}"

Convert safetensors model format(LLaVA model) into gguf format,Stack Overflow,N/A,"{""tags"": [""machine-learning"", ""artificial-intelligence"", ""large-language-model""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78763327, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1721671950, ""creation_date"": 1721292473, ""body"": ""I want to do LLaVA inference in ollama, so I need to convert it in gguf file format.\nMy model has the file format safetensors.(trained with lora)\nIt seems that ollama supports only llama, but not llava as shown here,\nhttps://github.com/ollama/ollama/blob/main/docs/import.md\nI followed the instruction of llama.cpp, and used the code convert_lora_to_gguf.py here,\nhttps://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py\nBut I get an error like,\n\nIf I write llama model in config.json of model file and run following code, then I got another error.\n\n\nIt seems that all codes and gguf package don&#39;t support llava, but llama only. I have to convert my own trained model into gguf. I cannot use gguf llava model from hugging face for inference.\nIs there a way to convert it?\n"", ""excerpt"": ""I want to do LLaVA inference in ollama, so I need to convert it in gguf file format.\nMy model has the file format safetensors.(trained with lora)\nIt seems that ollama supports only llama, but not llav &hellip; "", ""title"": ""Convert safetensors model format(LLaVA model) into gguf format""}"

why is my fine tuned model being saved as a safetensors file?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""classification"", ""huggingface-transformers"", ""image-classification""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78627955, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1718494830, ""creation_date"": 1718494830, ""body"": ""since im fairly new to this im trying to understand things slowly but this one thing has confused me a lot, is there a way to save the file to .h5 format or to convert it to tflite ? is the model good for mobile applications ? i havent figured it out ,and i really don&#39;t know what to do with this safetensor file that seems to only hold the model and no weights\ni have fine tuned a vit model on a dataset so i wanted to save it, which i did using \ni am using this particular model from huggingface\n\nnow im left with a safetensor format\n"", ""excerpt"": ""i havent figured it out ,and i really don&#39;t know what to do with this <span class=\""highlight\"">safetensor</span> file that seems to only hold the model and no weights\ni have fine tuned a vit model on a dataset so i wanted to save it, &hellip; ViTForImageClassification.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;, num_labels=num_classes)\n\nfeature_extractor = AutoProcessor.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)\n\nnow im left with a <span class=\""highlight\"">safetensor</span> &hellip; "", ""title"": ""why is my fine tuned model being saved as a safetensors file?""}"

"What are the standard, stable file formats used in Python for Data Science?",Stack Overflow,N/A,"{""tags"": [""python"", ""pandas"", ""numpy""], ""question_score"": 0, ""is_accepted"": true, ""answer_id"": 63583265, ""is_answered"": false, ""question_id"": 63583264, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1718301653, ""creation_date"": 1598373013, ""body"": ""Arbitrary Python data and code can be stored in the  pickle format.  While pickle files have security concerns because loading them can execute arbitrary code, if you can trust the source of a pickle file, it is a stable format.\nThe Python standard library&#39;s pickle page:\n\nThe pickle serialization format is guaranteed to be backwards compatible across Python releases provided a compatible pickle protocol is chosen and pickling and unpickling code deals with Python 2 to Python 3 type differences if your data is crossing that unique breaking change language boundary.\n\nSecurity concerns around the pickle format continue to grow.  While many deep learning models were originally distributed in the .pkl format, hugging face has championed distributing models in their safetensor format instead -- a move I guess the industry is likely to follow.\nMost python data can also be stored in the json format.  I haven&#39;t used this format much myself, but dawg recommends it.   Like the CSV and tab-delimited format I recommend for Pandas, the json format is a plain-text format that is very stable.\nNumpy arrays can be stored in the  or  numpy formats.  The npy format is a very simple format that stores a single numpy array.  I imagine it would be easy to read this format in any language.  The npz format allows the storing of multiple arrays in the same file.  Adapted from the docs,\n\nIf the integrity of the file being loaded is not guaranteed, be sure to use  to avoid arbitrary code execution.\nNote that  does NOT save in the stable  format. Although it looks more convenient because it simply calls a method on the data to be saved, this is not an option for saving data.\nPandas dataframes can be stored in a variety of formats.  As I wrote in a previous answer, Pandas offers a wide variety of formats.  For small datasets, I find plaintext file formats such as CSV and tab-delimited to work well for most purposes.  These formats are readable in a wide variety of languages and I have had no issues in working in a bilingual R and Python environment where both environments read from these files.\n\nWhen writing csv and tab files from pandas, I often use the  option to avoid saving the index, which loads as an oddly-named column by default.\n"", ""excerpt"": ""While many deep learning models were originally distributed in the .pkl format, hugging face has championed distributing models in their <span class=\""highlight\"">safetensor</span> format instead -- a move I guess the industry is likely &hellip; "", ""title"": ""What are the standard, stable file formats used in Python for Data Science?""}"

"&quot;_pickle.UnpicklingError: invalid load key, &#39;&#216;&#39;.&quot; error when using safetensors models in torchserve",Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""pickle"", ""large-language-model"", ""safe-tensors""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78617727, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1718280335, ""creation_date"": 1718280193, ""body"": ""I&#39;m trying to run an LLM model using torchserve, mainly following this guide and this quickstart. In particular I&#39;m trying to use the model mistralai/Mistral-7B-v0.3, but it doesn&#39;t matter since I&#39;ve tried a lot of different models from different providers.\nSome of the models I download can be used with a pytorch_model.bin.index.json, and I&#39;m able to run that just fine, but some others only come indexed with safetensors.model.index.json, and when using this, I get this stacktrace:\n\nIn particular, I think that the line that may be originating this issue is line 45 from , since I suspect the  method may need some further tuning to load a safetensors model or something similar. This error is found in multiple entries in a lot of forums, but none of the provided answers work for me (e.g. file is not corrupted or malformatted, and I have safetensors installed).\nIs there any tuning that can be done to avoid this error and properly load a model using safetensors?\nEdit:\nI would also like to mention that I found this question that mentions pickle files having issues when being compressed. However, the library is not mine and I don&#39;t think I have control whether they are compressed or not, so could you confirm if this may indeed be the same issue, and in that case, waht would the solution be?\n"", ""excerpt"": ""I&#39;m trying to run an LLM model using torchserve, mainly following this guide and this quickstart. In particular I&#39;m trying to use the model mistralai/Mistral-7B-v0.3, but it doesn&#39;t matter since I&#39;ve  &hellip; "", ""title"": ""&quot;_pickle.UnpicklingError: invalid load key, &#39;&#216;&#39;.&quot; error when using safetensors models in torchserve""}"

Adding multiple LoRa safetensors to my HuggingFace model in Python,Stack Overflow,N/A,"{""tags"": [""python"", ""model"", ""safe-tensors"", ""diffusers""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 77059057, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1717719397, ""creation_date"": 1694084969, ""body"": ""Suppose I use this script to load one fine-tuned model: (example taken from https://towardsdatascience.com/hugging-face-diffusers-can-correctly-load-lora-now-a332501342a3)\n\nThis adds one safetensors file. How can I load multiple safetensors? I tried the  argument when instantiating the , but it is unclear where I should put the safetensors folder I have. I have such an error:\n\nOSError: Could not found the necessary  weights in {&#39;vae/diffusion_pytorch_model.safetensors&#39;,\n&#39;text_encoder/pytorch_model.bin&#39;, &#39;safety_checker/model.safetensors&#39;, &#39;vae/diffusion_pytorch_model.bin&#39;,\n&#39;text_encoder/model.safetensors&#39;, &#39;unet/diffusion_pytorch_model.bin&#39;, &#39;safety_checker/pytorch_model.bin&#39;,\n&#39;unet/diffusion_pytorch_model.safetensors&#39;} (variant=None)\n\nI have also tried to load the weights one after the other, but results suggest that I&#39;m not keeping the previous loaded weights.\n"", ""excerpt"": ""Suppose I use this script to load one fine-tuned model: (example taken from https://towardsdatascience.com/hugging-face-diffusers-can-correctly-load-lora-now-a332501342a3)\nimport torch\nfrom diffusers  &hellip; "", ""title"": ""Adding multiple LoRa safetensors to my HuggingFace model in Python""}"

Model.save_pretrained is not saving .bin files,Stack Overflow,N/A,"{""tags"": [""huggingface-transformers"", ""huggingface"", ""fine-tuning"", ""huggingface-trainer""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 78548491, ""is_answered"": false, ""question_id"": 78266961, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1717072342, ""creation_date"": 1716974057, ""body"": "" is the binary of your model, model.bin is not used on new versions of transformers but safer format .safetensors\n"", ""excerpt"": ""model.safetensors is the binary of your model, model.bin is not used on new versions of transformers but safer format .safetensors\n"", ""title"": ""Model.save_pretrained is not saving .bin files""}"

How to download a model from huggingface?,Stack Overflow,N/A,"{""tags"": [""huggingface-transformers"", ""transformer-model""], ""question_score"": 74, ""is_accepted"": false, ""answer_id"": 78468453, ""is_answered"": false, ""question_id"": 67595500, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1715529083, ""creation_date"": 1715529083, ""body"": ""If you&#39;d like to download just a model weights from the huggingface, you can do that just with . Open weights page (for example, for the SDXL model, it is sd_xl_base_1.0.safetensors), copy the link from the download hyperlink, and then download weights in terminal using this command:&#160;\n\n"", ""excerpt"": ""If you&#39;d like to download just a model weights from the huggingface, you can do that just with wget. Open weights page (for example, for the SDXL model, it is sd_xl_base_1.0.safetensors), copy the lin &hellip; "", ""title"": ""How to download a model from huggingface?""}"

Problem with deploying finetuned gemma in AWS sagemaker as an endpoint,Stack Overflow,N/A,"{""tags"": [""amazon-web-services"", ""huggingface-transformers"", ""amazon-sagemaker"", ""large-language-model"", ""gemma""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78406505, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1714493380, ""creation_date"": 1714456758, ""body"": ""\nI have finetuned a gemma 7b LLM from HuggingFace using Lora and stored the model as a compressed  .tar.gz file.\nI have finetuned locally in sagemaker.\n\n\n\nthis is my .tar.gz  file structure of finetuned model :\n\nfinetuned_gemma/model-00004-of-00004.safetensors\nfinetuned_gemma/tokenizer_config.json\nfinetuned_gemma/model.safetensors.index.json\nfinetuned_gemma/config.json\nfinetuned_gemma/model-00002-of-00004.safetensors\nfinetuned_gemma/generation_config.json\nfinetuned_gemma/special_tokens_map.json\nfinetuned_gemma/model-00001-of-00004.safetensors\nfinetuned_gemma/tokenizer.json\nfinetuned_gemma/code/\nfinetuned_gemma/code/requirements.txt\nfinetuned_gemma/code/.ipynb_checkpoints/\nfinetuned_gemma/code/.ipynb_checkpoints/requirements-checkpoint.txt\nfinetuned_gemma/code/inference.py\nfinetuned_gemma/model-00003-of-00004.safetensors\n\nThe finetuned model is also stored in aws s3.\nHow do I now deploy the model as a sagemaker endpoint?\nBy the way I have used transformers version 4.38.0 as it is the minimum requirement for gemma tokenizer.\nI want to know how to deploy it along with the image Uri.\nPlease help\nI tried using sagemaker.huggingfacemodel and then tried deploying it but I&#39;m facing lots of difficulties.\n"", ""excerpt"": ""\nI have finetuned a gemma 7b LLM from HuggingFace using Lora and stored the model as a compressed  .tar.gz file.\nI have finetuned locally in sagemaker.\n\n\n\nthis is my .tar.gz  file structure of finetun &hellip; "", ""title"": ""Problem with deploying finetuned gemma in AWS sagemaker as an endpoint""}"

converting safe.tensor to pytorch bin files,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""nlp"", ""huggingface-transformers"", ""tensor""], ""question_score"": 1, ""is_accepted"": false, ""answer_id"": 78371698, ""is_answered"": false, ""question_id"": 78329328, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1713868936, ""creation_date"": 1713868936, ""body"": ""You will get a model in .safetensors format if you save the model using the following code:\nmodel.save_pretrained(&#39;folder/&#39;).\nAnd you will get a .bin format model if you save the model using the following code:\ntorch.save(model.state_dict(),&#39;folder/pytorch_model.bin&#39;.format(epoch)).\nAlternatively, you can use\nmodel.save_pretrained(output_dir, safe_serialization=False).\nWhen you downgrade the transformers library, it will automatically save the model in a .bin file.\n"", ""excerpt"": ""You will get a model in .safetensors format if you save the model using the following code:\nmodel.save_pretrained(&#39;folder/&#39;).\nAnd you will get a .bin format model if you save the model using the follo &hellip; "", ""title"": ""converting safe.tensor to pytorch bin files""}"

Loading a safetensor file in transformers,Stack Overflow,N/A,"{""tags"": [""python-3.x"", ""machine-learning"", ""nlp"", ""huggingface-transformers""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 4, ""is_answered"": true, ""question_id"": 76247802, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1712867436, ""creation_date"": 1684071978, ""body"": ""I have downloaded this model from huggingface. I am trying to load this model in transformers so I can do inferencing:\n\nBut i get the error saying that it expects a .bin or .h5 or .ckpt files but the above has only .safetensors or .pt files\nHow do i load the model?\n"", ""excerpt"": ""I have downloaded this model from huggingface. I am trying to load this model in transformers so I can do inferencing:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTok &hellip; "", ""title"": ""Loading a safetensor file in transformers""}"

How to converting GIT (ImageToText / image captioner ) model to ONNX format,Stack Overflow,N/A,"{""tags"": [""deep-learning"", ""artificial-intelligence"", ""huggingface-transformers"", ""onnx"", ""onnxruntime""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78223504, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1711437249, ""creation_date"": 1711436851, ""body"": ""I want to convert  GIT model  to .ONNX\nI&#39;ve found a tool called &#129303; Optimum but it does not support GIT model architecture , It says You need to pass a custom ONNX config so it can convert GIT from .safetensors to .onnx and it is not able to do it natively, And the problem is I couldn&#39;t find any onnx config for it yet or any alternative solution.\nthis is the command I used for conversion.\n\noptimum-cli error picture\n"", ""excerpt"": ""I want to convert  GIT model  to .ONNX\nI&#39;ve found a tool called &#129303; Optimum but it does not support GIT model architecture , It says You need to pass a custom ONNX config so it can convert GIT from .sa &hellip; "", ""title"": ""How to converting GIT (ImageToText / image captioner ) model to ONNX format""}"

How to convert model.safetensor to pytorch_model.bin?,Stack Overflow,N/A,"{""tags"": [""machine-learning"", ""pytorch"", ""bert-language-model"", ""sentence-transformers""], ""question_score"": 5, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 77708996, ""item_type"": ""question"", ""score"": 5, ""last_activity_date"": 1711010902, ""creation_date"": 1703364200, ""body"": ""I&#39;m fine tuning a pre-trained bert model and i have a weird problem:\nWhen i&#39;m fine tuning using the CPU, the code saves the model like this:\n\nWith the &quot;pytorch_model.bin&quot;. But when i use CUDA (that i have to), the model is saved like this:\n\nWhen i try to load this &quot;model.safetensors&quot; in the future, it raises an error &quot;pytorch_model.bin&quot; not found. I&#39;m using two differents venvs to test using the CPU and CUDA.\nHow to solve this? is some version problem?\nI&#39;m using sentence_transformers framework to fine-tune the model.\nHere&#39;s my training code:\n\nI did try the tests in two differentes venvs, and i&#39;m expecting the code to save a &quot;pytorch_model.bin&quot; not a &quot;model.safetensors&quot;.\nEDIT: i really don&#39;t know yet, but it seems that is the newer versions of transformers library that causes this problem. I saw that with hugging-face is possible to load the safetensors, but with Sentence-transformers (that i need to use) it&#39;s not.\n"", ""excerpt"": ""I&#39;m fine tuning a pre-trained bert model and i have a weird problem:\nWhen i&#39;m fine tuning using the CPU, the code saves the model like this:\n\nWith the &quot;pytorch_model.bin&quot;. But when i use CUDA (that i  &hellip; "", ""title"": ""How to convert model.safetensor to pytorch_model.bin?""}"

How to properly save the finetuned transformer model in safetensors without losing frozen parameters?,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""huggingface-transformers"", ""accelerate"", ""safe-tensors""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": false, ""question_id"": 78019134, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1710927530, ""creation_date"": 1708329977, ""body"": ""I have been trying to finetune a casual LM model by retraining its lm_head layer. I&#39;ve been training with Deepspeed Zero stage 3 (this part works fine). But I have problem saving my finetuned model and loading it back. I think the problem is that the  function automatically ignores the frozen parameters during saving. Here is my code and error messages:\n\nThe codes above will print a warning:\n\nAnd the saved model only takes 7MB disk space, however, I was expecting the saved model to be over 30GB. Looks like only the unfrozen part is saved to disk.\nTo verify my speculation, I tried to load it back with the following codes:\n\nBut it will result in an error of size mismatch.\n\nI alse tried following the instruction in the error message, but it&#39;s not working, either. The program printed a list of warnings and got stuck.\n\nThe warning messages clearly suggests loading the finetuned model is unsuccessful, and the reason why the program stuck looks like another issue. But all in all, my problem is how to save the full model instead of only the finetuned parameters? What&#39;s the proper convention to save/load finetuned huggingface models ?\nExpected Behaviors:\nThe  should save all the tensors in the huggingface transformer model, even if their  attribute is False.\n--- UPDATE ---\nI just located the cause of my problem. The state_dicts works fine during the entire saving process, but  function (in site-packages/transformers/pytorch_utils.py) will not get the correct pointer to the tensor. The output of this function will always be . \nIn practice, the  should be equal to the memory address of the tensor instead of 0. Thus, the source of this issue must lie in  function.\n"", ""excerpt"": ""I have been trying to finetune a casual LM model by retraining its lm_head layer. I&#39;ve been training with Deepspeed Zero stage 3 (this part works fine). But I have problem saving my finetuned model an &hellip; "", ""title"": ""How to properly save the finetuned transformer model in safetensors without losing frozen parameters?""}"

Load .ckpt files from CivitAi into pytorch for inference,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""dynamic-image-generation"", ""stable-diffusion""], ""question_score"": 3, ""is_accepted"": false, ""answer_id"": 78176122, ""is_answered"": false, ""question_id"": 75193901, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1710842535, ""creation_date"": 1710694348, ""body"": ""The requirement is that you install  and  ().\nLoading LoRA \nYou can use Hugging Face stable diffusion  method.\nFor this demo, I am downloading this LoRA weight: Styles for Pony Diffusion V6 XL.\n\nLoad the pipeline (be careful to load the same version as the one the LoRA was performed on! Here XL-1.0), then load the LoRA weighs:\n\n\n\n\nexample output\n\n\n\n\n\n\n\n\n\nLoading checkpoint \nIf you are looking to load checkpoint instead but have access to a  then you can&#39;t rely on . According to the documentation &quot;Load safetensors&quot;, you need to rely on  to initialize and load the pipeline:\nHere I am downloading the checkpoint from: Vapor - A Futuristic Retro Experience.\nThe following code was tested on Google collab:\n\nThen you can load the checkpoint with a single line:\n\n\n\n\nexample output\n\n\n\n\n\n\n\n\n"", ""excerpt"": ""type=Model&amp;format=<span class=\""highlight\"">SafeTensor</span>\n!mv &quot;396157?type=Model&quot; &quot;lora.safetensors&quot;\n\nLoad the pipeline (be careful to load the same version as the one the LoRA was performed on! &hellip; "", ""title"": ""Load .ckpt files from CivitAi into pytorch for inference""}"

No Safetensor Weights found - Hugging Face Docker Chat UI,Stack Overflow,N/A,"{""tags"": [""docker"", ""large-language-model"", ""huggingface""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78147258, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1710247744, ""creation_date"": 1710247674, ""body"": ""I wanted to build a HuggingFace space, using BioMistral 7B and the Docker Chat UI Template. Unfortunately it gives me following error:\n\n\n\nIt keeps running and fails, also when naming another model in .gguf with safetensor files. Anybody knows why?\n"", ""excerpt"": ""It keeps running and fails, also when naming another model in .gguf with <span class=\""highlight\"">safetensor</span> files. Anybody knows why? &hellip; "", ""title"": ""No Safetensor Weights found - Hugging Face Docker Chat UI""}"

Download file in an authenticated state with wget in bash,Stack Overflow,N/A,"{""tags"": [""bash"", ""authentication"", ""wget""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 78085400, ""is_answered"": false, ""question_id"": 77640702, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1709265322, ""creation_date"": 1709263621, ""body"": ""I don&#39;t know how to get it to work with wget, but that information given by https://education.civitai.com/civitais-guide-to-downloading-via-api/ is about using it with curl, not wget.\nFor example to get the model from https://civitai.com/models/308337/aetherverse-xl, you need to first the the download link (which is NOT the model page url!) by right clicking on the download button and then choosing &quot;copy link&quot;, from which you get https://civitai.com/api/download/models/346065.\nSo the full command line for getting the model is\n\ncurl -L -H &quot;Content-Type: application/json&quot; -H &quot;Authorization: Bearer\nyour-civitai-api-key&quot; https://civitai.com/api/download/models/346065\n--output=aetherverseXL_v10.safetensors\n\nOk, I&#39;ve just figure out how to do it using wget:\n\nwget &quot;civitai.com/api/download/models/346065?token=apikey&quot;\n--content-disposition\n\nNote the use of --content-disposition so that wget will save the model with the correct filename.\n"", ""excerpt"": ""I don&#39;t know how to get it to work with wget, but that information given by https://education.civitai.com/civitais-guide-to-downloading-via-api/ is about using it with curl, not wget.\nFor example to g &hellip; "", ""title"": ""Download file in an authenticated state with wget in bash""}"

Running Mixtral 8x7B on Colab Free Tier and MacOS?,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""huggingface-transformers"", ""huggingface""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78005566, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1708276968, ""creation_date"": 1708066236, ""body"": ""I am trying to run Mixtral 8x 7B on Colab Free Tier and Mac (M3 PRO 36 RAM).\nI ran the code from the model page on Hugging Face, but I encountered an out-of-memory error. I am wondering if it&#39;s impossible to run such a large model due to the limitations of the free version of Colab and my Mac environment, or if there is another solution.\nHere is the code I tried.\n\nAfter finishing downloading 19 safetensors, it starts loading checkpoint shards but stopped after 5%.\n"", ""excerpt"": ""I am trying to run Mixtral 8x 7B on Colab Free Tier and Mac (M3 PRO 36 RAM).\nI ran the code from the model page on Hugging Face, but I encountered an out-of-memory error. I am wondering if it&#39;s imposs &hellip; "", ""title"": ""Running Mixtral 8x7B on Colab Free Tier and MacOS?""}"

How to save and load safetensors files when fine-tuning a model for sequence classification with transformers?,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""huggingface-transformers"", ""safe-tensors""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77987301, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1707821222, ""creation_date"": 1707821222, ""body"": ""I am fine-tuning an AutoModelForSequenceClassification from the transformers library to classify free text into the five traits of personality. I am using different learning rates, attention and hidden dropouts, and I want to save the model with the best results.\nThe problem is that when I run this code on other computers, it works perfectly, and it saves all the files, including the safetensors files, in the output directory. However, when I run this code on my laptop, it saves all the files except for the safetensors files, which are missing. This prevents me from loading the model later and doing more experiments with it.\nDoes anyone know what could be the reason for this problem, and how to fix it? Any help would be appreciated. Thank you.\n"", ""excerpt"": ""I am fine-tuning an AutoModelForSequenceClassification from the transformers library to classify free text into the five traits of personality. I am using different learning rates, attention and hidde &hellip; "", ""title"": ""How to save and load safetensors files when fine-tuning a model for sequence classification with transformers?""}"

Whisper - How to convent model.safetensors to .h5 format or pytorch_model.bin?,Stack Overflow,N/A,"{""tags"": [""huggingface"", ""openai-whisper""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77745505, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1707233905, ""creation_date"": 1704191744, ""body"": ""After i train the model, I only have these file.\n[1]: (https://i.sstatic.net/7PpxF.png_\nHowever, i find that other people will get tf_model.h5 or pytorch_model.bin after train their model.\nI found that .safetensors is the latest format of that. But i need the ggml format. SO i want to convert the format to ggml with Whisper.cpp\nI have train my custom model of Whisper. from blow reference:\nhttps://huggingface.co/blog/fine-tune-whisper\nhttps://colab.research.google.com/drive/1qdS5ioA0pXoDPYPnbOlrOsXMxK0OR5yO\nand want to convert the format to ggml with Whisper.cpp\nhttps://github.com/ggerganov/whisper.cpp/tree/master/models\n"", ""excerpt"": ""After i train the model, I only have these file.\n[1]: (https://i.sstatic.net/7PpxF.png_\nHowever, i find that other people will get tf_model.h5 or pytorch_model.bin after train their model.\nI found tha &hellip; "", ""title"": ""Whisper - How to convent model.safetensors to .h5 format or pytorch_model.bin?""}"

How to convert safetensors model to onnx model?,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""onnx"", ""huggingface"", ""safe-tensors""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77855742, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1706392682, ""creation_date"": 1705855665, ""body"": ""I want to convert a  to ONNX, unfortunately I haven&#39;t found enough information about the procedure. The documentation of  package isn&#39;t enough and actually is not clear even how to get the original (pytorch in my case) model, since when I try something as\n\nthe  object has  but it seems hasn&#39;t any  method or something similar.\n"", ""excerpt"": ""I want to convert a model.safetensors to ONNX, unfortunately I haven&#39;t found enough information about the procedure. The documentation of safetensors package isn&#39;t enough and actually is not clear eve &hellip; "", ""title"": ""How to convert safetensors model to onnx model?""}"

llama.cpp conversion of finetuned HF ( huggingface ) fails for LLaMA2 - 7B model,Stack Overflow,N/A,"{""tags"": [""huggingface"", ""llama"", ""huggingface-trainer"", ""llamacpp""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77807764, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1705074720, ""creation_date"": 1705074720, ""body"": ""i use the simple https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py with some custom data and llama-2-7b-hf as the base model. Post training , it invokes trainer.save_model and the output dir has the following contents\n-rw-rw-r-- 1 ubuntu ubuntu 5100 Jan 12 14:04 README.md\n-rw-rw-r-- 1 ubuntu ubuntu 134235048 Jan 12 14:04 adapter_model.safetensors\n-rw-rw-r-- 1 ubuntu ubuntu 576 Jan 12 14:04 adapter_config.json\n-rw-rw-r-- 1 ubuntu ubuntu 1092 Jan 12 14:04 tokenizer_config.json\n-rw-rw-r-- 1 ubuntu ubuntu 552 Jan 12 14:04 special_tokens_map.json\n-rw-rw-r-- 1 ubuntu ubuntu 1842948 Jan 12 14:04 tokenizer.json\n-rw-rw-r-- 1 ubuntu ubuntu 4219 Jan 12 14:04 training_args.bin\n-rw-rw-r-- 1 ubuntu ubuntu 4827151012 Jan 12 14:04 adapter_model.bin\nas you can see it has no model.safetensors as required by convert.py .. i tried a bunch of other options to save the model ( trainer.model.save_pretrained , for example ) but the file was always adapter_model.safetensors.\ni tried convert-hf-to-gguf.py as well and it too complains about model.safetensors ( and that too after suppressing the error which complains about causalLLAMA architecture not supported )\nIs there any other convert script that handles such adapter safetensors ( i guess all models finetuned via peft will definitely be called adapter**_ ) ? when i went through the code i also noticed that the MODEL_ARCH only accomodates &quot;LLAMA&quot; and not &quot;LLAMA2&quot; ..is that why it also fails to find param names from adapter_safetensors in the MODEL_ARCH tmap methods ?\n"", ""excerpt"": ""i use the simple https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py with some custom data and llama-2-7b-hf as the base model. Post training , it invokes trainer.save_model and the o &hellip; "", ""title"": ""llama.cpp conversion of finetuned HF ( huggingface ) fails for LLaMA2 - 7B model""}"

OSError: Error no file named model.safetensors found in directory,Stack Overflow,N/A,"{""tags"": [""huggingface-transformers"", ""large-language-model"", ""huggingface"", ""llama""], ""question_score"": -1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77745229, ""item_type"": ""question"", ""score"": -1, ""last_activity_date"": 1704780319, ""creation_date"": 1704188397, ""body"": ""I am trying to load a LLAMA2 model saved in Hugging Face safe tensors format. The model is saved in two parts model-part1.safetensors and model-part2.safetensors.\nI am using LlamaForCausalLM.from_pretrained() Hugging Face API to load the model.\nWhen I pass the folder containing the model files, I am the following error\n\nThe code that I am using is:\n\nHow can I load a sharded model using Hugging Face API?\nCan anyone please help on this?\n"", ""excerpt"": ""I am trying to load a LLAMA2 model saved in Hugging Face safe tensors format. The model is saved in two parts model-part1.safetensors and model-part2.safetensors.\nI am using LlamaForCausalLM.from_pret &hellip; "", ""title"": ""OSError: Error no file named model.safetensors found in directory""}"

Loading a safetensors format model using Hugging Face Transformers,Stack Overflow,N/A,"{""tags"": [""python"", ""huggingface-transformers"", ""huggingface"", ""safe-tensors""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 76459041, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1704282263, ""creation_date"": 1686591352, ""body"": ""I try to load the &#39;notstoic/pygmalion-13b-4bit-128g&#39; model using Hugging Face&#39;s Transformers library. I am encountering an issue when trying to load the model, which is saved in the new safetensors format.\nHere&#39;s the code I&#39;m using:\n\nHowever, this code results in the following error:\n\nI&#39;m confused by this error because I&#39;ve set use_safetensors=True, as the model is stored in safetensors format. In the model directory (path/to/model), I have the following files:\n\n4bit-128g.safetensors\nconfig.json\ngeneration_config.json\npytorch_model.bin.index.json\nspecial_tokens_map.json\ntokenizer.json\ntokenizer.model\ntokenizer_config.json\n\nIt seems like the from_pretrained() function is not recognizing the safetensors format and instead is looking for the typical file formats (pytorch_model.bin, tf_model.h5, etc).\nI would appreciate if anyone could provide guidance on why this is happening and how I can successfully load this model.\n"", ""excerpt"": ""I try to load the &#39;notstoic/pygmalion-13b-4bit-128g&#39; model using Hugging Face&#39;s Transformers library. I am encountering an issue when trying to load the model, which is saved in the new safetensors fo &hellip; "", ""title"": ""Loading a safetensors format model using Hugging Face Transformers""}"

Can I convert safetensors to TensorFlow Lite model?,Stack Overflow,N/A,"{""tags"": [""tensorflow""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77676141, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1702850474, ""creation_date"": 1702849753, ""body"": ""I&#39;m new to AI and Python.\nI have an AI model with the anime.safetensors extension. I want to import this to Firebase ML but is not a supported file type. I am getting error. How can I import an AI file with .safetensor extension into Firebase ML?\nI tried this code but It’s not working.\n\n"", ""excerpt"": ""How can I import an AI file with .<span class=\""highlight\"">safetensor</span> extension into Firebase ML?\nI tried this code but It’s not working. &hellip; "", ""title"": ""Can I convert safetensors to TensorFlow Lite model?""}"

Extract trigger words from safetensors file,Stack Overflow,N/A,"{""tags"": [""python"", ""safe-tensors""], ""question_score"": 1, ""is_accepted"": true, ""answer_id"": 77597522, ""is_answered"": false, ""question_id"": 77597489, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1701670764, ""creation_date"": 1701670764, ""body"": ""Adapting from https://huggingface.co/docs/safetensors/metadata_parsing#python:\n\n"", ""excerpt"": ""Adapting from https://huggingface.co/docs/safetensors/metadata_parsing#python:\nimport json\nimport struct\n\nwith open(&quot;lora.safetensors&quot;, &quot;rb&quot;) as f:\n    length_of_header = struct.unpack(&#39;&lt;Q&#39;, f.read(8) &hellip; "", ""title"": ""Extract trigger words from safetensors file""}"

Safetensor model serving using pytorch torch serve container not starting,Stack Overflow,N/A,"{""tags"": [""docker"", ""pytorch"", ""torchserve""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77553127, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1701026194, ""creation_date"": 1701026194, ""body"": ""I cannot start my saved safetensor model in my pytorch/torchserve container.\nI can start my container using:\n\nI can enter the container, and the model directory is mounted.\n\nconfig.json  generation_config.json  model.safetensors  special_tokens_map.json  tokenizer.json  tokenizer_config.json\nNo model seems to be available but torchserve is running\n\nTorchServe is already running, please use torchserve --stop to stop TorchServe.\n\n{\n&quot;status&quot;: &quot;Healthy&quot;\n}\n\n2023-11-26T18:42:20,304 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n2023-11-26T18:42:20,304 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n2023-11-26T18:42:20,325 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n2023-11-26T18:42:20,325 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n2023-11-26T18:42:20,584 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml\n2023-11-26T18:42:20,584 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml\n2023-11-26T18:42:20,965 [INFO ] main org.pytorch.serve.ModelServer -\nTorchserve version: 0.8.2\nTS Home: /home/venv/lib/python3.9/site-packages\nCurrent directory: /home/model-server\nTemp directory: /home/model-server/tmp\nMetrics config path: /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml\nNumber of GPUs: 0\nNumber of CPUs: 12\nMax heap size: 2964 M\nPython executable: /home/venv/bin/python\nConfig file: /home/model-server/config.properties\nInference address: http://0.0.0.0:8080\nManagement address: http://0.0.0.0:8081\nMetrics address: http://0.0.0.0:8082\nModel Store: /home/model-server/model-store\nInitial Models: N/A\nLog dir: /home/model-server/logs\nMetrics dir: /home/model-server/logs\nNetty threads: 32\nNetty client threads: 0\nDefault workers per model: 12\nBlacklist Regex: N/A\nMaximum Response Size: 6553500\nMaximum Request Size: 6553500\nAny help is appreciated.\nThank you\n"", ""excerpt"": ""I cannot start my saved <span class=\""highlight\"">safetensor</span> model in my pytorch/torchserve container. &hellip; "", ""title"": ""Safetensor model serving using pytorch torch serve container not starting""}"

Converting huggingface model to safetensors failing with larger model,Stack Overflow,N/A,"{""tags"": [""huggingface""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77531431, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1700668682, ""creation_date"": 1700668682, ""body"": ""I use this code to merge a fine-tuned model on a machine with a A100-40GB:\n\nBut when I use the 13b instead of the 7b llama-2 model I am getting the following error:\n\nNotImplementedError: Cannot copy out of meta tensor; no data!\n\nI can set safe_serialization=False for it to work and merge, but then its not in safetensor format. I think its a offloading issue but is there any way to get around this?\n"", ""excerpt"": ""I can set safe_serialization=False for it to work and merge, but then its not in <span class=\""highlight\"">safetensor</span> format. I think its a offloading issue but is there any way to get around this? &hellip; "", ""title"": ""Converting huggingface model to safetensors failing with larger model""}"

Why does Hugging Face&#39;s push_to_hub convert saved models to .bin instead of using safetensor mode?,Stack Overflow,N/A,"{""tags"": [""machine-learning"", ""pytorch"", ""huggingface-transformers"", ""huggingface"", ""llama""], ""question_score"": 3, ""is_accepted"": false, ""answer_id"": 77062335, ""is_answered"": false, ""question_id"": 77044747, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1694114760, ""creation_date"": 1694114760, ""body"": ""For any one encountering the same problem, convert your model using the below link after making a push to hugging face.\nhttps://huggingface.co/spaces/safetensors/convert\n"", ""excerpt"": ""For any one encountering the same problem, convert your model using the below link after making a push to hugging face.\nhttps://huggingface.co/spaces/safetensors/convert\n"", ""title"": ""Why does Hugging Face&#39;s push_to_hub convert saved models to .bin instead of using safetensor mode?""}"

failed to download Transformers,Stack Overflow,N/A,"{""tags"": [""nlp"", ""model"", ""bert-language-model""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77037524, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1693828940, ""creation_date"": 1693828940, ""body"": ""To update pip, run:pip install--upgrade pipand then retry package installation.If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain. [end of output]note: This error originates from a subprocess, and is likely not a problem with pip.ERROR: Failed building wheel for safetensorsSuccessfully built pyyamlFailed to build tokenizers safetensors ERROR: Could not build wheels for tokenizers, safetensors, which is required to install pyproject.toml-based projects\nPadahal versi PIP saya adalah 23.2.1\n"", ""excerpt"": ""To update pip, run:pip install--upgrade pipand then retry package installation.If you did intend to build this package from source, try installing a Rust compiler from your system package manager and  &hellip; "", ""title"": ""failed to download Transformers""}"

"ConnectionError: HTTPSConnectionPool(host=&#39;cdn-lfs.huggingface.co&#39;, port=443): Read timed out",Stack Overflow,N/A,"{""tags"": [""nlp"", ""huggingface-transformers"", ""huggingface"", ""huggingface-tokenizers"", ""huggingface-hub""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77035723, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1693811316, ""creation_date"": 1693811316, ""body"": ""While running the code, model stops loading in between and gives the following error.\n\nDownloading shards: 0%\n0/2 [07:07&lt;?, ?it/s]\nDownloading (…)of-00002.safetensors: 29%\n2.86G/9.98G [07:03&lt;17:15, 6.87MB/s]\nTimeoutError                              Traceback (most recent call last)\nFile ~/.local/lib/python3.10/site-packages/urllib3/response.py:438, in HTTPResponse._error_catcher(self)\n437 try:\n--&gt; 438     yield\n440 except SocketTimeout:\n441     # FIXME: Ideally we&#39;d like to include the url in the ReadTimeoutError but\n442     # there is yet no clean way to get at it from this context.\nFile ~/.local/lib/python3.10/site-packages/requests/models.py:822, in Response.iter_content..generate()\n820     raise ContentDecodingError(e)\n821 except ReadTimeoutError as e:\n--&gt; 822     raise ConnectionError(e)\n823 except SSLError as e:\n824     raise RequestsSSLError(e)\nConnectionError: HTTPSConnectionPool(host=&#39;cdn-lfs.huggingface.co&#39;, port=443): Read timed out.\nCan Someone Help me to load the model?\n"", ""excerpt"": ""While running the code, model stops loading in between and gives the following error.\n    from getpass import getpass\n    import os\n    HUGGINGFACE_API_TOKEN = getpass()\n    os.environ[HUGGINGFACE_API &hellip; "", ""title"": ""ConnectionError: HTTPSConnectionPool(host=&#39;cdn-lfs.huggingface.co&#39;, port=443): Read timed out""}"

Develop a txt2img API for custom frontend,Stack Overflow,N/A,"{""tags"": [""python"", ""stable-diffusion"", ""gradio""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77015478, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1693481872, ""creation_date"": 1693481872, ""body"": ""I have been using Auto1111&#39;s webui for getting started with diffusion models and LoRas. I want to create my own text-to-image website where users can play around with different models and LoRas that I upload on the server, like a playground of sort.\nHence, I plan on building my own custom frontend ccomponents in React for the same instead of using gradio elements(since it does not give much UI customisation control), which is the case for Auto1111&#39;s repo. I have been trying to disintegrate the backend for the txt2img generation and it is very tricky sincy all the files are contained inside a single modules directory which contains the backend and frontend logic.\nI tried understanding the entire pipeline of Auto1111&#39;s application and figured that the main files I require for the txt2img generation are:\n\nmodelloader.py -&gt; Basically takes the .ckpt and .safetensor files from the models&gt;StableDiffusion directory and loads inside the gradio list component\nprocessing.py\ntxt2img.py\nsd_models\nand a few more.....\n\nIs there a simpler and less complex way to create an API for the difffusion models? Otherwise what approach should I use for making my custom frontend with Auto1111&#39;s complete backend logic for the text-to-image generation models.\n"", ""excerpt"": ""I tried understanding the entire pipeline of Auto1111&#39;s application and figured that the main files I require for the txt2img generation are:\n\nmodelloader.py -&gt; Basically takes the .ckpt and .<span class=\""highlight\"">safetensor</span> &hellip; "", ""title"": ""Develop a txt2img API for custom frontend""}"

Lycoris models fail in Stable Diffusion with error &quot;size of tensor a (768) must match size of tensor b (1024),Stack Overflow,N/A,"{""tags"": [""runtime-error""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 76502635, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1690326870, ""creation_date"": 1687124527, ""body"": ""I am using AUTOMATIC111/stable-diffusion-webui on my PC (Window 10, Intel Cor i9, 64GB RAM, Nvidia RTX 3060).  I have successfully installed the LoRA and LyCORIS extensions in Stable Diffusion, and both have tabs in Extra Networks. All of the LoRA models that I have downloaded from civiai.com work, but only one of the 10 LyCORIS models work.  For example, colouredglazecd-000006.safetensors works.  I got this image using img2img with a picture of a pink rose and the prompt: lyco:colouredglazecd-000006:1.0rose. glazed rose\nHowever, when I try to use  lyco:CenturyBotanicalIllustration:1.0rose or any of the other 9 Lycoris models I downloaded, I get the error message:  RuntimeError: The size of tensor a (768) must match the size of tensor b (1024) at non-singleton dimension 1.  Below is a sample of the beginning of the webui-user.bat output and a sample from the end of the code.  Changing the width or height of the output image doesn&#39;t help.  Any help with this will be greatly appreciated\nZaffer\nCode from beginning of webui-user.bat\n\n"", ""excerpt"": ""I am using AUTOMATIC111/stable-diffusion-webui on my PC (Window 10, Intel Cor i9, 64GB RAM, Nvidia RTX 3060).  I have successfully installed the LoRA and LyCORIS extensions in Stable Diffusion, and bo &hellip; "", ""title"": ""Lycoris models fail in Stable Diffusion with error &quot;size of tensor a (768) must match size of tensor b (1024)""}"

Big difference in the sizes of Llama 2 model files on huggingface hub depending on the format,Stack Overflow,N/A,"{""tags"": [""huggingface""], ""question_score"": 6, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 76722173, ""item_type"": ""question"", ""score"": 6, ""last_activity_date"": 1689876335, ""creation_date"": 1689775450, ""body"": ""The Llama2 7B model on huggingface (meta-llama/Llama-2-7b) has a pytorch .pth file consolidated.00.pth that is ~13.5GB in size. The hugging face transformers compatible model meta-llama/Llama-2-7b-hf has three pytorch model files that are together ~27GB in size and two safetensors file that are together around 13.5Gb.\nCould someone please explain the reason for the big difference in file sizes?\nI could not find an explanation in the huggingface model cards or in their blog Llama 2 is here - get it on Hugging Face.\nUpdate: When the models are downloaded to huggingface cache, I noticed that only the safetensors are downloaded and not the Pytorch binary model files. This avoids downloading both the safetensors and pytorch model files.\n"", ""excerpt"": ""The Llama2 7B model on huggingface (meta-llama/Llama-2-7b) has a pytorch .pth file consolidated.00.pth that is ~13.5GB in size. The hugging face transformers compatible model meta-llama/Llama-2-7b-hf  &hellip; "", ""title"": ""Big difference in the sizes of Llama 2 model files on huggingface hub depending on the format""}"

How to convert .safetensors or .ckpt Files and Using in FlaxStableDiffusionImg2ImgPipeline?,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""stable-diffusion"", ""flax""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 76544877, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1687588917, ""creation_date"": 1687588917, ""body"": ""I am trying to convert a .safetensors model to a diffusers model using the Python script found at https://github.com/huggingface/diffusers/blob/main/scripts/convert_original_stable_diffusion_to_diffusers.py. The command I tried is . After the conversion, I intend to use the diffusers model within the FlaxStableDiffusionImg2ImgPipeline.\nHowever, I encountered an error when running the script I provided below (full code):\n\nMy question is how I can fix these issues and properly convert the .safetensors model to a diffusers model, so I can use it with FlaxStableDiffusionImg2ImgPipeline without encountering any errors?\nFull Code:\n\nError Stack:\n\n"", ""excerpt"": ""I am trying to convert a .safetensors model to a diffusers model using the Python script found at https://github.com/huggingface/diffusers/blob/main/scripts/convert_original_stable_diffusion_to_diffus &hellip; "", ""title"": ""How to convert .safetensors or .ckpt Files and Using in FlaxStableDiffusionImg2ImgPipeline?""}"

How to use civitAI checkpoint with ControlNet,Stack Overflow,N/A,"{""tags"": [""python"", ""stable-diffusion""], ""question_score"": 1, ""is_accepted"": false, ""answer_id"": 76454628, ""is_answered"": false, ""question_id"": 76450546, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1686556939, ""creation_date"": 1686556939, ""body"": ""Thanks to @off99555\nTo summarize my steps\n\nDownload safetensors\nDownload python file\n\n\n\n\n\n\nThen I can use  model.\n"", ""excerpt"": ""Thanks to @off99555\nTo summarize my steps\n\nDownload safetensors\nDownload python file\nmkdir converted\npython convert_original_stable_diffusion_to_diffusers.py --checkpoint_path majicmixRealistic_v5.saf &hellip; "", ""title"": ""How to use civitAI checkpoint with ControlNet""}"

safetensors issue with Huggingface embeddings,Stack Overflow,N/A,"{""tags"": [""python"", ""jupyter"", ""huggingface-transformers"", ""huggingface"", ""sentence-transformers""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 76452425, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1686515767, ""creation_date"": 1686515767, ""body"": ""Running into the following:\nlocal Win10 Anaconda Setup in jupyter notebook.\n\nInstalled safetensors\nTrying to load Huggingface embeddings:\n\nGetting this error constantly. Anyone has an idea?\n\nAlready tried to install rust compiler, installed from wheel, all with same problem.\nTried the same notebook in google colab and it worked without any issues.\n"", ""excerpt"": ""Running into the following:\nlocal Win10 Anaconda Setup in jupyter notebook.\n!pip install safetensors Requirement already satisfied: safetensors in ...\\anaconda3\\envs\\myenv\\lib\\site-packages (0.3.1)\nIn &hellip; "", ""title"": ""safetensors issue with Huggingface embeddings""}"

How to apply LORAs like in SD WebUI to DreamShaper using python,Stack Overflow,N/A,"{""tags"": [""python"", ""machine-learning"", ""pytorch"", ""huggingface"", ""stable-diffusion""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 76397416, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1686290720, ""creation_date"": 1685816200, ""body"": ""I have been using stable diffusion WebUI to try out different models and LORAs for my application. I&#39;m trying to now do the equivalent of what I&#39;ve been doing in WebUI but in Python. I have a safetensors file for DreamShaper v5 beta-2 with VAE baked in. I am using two other safetensors files for two LORAs I downloaded from civitai.com.\nHere is the code that I tried:\n\nI tried using the ckpt file for dreamshaper instead of the safetensors file to evade the following error:\n\nBut then I got this error:\n\nMy source for this code was from the huggingface docs. I tried looking in the SD WebUI Github WIKI but didn&#39;t find anything.\nAgain, all I&#39;m trying to do is apply the LORAs to DreamShaper using Python just as I would have in SD WebUI. If it helps, here is where I got the LORAs from:\nhttps://civitai.com/models/19130/flat-illustration\nhttps://civitai.com/models/42190/improve-backgrounds\n"", ""excerpt"": ""I have been using stable diffusion WebUI to try out different models and LORAs for my application. I&#39;m trying to now do the equivalent of what I&#39;ve been doing in WebUI but in Python. I have a safetens &hellip; "", ""title"": ""How to apply LORAs like in SD WebUI to DreamShaper using python""}"

Getting ModuleNotFoundError in DreamBooth running on Google Colab,Stack Overflow,N/A,"{""tags"": [""python"", ""deep-learning"", ""google-colaboratory"", ""stable-diffusion""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 74589492, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1669764168, ""creation_date"": 1669547292, ""body"": ""I am using this google colab notebook to run dreambooth using automatic1111 webgui. Everything ran smoothly few days back. But for last few days, I am getting this error. I trained the model, then when I tried to test it to run the automatic1111 webgui, I got this error. I tried replacing these files like  etc. but I am unable to solve this error. Why this error arise? I dont have a fancy gpu (i have nvidia 940 MX 2 Gb) so i use google colab to generate art using dreambooth trained on my own images. Please help me out.\n\nTraceback (most recent call last):   File\n&quot;/content/gdrive/MyDrive/sd/stable-diffusion-webui/webui.py&quot;, line 13,\nin      from modules import shared, devices, sd_samplers,\nupscaler, extensions, localization, ui_tempdir   File\n&quot;/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py&quot;,\nline 8, in      import k_diffusion.sampling   File\n&quot;/content/gdrive/MyDrive/sd/stable-diffusion/src/k-diffusion/k_diffusion/init.py&quot;,\nline 1, in      from . import augmentation, config,\nevaluation, external, gns, layers, models, sampling, utils   File\n&quot;/content/gdrive/MyDrive/sd/stable-diffusion/src/k-diffusion/k_diffusion/external.py&quot;,\nline 6, in      from . import sampling, utils   File\n&quot;/content/gdrive/MyDrive/sd/stable-diffusion/src/k-diffusion/k_diffusion/sampling.py&quot;,\nline 7, in      import torchsde ModuleNotFoundError: No module\nnamed &#39;torchsde&#39;\n\nI deleted all the files, trained a new model. Now I am getting this error.\n\nTraceback (most recent call last):   File\n&quot;/content/gdrive/MyDrive/sd/stable-diffusion-webui/webui.py&quot;, line 13,\nin \nfrom modules import shared, devices, sd_samplers, upscaler, extensions, localization, ui_tempdir   File\n&quot;/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_samplers.py&quot;,\nline 11, in \nfrom modules import prompt_parser, devices, processing, images   File\n&quot;/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/processing.py&quot;,\nline 15, in \nimport modules.sd_hijack   File &quot;/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_hijack.py&quot;,\nline 10, in \nimport modules.textual_inversion.textual_inversion   File &quot;/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/textual_inversion/textual_inversion.py&quot;,\nline 13, in \nfrom modules import shared, devices, sd_hijack, processing, sd_models, images, sd_samplers   File\n&quot;/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/sd_models.py&quot;,\nline 8, in \nimport safetensors.torch ModuleNotFoundError: No module named &#39;safetensors&#39;\n\n"", ""excerpt"": ""I am using this google colab notebook to run dreambooth using automatic1111 webgui. Everything ran smoothly few days back. But for last few days, I am getting this error. I trained the model, then whe &hellip; "", ""title"": ""Getting ModuleNotFoundError in DreamBooth running on Google Colab""}"

Converting a PyTorch ONNX model to TensorRT engine - Jetson Orin Nano,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""onnx"", ""tensorrt""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 78787534, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1722206806, ""creation_date"": 1721813962, ""body"": ""I&#39;m trying to convert a  Vision Transformer model from the UNICOM repository on a Jetson Orin Nano. The model&#39;s Vision Transformer class and source code is here.\nI use the following code to convert the model to ONNX:\n\nI then use the following command line to convert the ONNX model to a TensorRT engine:\n\nThis results in the following error:\n\nThe problem seems to arise from the  class here and it doesn&#39;t seem as if the model is using any extraordinary methods and layers that aren&#39;t convertible by TensorRT. Here&#39;s the class&#39;s source code:\n\nWhat should I do to make the model convertible to TensorRT?\n\nUPDATE:\nRunning the code using the  instead of  gives this error:\n\n"", ""excerpt"": ""Compatible: Disabled\nONNX Native InstanceNorm: Disabled\nTensorRT runtime: full\nLean DLL Path:\nTempfile Controls: { in_memory: allow, temporary: allow }\nExclude Lean Runtime: Disabled\nSparsity: Disabled\n<span class=\""highlight\"">Safe</span> &hellip; Compatible: Disabled\nONNX Native InstanceNorm: Disabled\nTensorRT runtime: full\nLean DLL Path:\nTempfile Controls: { in_memory: allow, temporary: allow }\nExclude Lean Runtime: Disabled\nSparsity: Disabled\n<span class=\""highlight\"">Safe</span> &hellip; "", ""title"": ""Converting a PyTorch ONNX model to TensorRT engine - Jetson Orin Nano""}"

PyTorch C++ extension: efficiently accumulate results from threads in tensor,Stack Overflow,N/A,"{""tags"": [""c++"", ""multithreading"", ""pytorch"", ""openmp""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 78780662, ""is_answered"": false, ""question_id"": 78780576, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1721680078, ""creation_date"": 1721680078, ""body"": ""Allocations generally do not scale well. The thing is you perform a  for each iteration and it needs to allocate not only a  object but also a control block (the reference-counting data structure). This means at least 2 allocations per iteration.\nI am not sure you actually need a shared pointer. A simple  is certainly enough here since the pointer is not actually shared. In fact, I do not think you actually need dynamic allocation at all. Indeed,  can certainly be allocated on the stack once in a parallel section and reused between multiple iteration. If you need to call the constructor and there is no way to easily recycle the object, then you can recycle this memory area using a placement new (don&#39;t forget to call the destructor though).\nI wonder if lines like  also perform allocations. If  is not a view of the tensor, then memory should be allocated and a copy is certainly done. Both operations do not scale. Can&#39;t you use views or avoid creating copies?\nI assume other functions of the loops a thread safe and you did test your code carefully. Race conditions not only break the code but also tends to make the code slower.\nI also assume  just perform a basic 1D tensor copy. If so, then accessing  is not really a problem since the default OpenMP scheduling policy tends not to cause much false sharing issue.\n"", ""excerpt"": ""I assume other functions of the loops a thread <span class=\""highlight\"">safe</span> and you did test your code carefully. Race conditions not only break the code but also tends to make the code slower. &hellip; I also assume result.index_put_ just perform a basic 1D <span class=\""highlight\"">tensor</span> copy. &hellip; "", ""title"": ""PyTorch C++ extension: efficiently accumulate results from threads in tensor""}"

Correct way to swap PyTorch tensors without copying,Stack Overflow,N/A,"{""tags"": [""pytorch""], ""question_score"": 4, ""is_accepted"": false, ""answer_id"": 78703272, ""is_answered"": false, ""question_id"": 78674317, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1720466466, ""creation_date"": 1720024267, ""body"": ""First, you should know that every tensor in PyTorch has an underlying storage that holds its actual data. You can use  to retrieve the underlying storage of a tensor. Then, you should use  to replace the tensor&#39;s underlying storage with a new storage.\n\nNote: the swapping process does not affect refences to the tensors themselves. Also, swapping the storage does not interfere with the reference counting/GC since PyTorch handles reference counting and garbage collection automatically. Another reason is that you are not creating new tensors or modifying the reference counting directly.\nUpdate\nAfter mentioning in the comments that the swapping will be within an optimizer class, this can potentially affect the autograd graph. Swapping storage objects between tensors, it turns out that the data the autograd graph references is changed. This can lead to inconsistencies between the calculated gradients and the actual computations performed on the swapped tensors. In brief, swapping data directly within an optimizer can be problematic for autograd graph. Therefore, I do not recommend swapping tensors directly within an optimizer class.\nThe only solution is to use temporary tensors for safe swapping.\n"", ""excerpt"": ""First, you should know that every <span class=\""highlight\"">tensor</span> in PyTorch has an underlying storage that holds its actual data. You can use .storage() to retrieve the underlying storage of a <span class=\""highlight\"">tensor</span>. &hellip; The only solution is to use temporary tensors for <span class=\""highlight\"">safe</span> swapping. &hellip; "", ""title"": ""Correct way to swap PyTorch tensors without copying""}"

torch.tensor results are registered as constants in the trace,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""bayesian""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77035316, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1719006415, ""creation_date"": 1693806261, ""body"": ""I am trying to fit a baayesian linear regression in python using pytorch. But the parameter values that are being sampled from the posterior distribution are all same for each parameter. That is for the slope parameter 1000 samples are being sampled and all the 1000 values of slope are the same. While running the code getting this below warning.\nTracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\nWhat does this warning suggest and how to resolve the issue of getting the same sample value for the parameters?\n"", ""excerpt"": ""I am trying to fit a baayesian linear regression in python using pytorch. But the parameter values that are being sampled from the posterior distribution are all same for each parameter. That is for t &hellip; "", ""title"": ""torch.tensor results are registered as constants in the trace""}"

"Fine-tuned Phi-2 model did not work correctly, when save it as pytorch or Pickle",Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""large-language-model"", ""transformer-model"", ""fine-tuning""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78624391, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1718388379, ""creation_date"": 1718388379, ""body"": ""I have a problem here I did fine tune Phi-2 model with LoRA, and I saved the model as a safe-tensors , and here is what is inside my folder\n\nthe safe-tensors model is working very well: here is how I make predictions\n\nbut I want to convert that to pth (Pytorch) or Pickle (pkl) but in the end the output file was so big like 12GB.\nand that was not the only problem, I tried many times, to save it in pth ,pkl and h5, but in the end I got these long error\n\nhere are my many attempts to save it:\n\nWhat should I do to get my Pth, or pkl working just like safe-tensors?\nI have even try to disable the strict mode:\n\nand even Check if the model is wrapped in DataParallel\n\nIt works, but the model prediction is making no sense at all, just garbage.\nThanks a lot in advance.\nI am trying to save my safe-tensors as Pth or pkl files, to be load faster, and easier.\n"", ""excerpt"": ""I have a problem here I did fine tune Phi-2 model with LoRA, and I saved the model as a <span class=\""highlight\"">safe</span>-tensors , and here is what is inside my folder\nphi-2-sxd\\adapter_config.json\nphi-2-sxd\\adapter_model.safetensors &hellip; I am trying to save my <span class=\""highlight\"">safe</span>-tensors as Pth or pkl files, to be load faster, and easier. &hellip; "", ""title"": ""Fine-tuned Phi-2 model did not work correctly, when save it as pytorch or Pickle""}"

RuntimeError: The size of tensor a (51) must match the size of tensor b (50) at non-singleton dimension 1,Stack Overflow,N/A,"{""tags"": [""size"", ""tensor""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78556712, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1717093774, ""creation_date"": 1717093774, ""body"": ""\nTraceback (most recent call last):\nFile &quot;e:\\projects\\transformer.py&quot;, line 281, in \ngenerated_text = generate_text(model, tokenizer, prompt, max_length=50)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;e:\\projects\\transformer.py&quot;, line 258, in generate_text\noutputs = model(input_ids, mask[:, :input_ids.size(1)])  # Adjust mask size\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;E:\\projects\\venv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 1532, in _wrapped_call_impl\nreturn self._call_impl(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;E:\\projects\\venv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 1541, in _call_impl\nreturn forward_call(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;e:\\projects\\transformer.py&quot;, line 123, in forward\nx = self.postionalencod(x)\n^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;E:\\projects\\venv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 1532, in _wrapped_call_impl\nreturn self._call_impl(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;E:\\projects\\venv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 1541, in _call_impl\nreturn forward_call(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;e:\\projects\\transformer.py&quot;, line 28, in forward\nreturn x + pos_encoding\n~~^~~~~~~~~~~~~~\nRuntimeError: The size of tensor a (51) must match the size of tensor b (50) at non-singleton dimension 1\nRuntimeError: The size of tensor a (51) must match the size of tensor b (50) at non-singleton dimension 1\n"", ""excerpt"": ""through her veins.&quot;,\n    &quot;Under the guise of a tourist, he discretely gathered intelligence in plain sight.&quot;,\n    &quot;With each passing hour, the deadline for preventing disaster drew closer.&quot;,\n    &quot;The <span class=\""highlight\"">safe</span> &hellip; a (51) must match the size of <span class=\""highlight\"">tensor</span> b (50) at non-singleton dimension 1\nRuntimeError: The size of <span class=\""highlight\"">tensor</span> a (51) must match the size of <span class=\""highlight\"">tensor</span> b (50) at non-singleton dimension 1 &hellip; "", ""title"": ""RuntimeError: The size of tensor a (51) must match the size of tensor b (50) at non-singleton dimension 1""}"

"Could not convert string to float: &#39;5,994.98&#39;",Stack Overflow,N/A,"{""tags"": [""python"", ""pandas"", ""string"", ""dataframe"", ""data-conversion""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": false, ""question_id"": 78357793, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1713611058, ""creation_date"": 1713607352, ""body"": ""You can see here the desired output\n\nDesired Output\n\nCode if you want see = https://github.com/LLNL/mttime/blob/master/examples/notebooks/02_Prepare_Data_and_Synthetics_For_Inversion.ipynb\nError in moment tensor inversion: Cannot cast array data from dtype(&#39;O&#39;) to dtype(&#39;float64&#39;)\nI&#39;m trying to create a header file for my data and use it for moment tensor inversion. Code for the header file.\n\ni called the mtinv file for inversion\n\nBut its throwing error like;\nTypeError: Cannot cast array data from dtype(&#39;O&#39;) to dtype(&#39;float64&#39;) according to the rule &#39;safe&#39;\nDuring handling of the above exception, another exception occurred:\nValueError: could not convert string to float: &#39;5,994.98&#39;\n\nThe Header File Look Like This\n\n"", ""excerpt"": ""can see here the desired output\n\nDesired Output\n\nCode if you want see = https://github.com/LLNL/mttime/blob/master/examples/notebooks/02_Prepare_Data_and_Synthetics_For_Inversion.ipynb\nError in moment <span class=\""highlight\"">tensor</span> &hellip; inversion: Cannot cast array data from dtype(&#39;O&#39;) to dtype(&#39;float64&#39;)\nI&#39;m trying to create a header file for my data and use it for moment <span class=\""highlight\"">tensor</span> inversion. &hellip; "", ""title"": ""Could not convert string to float: &#39;5,994.98&#39;""}"

torch.compile - broken data types on uint8 operations?,Stack Overflow,N/A,"{""tags"": [""pytorch""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78356918, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1713585576, ""creation_date"": 1713584142, ""body"": ""I am working on a custom quantization kernel + finetuning (but that&#39;s offtopic) now.\nSo the linear quantized kernel forward pass should be\n\nHowever I started getting type errors regards  expressions.\nSo I started investigating them by separating functions:\n\nLet&#39;s see how it works normally:\n\nprints me\n\nAs expected.\n&quot;Joined&quot; function:\n\nprints me\n\nBasically the same\nNow let&#39;s see their compiled analogues\n\n\nfine...\nLet&#39;s see &quot;fused&quot; function.\n\n\nSo as I see in the Triton code (and I am terribly bad with it):\n\nit loaded my  into \nfilled 1-element tensor with 1 (using uint16) than converted to int8 than uint8 (strange but okay) -  - so  is a 3^0 = 1-filled array\nI don&#39;t get what exactly going on with :\n\nBy mask  (which should not be true here) it filled some array with  values, else (which should be used here) with . Fine...\nBy mask  (oh, so the previous one is about signed stuff? But  should be uint8? And  were made uint8 explicitly a line ago)...\n\nWell, that should not matter - since being uint8 -  should always be false as well as \nSo let&#39;s see &quot;else&quot; branch - . Sounds fine.\n\n\nSo technically speaking  should be uint8 array?\n\n\ntmp3 is basically uint8 array filled with 3 - it all told explicitly\nSo since  breaks - it means it breaks  dtype somewhere?\n\nThe notebook with reproducing this error:\nhttps://colab.research.google.com/drive/1SJj1biyAmj2RE1t2buUZkhHN_ZM8Bd7x?usp=sharing\nBy playing with complile backends I see only  affected.\np.s. I also tried CPU compilation\n\nIt failed with basically the same error\n\nAnd CPP code I seen generated was\n\nwhere  is really int32.\n"", ""excerpt"": ""So the linear quantized kernel forward pass should be\ndef quant_matmul(x, weight, scale):\n    &quot;&quot;&quot;\n    x is a float <span class=\""highlight\"">tensor</span> - float32 / float16\n    weight is a bytes <span class=\""highlight\"">tensor</span> (each byte describe 3**5 = 243 &hellip; 564 except exc.RestartAnalysis as e:\n\nFile ~/anaconda3/envs/bitlinear-numba/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1033, in transform_code_object(code, transformations, <span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""torch.compile - broken data types on uint8 operations?""}"

AssertionError: Tried to export a function which references untracked resource,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""keras"", ""tensorflow2.0""], ""question_score"": 9, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 6, ""is_answered"": true, ""question_id"": 69040420, ""item_type"": ""question"", ""score"": 9, ""last_activity_date"": 1713445108, ""creation_date"": 1630650502, ""body"": ""I wrote a unit-test in order to safe a model after noticing that I am not able to do so (anymore) during training.\n\nTrying to save the model will always throw the following error:\n\n\nNote: As you can see in the code above, but I am not able to retrieve this tensor with .\nI tried the following too, but the result is always empty:\n\n\nThe problem is that I do not understand why I am getting this because the affected layer is tracked by Keras as you can see in the screenshot below. I took it during a debug-session in the  function.\n\nI have no explanation for this and I am running out of ideas what the issue might be here.\nThe  list in the screenshot is a property of and getting constructed by a layer  like so:\n\n\nIn order to verify that the problem is not the , I created a unit-text for saving a model that is using just this particular layer.\n\nHere I am able to save this layer without any issues:\n\n\nAs it seems to be relevant, here is the (rather ugly) implementation of :\n\nSee also\n\ntensorflow/serving #1719\n\n"", ""excerpt"": ""I wrote a unit-test in order to <span class=\""highlight\"">safe</span> a model after noticing that I am not able to do so (anymore) during training. &hellip; /conv2d/kernel:0&#39; shape=(3, 3, 3, 32) dtype=float32&gt;\n\n\nNote: As you can see in the code above, but I am not able to retrieve this <span class=\""highlight\"">tensor</span> with tf.compat.v1.get_default_graph().get_tensor_by_name(&quot;77040: &hellip; "", ""title"": ""AssertionError: Tried to export a function which references untracked resource""}"

JIT tracable PyTorch Tensor with meaningful properties,Stack Overflow,N/A,"{""tags"": [""python-3.x"", ""pytorch""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77860679, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1706085506, ""creation_date"": 1705934668, ""body"": ""The Scenario\nI have a PyTorch module which outputs a tensor of shape . The final dimension (7) is a tensor of meaningfully interpretable values which can be used in loss calculation.\nI can access these values using slicing, and have successfully done so in the past like in the following examples\n\nor\n\nAt first the code base was littered with these slicings. Since the slicing is not very human readable, unless one has specific knowledge of the how the network internally works, I have instead created my own tensor class which defines the various features neatly.\n\nAt some point in the model I have a module which wraps a resulting tensor in  during the fowrard pass\n\nIn a corresponding loss module I can now simply write\n\nTo me this is much more readable, and I would like to keep it this way if possible.\nThe problem\nWhen now tracing the model containing a module like  above, I get the following warning:\n\n\nTracerWarning: torch.Tensor results are registered as constants in the trace.\nYou can safely ignore this warning if you use this function to create tensors\nout of constant variables that would be the same every time you call this\nfunction. In any other case, this might cause the trace to be incorrect.\nreturn MyTensor(x)\n\nI understand why I&#39;m getting this warning, the tracer apparently has no knowledge of how to dynamically assign values from the original tensor  to this  tensor. In principle, no operation should even be necessary since all I&#39;m doing is trying to add some ease of use accessability properties to an existing tensor, but apparently the tracer doesn&#39;t know what to do here.\nHas anyone run into an issue similar to this? Is there a way to keep the readability of using an approach like the one described above using  whilst still being able to trace the model? Any help would be much appreciated.\n"", ""excerpt"": ""The Scenario\nI have a PyTorch module which outputs a <span class=\""highlight\"">tensor</span> of shape (batch_size, 3, 512, 7). &hellip; x to this MyTensor <span class=\""highlight\"">tensor</span>. &hellip; "", ""title"": ""JIT tracable PyTorch Tensor with meaningful properties""}"

How to locally load a finetuned LLAMA2 model that currently saved to my local disk in safe tensor format,Stack Overflow,N/A,"{""tags"": [""nlp"", ""large-language-model"", ""huggingface"", ""llama""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77744407, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1704176430, ""creation_date"": 1704175909, ""body"": ""I am new to working with LLMs. I have a finetuned LLAMA2 model in safe tensor format saved to my local disk. I want to load my model locally by running a python notebook.\nI have tried googling, and I find many examples on loading Stable diffusion models but noting much on loading LLAMA models.\n"", ""excerpt"": ""I have a finetuned LLAMA2 model in <span class=\""highlight\"">safe</span> <span class=\""highlight\"">tensor</span> format saved to my local disk. I want to load my model locally by running a python notebook. &hellip; "", ""title"": ""How to locally load a finetuned LLAMA2 model that currently saved to my local disk in safe tensor format""}"

How can I get a pytorch Tensor containing some other Tensor&#39;s size (or shape) without conversion to Python int?,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""onnx""], ""question_score"": 3, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 77586285, ""item_type"": ""question"", ""score"": 3, ""last_activity_date"": 1703647745, ""creation_date"": 1701443812, ""body"": ""In the context of exporting pytorch code to ONNX, I get this warning:\nTracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\nHere is the offending line:\n\n is a  of shape \nAnd the warning is spot on and cannot be ignored, because the shape of  is supposed to be dynamic.\nI need  to be a  that contains the number  coming from the  of . The &quot;offending line&quot; from above succeeds in doing that, but we actually make a round trip from pytorch to a Python  and back to pytorch, because the elements in the  objects are Python s. This is (1) somewhat weird, (2) probably inefficient in terms of GPU -&gt; CPU -&gt; GPU and, as stated above, an actual problem in the ONNX exporting context.\nIs there some other way how I can use a tensor&#39;s shape in torch computations, without &quot;leaving&quot; the torch world?\n"", ""excerpt"": ""In the context of exporting pytorch code to ONNX, I get this warning:\nTracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this f &hellip; "", ""title"": ""How can I get a pytorch Tensor containing some other Tensor&#39;s size (or shape) without conversion to Python int?""}"

"Tensorflow 2.x: call model inference using C/C++ API from inputs, allocated in GPU memory",Stack Overflow,N/A,"{""tags"": [""gpu"", ""tensorflow2.0"", ""c-api""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77670940, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1702730436, ""creation_date"": 1702730436, ""body"": ""I’d like to use TensorFlow 2.13 in scientific simulation code, written in C++, via C-API.\nThe code runs simulation on GPU, so all necessary input data for the model are already placed on GPU too.\nI need to prepare input for the model, that contains multiple TF_Tensors.\nMy question is: Is it possible to control, where to place TF_Tensor? Can I make it point to  existing on-GPU array to avoid CPU-to-GPU memory transfer?\nIf wrapping TF tensor around existing data is not possible,  would that be possible to copy memory within GPU?\nI found TF_AllocatorAttributes struct, that contains placement flag and it is used, for example, in TF_AllocateTemp,\nbut this function requires also . Unfortunately it is not clear for me where to take it and is it safe at all to use it in C-API\n"", ""excerpt"": ""If wrapping TF <span class=\""highlight\"">tensor</span> around existing data is not possible,  would that be possible to copy memory within GPU? &hellip; Unfortunately it is not clear for me where to take it and is it <span class=\""highlight\"">safe</span> at all to use it in C-API &hellip; "", ""title"": ""Tensorflow 2.x: call model inference using C/C++ API from inputs, allocated in GPU memory""}"

sagemaker training job doesn&#39;t create tar file,Stack Overflow,N/A,"{""tags"": [""amazon-web-services"", ""amazon-sagemaker"", ""amazon-sagemaker-studio""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77621312, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1701963702, ""creation_date"": 1701963702, ""body"": ""I have trained some models whose artifqcts I can see in my S3 bucket, including the safe tensors, and other resources. But It has not created any tar file for inference. Or if I wanted to transfer the model to other architectures, I can&#39;t seem to do that. I can&#39;t even download this folder as such. Does anyone know how I can create the tar file?\nI tried to create the model from the training job, but it also points to the same location, without the tar file\n"", ""excerpt"": ""I have trained some models whose artifqcts I can see in my S3 bucket, including the <span class=\""highlight\"">safe</span> tensors, and other resources. But It has not created any tar file for inference. &hellip; "", ""title"": ""sagemaker training job doesn&#39;t create tar file""}"

PyTorch: How to sample from a tensor where each value in the tensor has a different likelihood of being selected?,Stack Overflow,N/A,"{""tags"": [""python"", ""random"", ""pytorch"", ""probability"", ""sampling""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 4, ""is_answered"": true, ""question_id"": 72467096, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1700685536, ""creation_date"": 1654111387, ""body"": ""Given tensor\n containing probabilities which sum to 1 (I removed some decimals but it&#39;s safe to assume it&#39;ll always sum to 1), I want to sample a value from  where the value itself is the likelihood of getting sampled. For instance, the likelihood of sampling  from  is . The output of the value sampled should still be a tensor.\nI tried using  but it doesn&#39;t allow the value selected to be a tensor anymore, instead it detaches.\nOne caveat that makes this tricky is that I want to also know the index of the sampled value as it appears in the tensor. That is, say I sample , I want to know if it&#39;s index ,  or  of tensor .\n"", ""excerpt"": ""Given <span class=\""highlight\"">tensor</span>\nA = torch.tensor([0.0316, 0.2338, 0.2338, 0.2338, 0.0316, 0.0316, 0.0860, 0.0316, 0.0860]) containing probabilities which sum to 1 (I removed some decimals but it&#39;s <span class=\""highlight\"">safe</span> to assume it&#39;ll always &hellip; The output of the value sampled should still be a <span class=\""highlight\"">tensor</span>.\nI tried using WeightedRandomSampler but it doesn&#39;t allow the value selected to be a <span class=\""highlight\"">tensor</span> anymore, instead it detaches. &hellip; "", ""title"": ""PyTorch: How to sample from a tensor where each value in the tensor has a different likelihood of being selected?""}"

Too many theano runtime errors,Stack Overflow,N/A,"{""tags"": [""python"", ""deep-learning"", ""conda"", ""tensor"", ""theano""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77512470, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1700427288, ""creation_date"": 1700427288, ""body"": ""I created a conda envirinment with python 3.9 and Installed theano 1.0.5. But when I write this simple program, I encounter many errors. This is my code:\n\nand this is my errors:\n\nWhat&#39;s the problem?\nI think there are some conflicts or incompatibility between python version and packages. But I don&#39;t know what exactly it is.\nI tried with python 3.8 and theano 1.0.4, but anything didn&#39;t change. Also I changed the versions of some packages like mkl, but nothing changed.\nThanks.\n"", ""excerpt"": ""This is my code:\nimport theano\nfrom theano import <span class=\""highlight\"">tensor</span> as T\n\nx = T.vector(&#39;x&#39;)\nW = T.matrix(&#39;W&#39;)\nb = T.vector(&#39;b&#39;)\n\ndot = T.dot(x, W)\nout = T.nnet.sigmoid(dot + b)\n\nand this is my errors:\nF:\\Projects &hellip; Doing this will not modify any behavior and is <span class=\""highlight\"">safe</span>. If you specifically wanted the numpy scalar type, use `np.bool_` here. &hellip; "", ""title"": ""Too many theano runtime errors""}"

The LLM I have imported from Huggingface is running very slowly on Google Colab,Stack Overflow,N/A,"{""tags"": [""google-colaboratory"", ""large-language-model""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77491736, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1700096526, ""creation_date"": 1700096526, ""body"": ""I&#39;m running the code contained in the following snippet in my Google Colab account\nhttps://huggingface.co/quantumaikr/llama-2-70b-fb16-korean.\nThe code is here.\n\nI have a Google Pro Plus account and am using a T4 TPU. When I import the safe tensors via the AutoModelForCausalLM.from_pretrained command, this uses up almost all my disk space on Colab, which causes the subsequent code to run very slowly. However, when I mount to my Google Drive and cache the tensors in there, this also causes the code to run slowly!\nWhat am I doing wrong? It seems unlikely that this code is not capable of being run on Colab.\n"", ""excerpt"": ""When I import the <span class=\""highlight\"">safe</span> tensors via the AutoModelForCausalLM.from_pretrained command, this uses up almost all my disk space on Colab, which causes the subsequent code to run very slowly. &hellip; "", ""title"": ""The LLM I have imported from Huggingface is running very slowly on Google Colab""}"

Is the tensorflow lite c library method for feed forward (TfLiteInterpreterInvoke(interpreter);) real-time safe with respect to audio callbacks?,Stack Overflow,N/A,"{""tags"": [""c"", ""tensorflow"", ""tensorflow-lite"", ""inference"", ""inference-engine""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77287297, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1697195720, ""creation_date"": 1697195619, ""body"": ""In real-time audio software, any code that blocks the audio processing thread must be avoided at all costs, as it can cause glitches in the signal. This includes system calls on general-purpose operating systems that can lead to blocking interactions with other threads, the thread scheduler and/or the virtual memory paging mechanism. Or heap memory allocations and unconditional locks (mutexes) that have unpredictable execution times. Bencina, R. (2014, July). Interfacing real-time audio and file I/O. In Proc. of the Australasian Computer Music Conference (ACMC) (pp. 21-28).\nI wonder if the tensorflow lite c library method for forwarding the input tensor  uses any non real-time safe operations like the ones described above, which could have unpredictable execution times and therefore block the audio processing thread on callback at worst-case execution times. The interpreter creation and memory allocation can be done beforehand, so I only refer to the actual inference. Is such a call real-time safe?\nUnfortunately, it is quite difficult to check all these non real-time safe operations in the inference method... That&#39;s why I hope to find an answer here.\n"", ""excerpt"": ""I wonder if the tensorflow lite c library method for forwarding the input <span class=\""highlight\"">tensor</span> TfLiteInterpreterInvoke(interpreter); uses any non real-time <span class=\""highlight\"">safe</span> operations like the ones described above, which could &hellip; Is such a call real-time <span class=\""highlight\"">safe</span>?\nUnfortunately, it is quite difficult to check all these non real-time <span class=\""highlight\"">safe</span> operations in the inference method... That&#39;s why I hope to find an answer here. &hellip; "", ""title"": ""Is the tensorflow lite c library method for feed forward (TfLiteInterpreterInvoke(interpreter);) real-time safe with respect to audio callbacks?""}"

"TypeError: Expected int32 passed to parameter &#39;size&#39; of op &#39;Slice&#39;, got [4608.0] of type &#39;list&#39; instead",Stack Overflow,N/A,"{""tags"": [""python"", ""python-3.x"", ""tensorflow"", ""keras""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": false, ""question_id"": 71143690, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1687441887, ""creation_date"": 1645021763, ""body"": ""I am trying to train a deep learning model in keras and i am getting this error\n\nI even tried changing my model, but error is still there\nHere is my model\n\nHere is shape of my data\n\nError logs\n\n"", ""excerpt"": ""issubclass(v.dtype.type, expected_types))):\n--&gt; 269         _check_failed(v)\n    270 \n\n~/venv/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py in _check_failed(v)\n    249   # it is <span class=\""highlight\"">safe</span> &hellip; 203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\n\n~/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in slice(input_, begin, size, name)\n   1101     A `<span class=\""highlight\"">Tensor</span> &hellip; "", ""title"": ""TypeError: Expected int32 passed to parameter &#39;size&#39; of op &#39;Slice&#39;, got [4608.0] of type &#39;list&#39; instead""}"

Pytorch Tensorboard Add_graph &quot;Cannot insert a Tensor that requires grad as a constant&quot;,Stack Overflow,N/A,"{""tags"": [""python"", ""deep-learning"", ""neural-network"", ""pytorch"", ""tensorboard""], ""question_score"": 1, ""is_accepted"": false, ""answer_id"": 76171963, ""is_answered"": false, ""question_id"": 70747578, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1683194708, ""creation_date"": 1683194708, ""body"": ""I my case, I got the error message when tracing a model which was ill-formed in pytorch terms.\nSpecifically,  needs to register all layers and submodules internally. If a layer / module is not registered but encountered during the trace, the cryptic error message is raised.\nIn most cases, registering is done during  by either:\n\nassigning submodules as instance attributes ( catches these by overriding ): \nexplicit via  or \n\nThe causes for a failed registration can be diverse and subtle when using nested submodules or fancy logic in . The printed tensor may be helpful to find the missing module.\nIn any case, it should be a safe bet to check if all direct submodules appear in .\nThe same thing applies to Parameters () and Buffers (). The source code of  might be worth looking at when in doubt.\nIn my case, I used a dataclass to bundle multiple submodules, and just assigning the dataclass caused  to miss the actual submodules inside.\nThere are probably other causes for this error message, because the message text seems not descriptive of the problem, but something different. Anyways, I wanted to share my solution because I did not find it mentioned online already.\n"", ""excerpt"": ""The printed <span class=\""highlight\"">tensor</span> may be helpful to find the missing module.\nIn any case, it should be a <span class=\""highlight\"">safe</span> bet to check if all direct submodules appear in self._modules. &hellip; "", ""title"": ""Pytorch Tensorboard Add_graph &quot;Cannot insert a Tensor that requires grad as a constant&quot;""}"

How to implement graph data structure consisting of ndarray arrays?,Stack Overflow,N/A,"{""tags"": [""generics"", ""rust"", ""autodiff"", ""rust-ndarray""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 75762717, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1679120918, ""creation_date"": 1679012051, ""body"": ""I wanted to implement some computational graph data structure in Rust (that can hopefully support autodiff), and I decided to use the ndarray crate, since that is most similar to NumPy. Making a graph data structure isn&#39;t really all that difficult, and involves either using a  where each Node contains references to other vector elements, or involves a node that has reference counted pointers to other nodes.\nHowever, this entirely fails when using ndarray. Suppose we have a fairly common type, which is . This is an n-dim array that owns its data, and has two generic parameters: T for the type (e.g. f32, i32, , etc.) and D for the number of dimensions (could be dynamic using ). How can I create a useful graph data structure where each single node has different generics? For instance, how can I make a graph where the one node has some D and T, and the node that it points to has a D and T that are of different types?\nYou might be asking, why would I need that? Why not fix T to some value that makes sense, like , and use dynamic sized arrays? The problem is that I would like to support transformations from real numbers to complex numbers, such as FFT.\nSo given that you want to implement some variant of reverse mode automatic differentiation on tensors, how would you implement the computational graph?\nThis is one of my attempts:\n\nThis code is inspired by this blog post:\nI actually did get such a graph working, where there is a central list of pointers to a bunch of dynamically dispatched nodes (this is what the Tape struct does). What&#39;s the problem, you say? Well, given an index, suppose I want to retrieve some array from a Tape. How would I do that? Because the tape stores pointers to trait objects, should we just add a getter method to the trait definition? No! If we try to return an ndarray Array with generic parameters and add that function to the trait definition, we will create a trait that is not object safe. So, this whole system falls apart. To be honest, it&#39;s a bit frustrating.\nIs there any way to make this system work, or does the entire approach need to change? If so, what approach would you use? I know there is some approach that works, because the neuronika crate is able to generate a computational graph with nodes of differing generic types.\nEDIT: After reviewing how Neuronika&#39;s implementation is able to function with ndarray generics (answer posted below), how would you approach this problem? Would you try a different approach, or would you just stick to some variant of this one?\n"", ""excerpt"": ""If we try to return an ndarray Array with generic parameters and add that function to the trait definition, we will create a trait that is not object <span class=\""highlight\"">safe</span>. So, this whole system falls apart. &hellip; "", ""title"": ""How to implement graph data structure consisting of ndarray arrays?""}"

"What is the difference between detach, clone and deepcopy in Pytorch tensors in detail?",Stack Overflow,N/A,"{""tags"": [""python"", ""machine-learning"", ""pytorch""], ""question_score"": 26, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 62437509, ""item_type"": ""question"", ""score"": 26, ""last_activity_date"": 1677499515, ""creation_date"": 1592424385, ""body"": ""I&#39;ve been struggling to understand the differences between ,  and  when using Pytorch. In particular with Pytorch tensors.\nI tried writing all my question about their differences and uses cases and became overwhelmed quickly and realized that perhaps have the 4 main properties of Pytorch tensors would clarify much better which one to use that going through every small question. The 4 main properties I realized one needs keep track are:\n\nif one has a new pointer/reference to a tensor\nif one has a new tensor object instance (and thus most likely this new instance has it&#39;s own meta-data like , shape, , etc.)\nif it has allocated a new memory for the tensor data (i.e. if this new tensor is a view of a different tensor)\nif it&#39;s tracking the history of operations or not (or even if it&#39;s tracking a completely new history of operations or the same old one in the case of deep copy)\n\nAccording to what mined out from the Pytorch forums and the documentation this is my current distinctions for each when used on tensors:\nClone\nFor clone:\n\nI believe this is how it behaves according to the main 4 properties:\n\nthe cloned  has it&#39;s own python reference/pointer to the new object\nit has created it&#39;s own new tensor object instance (with it&#39;s separate meta-data)\nit has allocated a new memory for  with the same data as \nit is keeping track of the original history of operations and in addition included this  operation as \n\nit seems that the main use of this as I understand is to create copies of things so that  operations are safe. In addition coupled with  as  (the &quot;better&quot; order to do it btw) it creates a completely new tensor that has been detached with the old history and thus stops gradient flow through that path.\nDetach\n\n\ncreates a new python reference (the only one that does not is doing  of course). One can use  for this one I believe\nit has created it&#39;s own new tensor object instance (with it&#39;s separate meta-data)\nit has NOT allocated a new memory for  with the same data as x\nit cuts the history of the gradients and does not allow it to flow through it. I think it&#39;s right to think of it as having no history, as a brand new tensor.\n\nI believe the only sensible use I know of is of creating new copies with it&#39;s own memory when coupled with  as . Otherwise, I am not sure what the use it. Since it points to the original data, doing in place ops might be potentially dangerous (since it changes the old data but the change to the old data is NOT known by autograd in the earlier computation graph).\ncopy.deepcopy\n\n\nif one has a new pointer/reference to a tensor\nit creates a new tensor instance with it&#39;s own meta-data (all of the meta-data should point to deep copies, so new objects if it&#39;s implemented as one would expect I hope).\nit has it&#39;s own memory allocated for the tensor data\nIf it truly is a deep copy, I would expect a deep copy of the history. So it should do a deep replication of the history. Though this seems really expensive but at least semantically consistent with what deep copy should be.\n\nI don&#39;t really see a use case for this. I assume anyone trying to use this really meant 1)  or just 2)  by itself, depending if one wants to stop gradient flows to the earlier graph with 1 or if they want just to replicate the data with a new memory 2).\nSo this is the best way I have to understand the differences as of now rather than ask all the different scenarios that one might use them.\nSo is this right? Does anyone see any major flaw that needs to be correct?\nMy own worry is about the semantics I gave to deep copy and wonder if it&#39;s correct wrt the deep copying the history.\nI think a list of common use cases for each would be wonderful.\n\nResources\nthese are all the resources I&#39;ve read and participated to arrive at the conclusions in this question:\n\nMigration guide to 0.4.0 https://pytorch.org/blog/pytorch-0_4_0-migration-guide/\nConfusion about using clone: https://discuss.pytorch.org/t/confusion-about-using-clone/39673/3\nClone and detach in v0.4.0: https://discuss.pytorch.org/t/clone-and-detach-in-v0-4-0/16861/2\nDocs for clone:\n\nhttps://pytorch.org/docs/stable/tensors.html#torch.Tensor.clone\n\n\nDocs for detach (search for the word detach in your browser there is no direct link):\n\nhttps://pytorch.org/docs/stable/tensors.html#torch.Tensor\n\n\nDifference between detach().clone() and clone().detach(): https://discuss.pytorch.org/t/difference-between-detach-clone-and-clone-detach/34173\nWhy am I able to change the value of a tensor without the computation graph knowing about it in Pytorch with detach? Why am I able to change the value of a tensor without the computation graph knowing about it in Pytorch with detach?\nWhat is the difference between detach, clone and deepcopy in Pytorch tensors in detail? What is the difference between detach, clone and deepcopy in Pytorch tensors in detail?\nCopy.deepcopy() vs clone() https://discuss.pytorch.org/t/copy-deepcopy-vs-clone/55022/10\n\n"", ""excerpt"": ""like require_grads, shape, is_leaf, etc.)\nif it has allocated a new memory for the <span class=\""highlight\"">tensor</span> data (i.e. if this new <span class=\""highlight\"">tensor</span> is a view of a different <span class=\""highlight\"">tensor</span>)\nif it&#39;s tracking the history of operations or not &hellip; operations and in addition included this clone operation as .grad_fn=&lt;CloneBackward&gt;\n\nit seems that the main use of this as I understand is to create copies of things so that inplace_ operations are <span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""What is the difference between detach, clone and deepcopy in Pytorch tensors in detail?""}"

Add values of one tensor to another without affecting the graph,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""tensor"", ""addition"", ""backpropagation""], ""question_score"": 0, ""is_accepted"": true, ""answer_id"": 75298860, ""is_answered"": false, ""question_id"": 75291122, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1675176342, ""creation_date"": 1675176342, ""body"": ""You should be able to use  to return a copy of the tensor () with . Using the inplace += operator causes errors during backpropagation (i.e. at various times during the forward pass, the same variable stored 2 different values with 2 different associated gradients, but only one set of value/ gradient is stored in that variable during the backwards pass.) I&#39;m a bit fuzzy on whether in-place operations are allowed for variables that are part of the computation graph but when the operation itself is not. You can test this to see, but to be safe I recommend:\n\nLater, if you want to do another operation using tensor2 where the gradient IS part of the computation graph, you still can do this as well.\n"", ""excerpt"": ""You should be able to use detatch() to return a copy of the <span class=\""highlight\"">tensor</span> (tensor2) with requires_grad = False. &hellip; You can test this to see, but to be <span class=\""highlight\"">safe</span> I recommend:\ntensor1[:,:,:3] =  torch.add(tensor1[:,:,:3],tensor2[:,:,:3].detach())\n\nLater, if you want to do another operation using tensor2 where the gradient &hellip; "", ""title"": ""Add values of one tensor to another without affecting the graph""}"

libtorch: Is writing to non-overlapping slices of a torch::Tensor from different threads safe when using index_put_ method?,Stack Overflow,N/A,"{""tags"": [""c++"", ""multithreading"", ""thread-safety"", ""tensor"", ""libtorch""], ""question_score"": 3, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 75086319, ""item_type"": ""question"", ""score"": 3, ""last_activity_date"": 1673455135, ""creation_date"": 1673455135, ""body"": ""I want to write to different non-overlapping slices of a  object by calling libtorch&#39;s  method from different threads concurrently, i.e. each thread writes to a different part of the tensor. Is this safe without any synchronization (i.e. mutexes)? I cannot find any related documentation.\n"", ""excerpt"": ""the <span class=\""highlight\"">tensor</span>. &hellip; Is this <span class=\""highlight\"">safe</span> without any synchronization (i.e. mutexes)? I cannot find any related documentation. &hellip; "", ""title"": ""libtorch: Is writing to non-overlapping slices of a torch::Tensor from different threads safe when using index_put_ method?""}"

RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same,Stack Overflow,N/A,"{""tags"": [""python"", ""python-3.x"", ""machine-learning"", ""deep-learning"", ""pytorch""], ""question_score"": 185, ""is_accepted"": false, ""answer_id"": 74863152, ""is_answered"": false, ""question_id"": 59013109, ""item_type"": ""answer"", ""score"": 8, ""last_activity_date"": 1671539540, ""creation_date"": 1671539540, ""body"": ""Notice that (from pytorch documentation):\n\nIf the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.\n\nThat is, you might need to do:\n\nInstead of just:\n\nWith the first approach you&#39;ll be in the safe side.\n"", ""excerpt"": ""Otherwise, the returned <span class=\""highlight\"">tensor</span> is a copy of self with the desired torch.dtype and torch.device. &hellip; That is, you might need to do:\nmodel = model.to(&quot;cuda&quot;)\ndata = data.to(&quot;cuda&quot;)\n\nInstead of just:\nmodel.to(&quot;cuda&quot;)\ndata.to(&quot;cuda&quot;)\n\nWith the first approach you&#39;ll be in the <span class=\""highlight\"">safe</span> side. &hellip; "", ""title"": ""RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same""}"

Efficient GC-assisted cleanup of LARGE native resources,Stack Overflow,N/A,"{""tags"": [""java"", ""garbage-collection"", ""java-native-interface""], ""question_score"": 0, ""is_accepted"": true, ""answer_id"": 74293734, ""is_answered"": false, ""question_id"": 74270156, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1667411856, ""creation_date"": 1667411856, ""body"": ""I think that I have now figured out a solution that works as good as it can.\nThe problem\nIf you face a similar issue you probably have code that fits some of these criterias:\n\nA high allocation rate of small objects, which hold large native resources\nObjects referencing each other in complicated ways that is hard for the GC to untangle\nNo place in the code where you can safely determine that the resources are no longer in use\n\nRequirements for a potential solution\nYour requirements probably are:\n\nDon&#39;t bottleneck the loop that allocates the native handles\nNearly instantanious cleanup after the native handle becomes unreferenced\n\nThe tradeoff\nIt turns out you cannot accomplish both these requirements at once.\nYou unfortunately have to choose between one or the other.\nIf you don&#39;t want to bottleneck the loop that allocates these native handles at a high rate, you need to trade RAM to do that.\nIf you want instantatious cleanup after the native handle becomes unreferenced,\nyou have to sacrifice the execution speed of the code that allocates the native handles.\nThe (hacky) solution\nCreate a mechanism such that you can asynchronously request a full GC to be performed.\n\nIdeally, you have a region of code that is loosely associated with cleanup of these handle objects. It doesn&#39;t have to mean that these objects can be safely disposed at this point in time, it just has to mean that the object is &gt;probably&lt; safe to delete. This callsite merely serves a statistical metric to determine the best intervall in which to trigger the Garbage Collection.\nYou should also know the size of your native resource, or alternatively an estimate of how bad it would be to keep a given object arround.\nAlternatively you could also place this at the point of the allocation of your native handles, but note that the effectiveness of the statistical metric that you collect is less effective.\nThis is an example of such a method in my tensor processing library Sci-Core:\n\nTo fight against bottlenecking your allocation loop, you can use the following JVM arguments:\n\nWhy would this work?\nTriggering regular garbage collection seems to make the garbage collector interested in cleaning the very small handle objects (among basically every other object that you create in your application. You still don&#39;t have &quot;prioritization&quot; for your handles, they just happen to also be garbage collected. If your application in addition to the native handle objects also allocates a significant amount of other small objects, the effectiveness of this technique will be significantly reduced.\nNote however, that triggering the Garbage collector is expensive and thus the maximum value for  and  must be carefully chosen.\nRunning the garbage collector asynchronously is less expensive, as it will not bottleneck your allocation loop very much but also less effective than calling the garbage collector on the same thread the objects are allocated. So, doing both in carefully chosen intervals can probably get you a good compromise between execution speed of your allocation loop and memory footprint.\n"", ""excerpt"": ""It doesn&#39;t have to mean that these objects can be safely disposed at this point in time, it just has to mean that the object is &gt;probably&lt; <span class=\""highlight\"">safe</span> to delete. &hellip; * @param <span class=\""highlight\"">tensor</span> the <span class=\""highlight\"">tensor</span> to drop the computation history for\n     */\n    public void dropHistory(ITensor <span class=\""highlight\"">tensor</span>) {\n      // for all nodes now dropped from the graph\n      ... &hellip; "", ""title"": ""Efficient GC-assisted cleanup of LARGE native resources""}"

Using PyTorch tensors with scikit-learn,Stack Overflow,N/A,"{""tags"": [""python"", ""numpy"", ""scikit-learn"", ""pytorch"", ""tensor""], ""question_score"": 5, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 70021547, ""item_type"": ""question"", ""score"": 5, ""last_activity_date"": 1666040801, ""creation_date"": 1637245519, ""body"": ""Can I use PyTorch tensors instead of NumPy arrays while working with scikit-learn?\nI tried some methods from scikit-learn like  and , and it seems to work just fine, but is there anything I should know when I&#39;m using PyTorch tensors instead of NumPy arrays?\nAccording to this question on https://scikit-learn.org/stable/faq.html#how-can-i-load-my-own-datasets-into-a-format-usable-by-scikit-learn :\n\nnumpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable.\n\nDoes that mean using PyTorch tensors is completely safe?\n"", ""excerpt"": ""Does that mean using PyTorch tensors is completely <span class=\""highlight\"">safe</span>? &hellip; "", ""title"": ""Using PyTorch tensors with scikit-learn""}"

tf.bitcast equivalent in pytorch?,Stack Overflow,N/A,"{""tags"": [""python"", ""numpy"", ""tensorflow"", ""pytorch""], ""question_score"": -2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 73853956, ""item_type"": ""question"", ""score"": -2, ""last_activity_date"": 1664197565, ""creation_date"": 1664194227, ""body"": ""This question is different from tf.cast equivalent in pytorch?.\nbitcast do bitwise reinterpretation(like  in C++) instead of &quot;safe&quot; type conversion.\nThis operation is useful when you want to store bfloat16 tensor with numpy.\n\nCurrently numpy doesn&#39;t natively support bfloat16, so  will raise \n"", ""excerpt"": ""bitcast do bitwise reinterpretation(like reinterpret_cast in C++) instead of &quot;<span class=\""highlight\"">safe</span>&quot; type conversion. &hellip; This operation is useful when you want to store bfloat16 <span class=\""highlight\"">tensor</span> with numpy.\nx = torch.ones(224, 224, 3, dtype=torch.bfloat16\nx_np = bitcast(x, torch.uint8).numpy()\n\nCurrently numpy doesn&#39;t natively support &hellip; "", ""title"": ""tf.bitcast equivalent in pytorch?""}"

Creating a keras loss function by averaging MAE across groups from another column,Stack Overflow,N/A,"{""tags"": [""python"", ""pandas"", ""tensorflow"", ""keras""], ""question_score"": 1, ""is_accepted"": true, ""answer_id"": 73725206, ""is_answered"": false, ""question_id"": 73660960, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1663225843, ""creation_date"": 1663211127, ""body"": ""Firstly, I would advise against using pandas directly as input to keras model, since the way keras interacts with pandas is not officially documented. For example, while I can fit the model using dataframes\n\nI got an error trying to call the model on \n\nHowever, using pandas to preprocess data then feed the underlying numpy arrays, e.g.,  and , to the model is completely fine, so I will do exactly that in my following answer.\n\nFrom your descriptions, I assume that your  and  data (both training and validation) are something like\n\nThat is,  is a dataframe of the form (only 2/80 features are used for simplicity)\n\nand  is a dataframe of the form\n\nTo start with, convert the one-hot encoded category of the samples to integer form, e.g.,\n\nThen, define an additional model input for this integer series and concatenate that with the original output. Doing so creates an output tensor containing both the prediction and the integer category.\n\nNow comes the main part, which is to write a custom loss function to calculate the grouped mae from the composite output tensor. To do so, we re-split the prediction tensor and category tensor from , then make use of TF&#39;s segmented ops, more specifically , inside the function.\n\nThe  statements are just for debugging purposes and can be safely removed later. Also, only groups that have actual samples in the batch are taken into account when calculating .\nCompiling the model with this custom loss function and fit it with a batch size of 5 gives the following printout\n\nwhich confirms the correctness of the implementation. Note that  is always ordered so that the i-th entry corresponds to group i.\nOnce training is finished, you can call the model and extract the prediction tensor as\n\n"", ""excerpt"": ""To do so, we re-split the prediction <span class=\""highlight\"">tensor</span> and category <span class=\""highlight\"">tensor</span> from y_pred, then make use of TF&#39;s segmented ops, more specifically unsorted_segment_mean, inside the function.\ndef group_mae(y_true, y_pred &hellip; Once training is finished, you can call the model and extract the prediction <span class=\""highlight\"">tensor</span> as\nmodel([x.values, x_int_cat.values])[...,:-1] &hellip; "", ""title"": ""Creating a keras loss function by averaging MAE across groups from another column""}"

Why GPU memory usage is increasing when I call a method that uses tensors already exist on GPU?,Stack Overflow,N/A,"{""tags"": [""python"", ""memory"", ""time"", ""pytorch"", ""gpu""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 73686586, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1662978183, ""creation_date"": 1662972070, ""body"": ""Consider the following overviews of two codes in PyTorch. Dataloader and all other aspects are the same for the two codes except the , which is present in some other module.\nCode #1:\n\nCode #2:\n\nExtra class in another module\n\nWhen I run code #1, then there is a constant usage of GPU memory (say 70%). But when I run code #2, while executing the line , the memory of GPU is increasing suddenly and after the line, it is decreasing. So, I can safely infer that the line is loading many tensors into GPU memory during execution, which is causing an increase in executing time. But I am not understanding why it is happening.\nI am not using  in  of  . The  is only using the tensors that are transferred to the GPU device in  of  only. So all the transfers have to be done during object creation only ( ) if I am correct, but why the transfer is happening when I call the  on the object?\n"", ""excerpt"": ""Consider the following overviews of two codes in PyTorch. Dataloader and all other aspects are the same for the two codes except the extra_class, which is present in some other module.\nCode #1:\nmain(a &hellip; "", ""title"": ""Why GPU memory usage is increasing when I call a method that uses tensors already exist on GPU?""}"

Tensorflow Lite c++ Build,Stack Overflow,N/A,"{""tags"": [""c++"", ""tensorflow"", ""tensorflow-lite""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 73610444, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1662454574, ""creation_date"": 1662385816, ""body"": ""I have already put a question about the access violation of the TensorFlow lite c++ API. No one answered it so far, I believe the error I made is with selecting the wrong header- and library files from the Bazel build.\nThe steps that I have done to get the Tensorflow Lite Header and Libraries are from Youtube Tutorial and from Tensorflow.\n\nGet Required Python (for me Python 3.9.5)\nInstall required Packages locally\nInstall Bazel (for me 3.7.2) and MSYS2 (after installation run ) and add it to Path\nCheck VS Build Tools 2019 for C++ (I have VS 19 Community with MSVC v142 &amp; Windows 10 SDK)\nDownload and Unzip Tensorflow Sources from Github (Release of 2.5.3)\nInside the Tensorflow Sources, use  to start configure the bazel build (I only used Yes for  , the rest is kept on the default value)\nThe I opened GitBash cmd inside the tensorflow source \nAfter a successful build later I get the &quot;bazel-bin&quot;, &quot;bazel-out&quot;, &quot;bazel-tensorflow-2.5.3&quot; and &quot;bazel-testlogs&quot; folder.\nI created the following folders tensorflow/include/tensorflow/lite &amp; core and  tensorflow/include/flatbuffers for the headers and finally the tensorflow/lib for the libraries.\nI copied the  &amp;  from the build directory (tensorflow-2.5.3\\bazel-bin\\tensorflow\\lite) into the tensorflow/lib directory together with the  (from tensorflow-2.5.3\\bazel-bin\\external\\flatbuffers\\src)\nI copied the tensorflow-2.5.3\\bazel-bin\\external\\flatbuffers\\src_virtual_includes\\flatbuffers\\flatbuffers headers into the tensorflow/include/flatbuffers directory\nI copied the tensorflow-2.5.3\\tensorflow\\lite and tensorflow-2.5.3\\tensorflow\\core from the original sources into the tensorflow/include/tensorflow/lite &amp; core directory.\n\nAfter those steps, I could create a new VS Project and add the created linker and include information. And created the following short example to read the input layer.\n\nBut I am still receiving the access violation exception inside  at\n\nWhat am I doing wrong? I dont want to build the shared library since the target (Coral Edge) has direct access to those functions (ex.  too.\n"", ""excerpt"": ""= Pre-invoke Interpreter State ===\\n&quot;);\n    tflite::PrintInterpreterState(interpreter.get());\n\n    interpreter-&gt;SetAllowFp16PrecisionForFp32(true);\n    interpreter-&gt;SetNumThreads(1);\n    // Get Input <span class=\""highlight\"">Tensor</span> &hellip; typed_input_tensor&lt;unsigned char&gt;(0);\n}\n\nBut I am still receiving the access violation exception inside interpreter.h at\n  const Subgraph&amp; primary_subgraph() const {\n    return *subgraphs_.front();  // <span class=\""highlight\"">Safe</span> &hellip; "", ""title"": ""Tensorflow Lite c++ Build""}"

"Modelling the composition of units (e.g. Inch, Dollar, etc) in Haskell",Stack Overflow,N/A,"{""tags"": [""haskell"", ""types"", ""type-systems""], ""question_score"": 2, ""is_accepted"": true, ""answer_id"": 73393851, ""is_answered"": false, ""question_id"": 73390539, ""item_type"": ""answer"", ""score"": 5, ""last_activity_date"": 1660763782, ""creation_date"": 1660763782, ""body"": ""First, let me give this opinion: IMO the type should be called , not . The constructor should be called , but the beauty of representing physical quantities with types is that the unit really becomes an “invisible” implementation detail. You could in fact use  to work with different constructors for different units of the same length type, and/or lens-isomorphism for unit conversion. But this is somewhat tangential to the question.\nThe comments have already linked to existing physical-units libraries. Using one of those would definitely be the most sensible approach for a real project.\nStephan Boyer&#39;s blog post you&#39;ve linked is definitely intriguing, however representing dimension-quotiens by general functions is really not very practical.\nI&#39;ll show something in between: still using bespoke types instead of bell&amp;whistley physical-unit ones, but anyway within a more numerics-suitable framework.\nAlready in your previous question, I pointed to the vector-space library, because  (unlike ) is a suitable abstraction for physical quantities. As you&#39;ve noticed, it only supports addition and scaling-by-real-number though, not multiplying or dividing physical quantities.\nBut the mathematical concept of vector spaces does extend to such operations as well. The Boyer blog goes in this direction: it represents  by a function . Which does make sense: what is a velocity? It&#39;s something that tells you, “if you wait for so and so long, how long will the object travel”.\nHowever,  is a way too big type, both in the sense that storing an arbitrary function is total overkill and inefficient for something that you know can also be represented by a single number, but more importantly also in that it doesn&#39;t capture the fundamental idea: a velocity is by definition a linearized function , because for sufficiently small time-deltas the motion can always be approximated by the first two Taylor terms.And it is well known that linear functions are sensibly described as matrices†. In our case, both time and length is a 1-dimensional space, so it&#39;ll be a 1&#215;1 matrix... IOW a single number again.\nThis idea of abstracting over linear functions in a type-safe manner but still having numbers/matrices as the internal representation is what I wrote the linearmap-category package for. It builds upon , but the classes turn out to become a lot uglier. Fortunately, for a simple type like yours the instances can be auto-generated: first you use  for making instances of the  classes, then there&#39;s a Template Haskell macro for also defining the linear-map etc. types.\n\nNow you can use the type combinators from , and always immediately have the vector space operations on them! As I already said, quotients correspond to linear maps. Products correspond to tensor products, which again is for the 1-dimensional case also just a newtype wrapper around a single number.\n\nIt&#39;s not really clear to me what you want the  function to do, but this would probably be implemented with .\n\n†Actually though, matrices are also often not a good representation, because they scale quadratically in the dimension of the spaces. Of course this is not relevant here.\n"", ""excerpt"": ""This idea of abstracting over linear functions in a type-<span class=\""highlight\"">safe</span> manner but still having numbers/matrices as the internal representation is what I wrote the linearmap-category package for. &hellip; Products correspond to <span class=\""highlight\"">tensor</span> products, which again is for the 1-dimensional case also just a newtype wrapper around a single number.\ntype Area = Length ⊗ Length\n\ntype PricePerArea = Area +&gt; Price\n\ncircleArea &hellip; "", ""title"": ""Modelling the composition of units (e.g. Inch, Dollar, etc) in Haskell""}"

how to modify tf.data.batchDataset,Stack Overflow,N/A,"{""tags"": [""python"", ""pandas"", ""tensorflow"", ""keras"", ""time-series""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 73265043, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1660021943, ""creation_date"": 1659849122, ""body"": ""I&#39;m looking for a way to modify a time-based dataset obtained by merging multiple time series (window and interleave).\nTLDR;\nThe time series don&#39;t overlap, creating windows/tensors that contain NaNs.  I&#39;m looking for a way to remove them from the batch dataset, altering the batches&#39; size.\n\nMore details\nHere is an example of the data I&#39;m working with:\nEach dataset pertains to an object that sends some features&#39; values through time.\nThe objects are not necessarily online at the same time, as depicted below.\n\nBecause all objects are not present at the same time and the window is a parameter of the time series, we&#39;ll contend with &quot;incomplete&quot; tensors that contain NaNs.\nI want to remove those incomplete tensors while keeping the time-based batching; I need to ensure that all data in a batch trace back to the same point in time.\ndesired output:\n\nI can&#39;t find a good way to achieve that goal.\nDo you have any ideas to share?\nfor info, the real dataset is\n\n10&#39;s k objects\n100&#39;s features\n10 k time points\nwindow size is a parameter I&#39;m experimenting with.\n\n\nI thought of: \na- creating a layer in the model to discard any tensor that has NaNs...\nHow to conditionally and safely &quot;disappear&quot; an input from a tf.keras.layers.Layer?\nI also think it&#39;s cleaner if that type of processing is made upstream from the model itself.\nb- using map_fn to remove the incomplete tensors:\n\n\nc- clean and rebuild the batchDataset, batch by batch (make a pandas.Dataframe out of the cleaned dataset then make a batchDataset)\n\n\nd- using tf.data.experimental.dense_to_ragged_batch to force the original batchDataset into a Ragged-batchDataset then remove the &quot;incomplete&quot; tensors\n\n\n"", ""excerpt"": ""I thought of: \na- creating a layer in the model to discard any <span class=\""highlight\"">tensor</span> that has NaNs...\nHow to conditionally and safely &quot;disappear&quot; an input from a tf.keras.layers.Layer? &hellip; Ragged-batchDataset then remove the &quot;incomplete&quot; tensors\n    def Ragged_cleanBatch (aBatchedDS):\n        # \n        # cleanBatch processes the batched dataset by removing, in each batch, any innermost <span class=\""highlight\"">tensor</span> &hellip; "", ""title"": ""how to modify tf.data.batchDataset""}"

What is a protobuf message?,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""protocol-buffers"", ""tfrecord""], ""question_score"": 2, ""is_accepted"": true, ""answer_id"": 72850228, ""is_answered"": false, ""question_id"": 72845039, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1656962552, ""creation_date"": 1656886203, ""body"": ""A message is classically thought of as a collection of bytes that are conveyed from one process/thread to another process/thread. Typically (but not necessarily), the collection of bytes means something to the sender and receiver, e.g. it&#39;s an object that has been serialised somehow (perhaps using Google Protocol Buffers). So, an object can become a message by serialising it and placing the bytes into an array that one might term a &quot;message&quot;.\nIt&#39;s not necessarily the case the processes handling the collection of bytes will deserialise them. For example, a process that is simply going to pass them onwards down another connection need not actually deserialise them, if it already knows where the bytes are supposed to be sent.\nThe means by which a message is conveyed is typically some sort of queue / pipe / socket / stream / etc. Where it gets interesting is that most data transports of this sort are stream connections; whatever bytes you push in one end comes out the other. So, then, how to use those for sending messages?\nThe answer is that there has to be some way of demarcating between messages. There&#39;s lots of ways of doing that, but these days it makes far more sense to use something like ZeroMQ, which takes care of all that for you (and more besides). ZeroMQ is a library / protocol that allows a program to transfer a collection of bytes from one process/thread to another via stream connections, and ensure that the receiving program gets the collection in one nice and complete buffer. Those bytes could be objects serialised by Google Protocol Buffer, or serialised in some other way (there&#39;s lots). HTTP is also used as a way of moving objects around, e.g. a page of HTML.\nSo the pattern is object -&gt; serialisation -&gt; message buffer -&gt; some sort of byte transport that demarcates one message from another -&gt; message buffer -&gt; deserialisation -&gt; object.\nAn advantage of serialisations like Protocol Buffers is that the sender and receiver need not be written in the same language, or share anything at all except for the .proto file. Other approaches to serialisation often involves marking up class definitions in the program source code, which then makes it difficult to deserialise data in another language.\nAlso in languages like C/C++ one might get away with simply copying the bytes at the object&#39;s address from one place to another. This can be a total disaster if the destination is a different machine; endianness etc. can matter a lot. There are serialisation standards that get close to this, specifically  (see this).\nThere are variations. Within a process, &quot;passing a message&quot; can simply mean passing ownership of an object around. Ownership can be by convention, i.e. if I&#39;ve just written the object pointer to a message queue, I won&#39;t mutate the object anymore. I think in Rust it&#39;s even expressed by the language syntax, in that once object ownership has been given up the language won&#39;t let you mutate the object (worked out at compile time, part of what makes Rust so good). The net result looks like message transfer, but in fact all that&#39;s happened is a pointer (typically, 64bits) has been copied from A to B, not the entire data in the object. This is a lot faster.\nEDIT\nSo, How Does a Message Transport Protocol Work?\nIt&#39;s worth digging into how something like ZeroMQ works. For it to be able to pass whole application messages across a stream connection, it needs operate some sort of protocol. That protocol is itself going to involve objects (Protocol Data Units) being &quot;serialised&quot; (well, converted to an agreed wire format), pushed through the stream connection, deserialised, and understood by the ZeroMQ library that&#39;s on the receiving end. And, when gets on down to it, ZeroMQ is using TCP (over a network), and that too is a protocol built on IP. And that goes on down to Ethernet frames.\nSo, there&#39;s protocols running atop protocols, running atop other protocols (in fact, this is the Layer Model of how computer interconnectedness works).\nWhy That Matters, and What Can Go Wrong\nIt&#39;s useful to bearing this protocol layering in mind. Sometimes, one might have a requirement to (for example), take very strong measures against buffer overflows, perhaps to prevent remote exploitation. That might be a reason to pick a serialisation technology that helps guard against such things - e.g. Protocol Buffers. However, when picking such a technology, one has to realise that the requirement is met provided that all of the protocol layerings are equally robust. There&#39;s no point using, say, Protocol Buffers and declaring oneself safe against buffer overflows, if the OS&#39;s IP stack is broken and exploitable.\nThis is well illustrated by the Heartbleed bug in OpenSSL (see here). This was caused effectively by a weakly specified protocol (see RFC6520); it&#39;s defined in English language, and requires the programmer to read this, code up the protocol by hand, and pay attention to all the strictures written in the document. The associated RFC5426 even says:\n\nThis document deals with the formatting of data in an external\nrepresentation.  The following very basic and somewhat casually\ndefined presentation syntax will be used.  The syntax draws from\nseveral sources in its structure.  Although it resembles the\nprogramming language &quot;C&quot; in its syntax and XDR [XDR] in both its\nsyntax and intent, it would be risky to draw too many parallels.  The\npurpose of this presentation language is to document TLS only; it has\nno general application beyond that particular goal.\n\nThe Heartbleed bug in OpenSSL was a result of the coding up of the English language spec being done wrong, and given that highlighted statement perhaps it&#39;s no great surprise. Applications that were using OpenSSL were wide, wide open to exploitation, even thought the applications themselves (e.g. Web servers) were very well written implementations of, say, HTTPS.\nNow, had the designers of TLS chosen to use a decent and strict serialisation technology - perhaps even Google Protocol Buffers (plus some message demarcation) - to define the PDUs in TLS, it would have been far more likely that Heartbleed wouldn&#39;t have happened. Specifically, the  field in a request / response would have been taken care of inside Google Protocol Buffers, thereby removing responsibility for handling the length of the  from the developer.\nWhat&#39;s interesting is to compare protocol specifications as written in RFCs with those that tend to be found in the world of telephony (regulated by the International Telephony Union). The ITU&#39;s specifications and tools are very &quot;comprehensive&quot; (that ought to be an acceptably neutral way of describing them). A lot of telephony uses ASN.1, which is not disimilar to (and substantially pre-dates) Google Protocol Buffers, but allows for very strict definitions of messages, requires pretty comprehensive tools to do it right, but is bang up to date (it even has JSON as a wire format these days).\n&quot;But&quot;, one points out, &quot;what if the ASN.1 tools (or Google Protocol Buffers) has a bug?&quot;. Well indeed that is a problem, and that has indeed happened to ASN.1 (from one of the commercial ASN.1 tools vendors, can&#39;t rememeber which). But the point is that if there&#39;s one library that is widely used for defining lots of interfaces, then there&#39;s a greater chance of bugs being identified (I myself have found and reported bugs in commercial ASN.1 tools). Whereas if a messaging protocol is defined using, say, English language, there&#39;s only ever going to be a very few set of eyes on how well the developer has coded up the meaning of that English language.\nNot Everyone Has Got the Message\nWhat I find disappointing is that, across a large portion of the software world, there&#39;s still resistance to using tools like Google Protocol Buffers, ASN.1. There&#39;s also projects that, having identified the need for such things, go and invent their own.\nOne such example is dBus - which to be fair is pretty good. However they did go an invent their own serialisation technology for specifying dBus messages; I&#39;m not sure what they gained over using something mature and off-the-shelf.\nGoogle themselves, when they first announced Google Protocol Buffers to the world, were asked &quot;Why didn&#39;t you use ASN.1?&quot;, and the Googler on the stage had to admit to never having heard of it. So, Googlers in Google hadn&#39;t used Google to Google for &quot;binary serialisation technologies&quot;; they&#39;d just gone ahead and wrote their own, and GPB is missing a ton of useful features. Oh, the irony. They&#39;d not even have had to write a toolset from scratch; they could have simply adopted and improved on one of the open source ASN.1 implementations.\nTransliteration Problem\nThis fragmentation and proliferation causes problems. Say, for example, in your project you want to be able to transfer some of your messages into a dBus service on Linux. To do that, you&#39;ve got a .proto defining your messages, which is great for communicating in/out of Tensor Flow, but fundamentally useless for dBus, which speaks its own format. You&#39;d end up having something like\n\nand so on. Very laborious, very unmaintainable, and needlessly consumes resources. The other option would be simply to wrap up your GPB encoded messages in a byte array in a dBus message, but one feels that&#39;s missing the point (it bypasses any opportunity for dBus to assert that messages it&#39;s passing are correctly formed and within specifications).\nIf the world agreed on the One True Serialisation technology then the flexibility in object / message exchange would be fantastic.\n"", ""excerpt"": ""There&#39;s no point using, say, Protocol Buffers and declaring oneself <span class=\""highlight\"">safe</span> against buffer overflows, if the OS&#39;s IP stack is broken and exploitable. &hellip; To do that, you&#39;ve got a .proto defining your messages, which is great for communicating in/out of <span class=\""highlight\"">Tensor</span> Flow, but fundamentally useless for dBus, which speaks its own format. &hellip; "", ""title"": ""What is a protobuf message?""}"

Runtime ~100X higer when return a graph with tf.function and serving,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""serving""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 71492171, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1647408628, ""creation_date"": 1647408628, ""body"": ""I have a simple tf.function to calculate sum of entries from a mask; noticed runtime is around .25s on my laptop, each time the function is called for serving (after warmup).\nwhen I  instead of  the runtime is 3 orders lower, ~0.003. is it safe to assume this is because by returning the tensor it tf calls graph to value conversion which in turn takes runtime?\nmore importantly, any thoughts on improving the runtime while enabling access to value when function is called for serving?\nthank you.\n\n"", ""excerpt"": ""entries from a mask; noticed runtime is around .25s on my laptop, each time the function is called for serving (after warmup).\nwhen I return 0 instead of my_var the runtime is 3 orders lower, ~0.003. is it <span class=\""highlight\"">safe</span> &hellip; to assume this is because by returning the <span class=\""highlight\"">tensor</span> it tf calls graph to value conversion which in turn takes runtime? &hellip; "", ""title"": ""Runtime ~100X higer when return a graph with tf.function and serving""}"

"Deep Q-Learning model, Neural Network won&#39;t train",Stack Overflow,N/A,"{""tags"": [""python"", ""machine-learning"", ""deep-learning"", ""neural-network"", ""pytorch""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 70485191, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1640518219, ""creation_date"": 1640508119, ""body"": ""I am trying to train a DQN tic-tac-toe game model using pytorch.\n\nOur state is a 1x9 tensor or our game board and actions include all the playable moves (which includes 1 to 8 positions in our board). Only after both players making a move we try to get our next state\n\nPlayer 1(which plays first) is our DQN agent and Player -1 is a random bot the plays a random valid move(move that is not played before), also the board is a 1x9 tensor containing all zero values(0 corresponds to a blank cell, 1 for our agent and -1 for random bot)\n\nthere are 2 neural networks, (a) Policy network(network that is trained) (b) Target network(from where we fetch next state q values)\n\nthe reward system rewards Player X with +1 if it wins, -1 if it loses, 0 for a draw, and a reward of 0.5 is given for every safe move played(which I think should be eliminated)\n\nfor a certain batch size initially, every move played with its associated reward and next state are stored in a replay memory for later training\n\nam using epsilon greedy strategy meaning the first few moves taken by our agent are going to be random but after a certain decay it will start taking actions based on its policy network\n\n\n\nthis is my neural network class, DQN\nThe problem am facing is that even after training the model, it is choosing the same action for every state-next state pairs.\n\n is used to store states and then train the network\n\nand this is where the agent is choosing the same action for every possible state, the line  prints the same action value for every state.\ngithub link to my project\n"", ""excerpt"": ""Our state is a 1x9 <span class=\""highlight\"">tensor</span> or our game board and actions include all the playable moves (which includes 1 to 8 positions in our board). &hellip; that is trained) (b) Target network(from where we fetch next state q values)\n\nthe reward system rewards Player X with +1 if it wins, -1 if it loses, 0 for a draw, and a reward of 0.5 is given for every <span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""Deep Q-Learning model, Neural Network won&#39;t train""}"

How do I find the length of a vocabulary computed during TFX Transform?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""tensor"", ""vocabulary"", ""tfx""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 69667619, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1634931725, ""creation_date"": 1634843763, ""body"": ""I&#39;m currently building a project in TFX and during the Transform step I compute the &quot;vocabulary&quot; for a categorical variable. For later steps (but still during preprocessing), I want to use the length of that vocabulary (i.e. the number of distinct categories) to do related transformations.  The number of categories is generated by an external process with items often being added (and rarely removed), and the model needs to be repeatedly trained with up-to-date data. (I&#39;m training a multilabel classifier, and the output set of categories should match the input set, so I&#39;m creating a SparseVector representation of the labels.) Therefore, I cannot set the vocab length at compile time.  I need it to be computed when the model is trained. (After training, any additional categories can be safely filed as &quot;unknown.&quot;)\nHow do I get the length of the vocab?\nThings I&#39;ve tried with  and  being the input set of values coming from ExampleGen.\n\nI&#39;m fairly sure that  call is right or close to it, but the &quot;vocab_len&quot; is probably wrong.  The &quot;vocab_uri&quot; variable name is a bit of a misnomer, since it appears to be a relative path... and probably a logical path involving Protobuf.  I haven&#39;t so far been able to find it stored on disk, but it&#39;s possible I&#39;ve been looking in the wrong subdirectory of &quot;pipeline_output/transform/&quot;.  It&#39;s also possible it doesn&#39;t get written out until the Transform is complete.\nSo I tried to find a vocabulary_length function which seemed compatible. In this iteration I&#39;ve been working mainly with , but I&#39;m not convinced it&#39;s the right function, or that I&#39;m calling it properly.  Its output is a Tensor which I am having trouble turning into an actual, instantiated integer within the graph-mode execution of the Transform. Interpretations / manipulations I&#39;ve tried:\n\nRaw value (as coded above) throws an error in downstream code that expects an actual numerical value and doesn&#39;t know how to handle a Tensor.\n crashes with &quot;AttributeError: &#39;Tensor&#39; object has no attribute &#39;numpy&#39;&quot;.\n returns None.\n crashes with &quot;ValueError: Cannot evaluate tensor using : No default session is registered. Use  or pass an explicit session to &quot; I&#39;m using TF 2.3.3, and Sessions don&#39;t appear to have existed since TF 1.x. The  class I&#39;ve seen reference to in docs doesn&#39;t appear to exist in TF 2.x.\nwrapping the  and  assignments in a  block. (The &quot;ctxt&quot; Context was initialized and had SYNC execution mode and was from this Tensorflow module.) That also complained that  was just a  Tensor.\n\nI&#39;ve found that I can use TFTransformOutput in later components, but it seems unlikely to work when I&#39;m still in the middle of the Transform itself... and even if it did, I don&#39;t know what argument to pass it.\nI also tried slicing , but got &quot;ValueError: Cannot convert a partially known TensorShape to a Tensor&quot;\nThis feels like it should be a straightforward use-case, but I haven&#39;t found helpful docs or code.\nIdeas?\n"", ""excerpt"": ""() crashes with &quot;AttributeError: &#39;<span class=\""highlight\"">Tensor</span>&#39; object has no attribute &#39;numpy&#39;&quot;.\ntensorflow.get_static_value(vocab_len) returns None.\nvocab_len.eval() crashes with &quot;ValueError: Cannot evaluate <span class=\""highlight\"">tensor</span> using &hellip; That also complained that vocab_len was just a PlaceholderWithDefault:0 <span class=\""highlight\"">Tensor</span>. &hellip; "", ""title"": ""How do I find the length of a vocabulary computed during TFX Transform?""}"

Sigmoid vs Binary Cross Entropy Loss,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""loss-function"", ""sigmoid"", ""automatic-mixed-precision""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 69454806, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1633461353, ""creation_date"": 1633454561, ""body"": ""In my torch model, the last layer is a  and the loss is the .\nIn the training step, the following error has occurred:\n\nHowever, when trying to reproduce this error while computing the loss and backpropagation, everything goes correctly:\n\nWhat am i missing?\nI appreciate any help you can provide.\n"", ""excerpt"": ""binary_cross_entropy_with_logits and BCEWithLogits are\n<span class=\""highlight\"">safe</span> to autocast. &hellip; &gt;)\n\nout = bce_loss(pred_cls, true_cls)\nout\n# <span class=\""highlight\"">tensor</span>(0.7258, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)\n\nout.backward()\n\nWhat am i missing? &hellip; "", ""title"": ""Sigmoid vs Binary Cross Entropy Loss""}"

Overload operators on different templated types with C++ concepts,Stack Overflow,N/A,"{""tags"": [""c++"", ""templates"", ""linear-algebra"", ""c++20"", ""derived-class""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 68244957, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1625614298, ""creation_date"": 1625403591, ""body"": ""I am trying to provide out-of-class definitions of arithmetic operators  (and in-place  etc.) for differently templated types. I read that C++20 concepts is the good way to go, as one could constrain the input/output type to provide only one templated definition, although I could not find much examples of this...\nI am using a type-safe vector as base class:\n\nI have a derived class for tensors like so:\n\nand I shall also define a derived class for read-only views/slices, overriding  to jump accross strides. I&#39;d like to hard code little more than  and  methods inside each class and avoid reproducing boilerplate code as much as possible.\nI first had a bit of trouble coming up with a suitable concept for -like classes due to different template parameters, but the one below seems to work:\n\n\nTry 2:\nBased on this question I figure out-of-class definition has to be a little more verbose, so I added a couple lines to .\nThis seems contrived as it would require (3 * N_operators) type signature definitions, where avoiding code duplication is what motivates this question. Plus I don&#39;t really understand what the  keyword is doing here.\n\n\nI am guessing the compiler is complaining about implementation being defined in  instead of ?\nQuestion: What is the correct C++ way to do this? Are there any ways to make the compiler happy e.g. with header files?\nI am really looking for DRY answers here, as I know the code would work with a fair amount of copy-paste :)\nThanks!\n"", ""excerpt"": ""I am using a type-<span class=\""highlight\"">safe</span> vector as base class:\n// vect.cpp\ntemplate&lt;size_t n, typename T&gt; \nstruct Vect {\n    \n    Vect(function&lt;T(size_t)&gt; f) {\n        for (size_t i=0; i &lt; n; i++) {\n            values[i &hellip; }\n    }\n    \n    T values [n];\n\n    T operator[] (size_t i) {\n        return values[i];\n    }\n}\n\nI have a derived class for tensors like so:\n// tensor.cpp\ntemplate &lt;typename shape, typename T&gt;\nstruct <span class=\""highlight\"">Tensor</span> &hellip; "", ""title"": ""Overload operators on different templated types with C++ concepts""}"

ValueError: setting an array element with a sequence. ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type Series),Stack Overflow,N/A,"{""tags"": [""python"", ""arrays"", ""pandas"", ""numpy"", ""tensorflow""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 68175469, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1624956663, ""creation_date"": 1624956663, ""body"": ""I want to extract signals from time series data for machine learning using tensorflow. I got this error when I try to run the program.\n\n/Users/renzha/Library/Application Support/JetBrains/PyCharmCE2021.1/scratches/pywavelet.py:38: DeprecationWarning:  is a deprecated alias for the builtin . To silence this warning, use  by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use  here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\nX = np.asarray(X).astype(np.float)\nTraceback (most recent call last):\nFile &quot;/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/series.py&quot;, line 129, in wrapper\nraise TypeError(f&quot;cannot convert the series to {converter}&quot;)\nTypeError: cannot convert the series to &lt;class &#39;float&#39;&gt;\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile &quot;/Users/renzha/Library/Application Support/JetBrains/PyCharmCE2021.1/scratches/pywavelet.py&quot;, line 38, in \nX = np.asarray(X).astype(np.float)\nValueError: setting an array element with a sequence.\n\nHowever, if I use , then the error will be\n\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type Series).\n\nI have tried , but it will return the same error.\nThe code is here:\n\n"", ""excerpt"": ""Doing this will not modify any behavior and is <span class=\""highlight\"">safe</span>. If you specifically wanted the numpy scalar type, use np.float64 here. &hellip; However, if I use X = np.array(X), then the error will be\n\nValueError: Failed to convert a NumPy array to a <span class=\""highlight\"">Tensor</span> (Unsupported object type Series). &hellip; "", ""title"": ""ValueError: setting an array element with a sequence. ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type Series)""}"

"Estimator.predict() TypeError: Expected any non-tensor type, got a tensor instead",Stack Overflow,N/A,"{""tags"": [""python"", ""python-3.x"", ""tensorflow"", ""machine-learning""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": false, ""question_id"": 67485998, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1620742528, ""creation_date"": 1620733165, ""body"": ""I try to predict the word by lip reading from a video with a VGG neural network using  and I can&#39;t manage to get the predictions from the estimator.\nThe model is custom made and I didn&#39;t have any problem in training or evaluation.\nHere is the code:\n\nThe line  throws  and I can&#39;t find any alternative to this.\n doesn&#39;t work either.\nFull error log:\n\nAnybody had this kind of problem?\n"", ""excerpt"": ""283        if isinstance(v, ops.Tensor)]\n    284 # pylint: enable=invalid-name\n\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py in _check_failed(v)\n    249   # it is <span class=\""highlight\"">safe</span> &hellip; type, got a <span class=\""highlight\"">tensor</span> instead.&quot;)\n    335     else:\n    336       raise TypeError(&quot;Expected %s, got %s of type &#39;%s&#39; instead.&quot; %\n\nTypeError: Expected any non-<span class=\""highlight\"">tensor</span> type, got a <span class=\""highlight\"">tensor</span> instead. &hellip; "", ""title"": ""Estimator.predict() TypeError: Expected any non-tensor type, got a tensor instead""}"

What is the right way to generate long sequence using PyTorch-Transformers?,Stack Overflow,N/A,"{""tags"": [""deep-learning"", ""nlp"", ""pytorch"", ""huggingface-transformers"", ""transformer-model""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 67381993, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1620137867, ""creation_date"": 1620119976, ""body"": ""I am trying to generate a long sequence of text using PyTorch-Transformers from a sample text. I am following this tutorial for this purpose. Because the original article only predicts one word from a given text, I modified that script to generate long sequence instead of one. This is the modified part of the code\n\nOutput\n\nAn examination can be defined as a detailed inspection or analysis\nof an object or person. For example, an engineer will examine a\nstructure,   like a bridge, to see if it is safe. A doctor may conduct\nan examination of a patient&#39;s body to see if it is safe.\nThe doctor may also examine a patient&#39;s body to see if it is safe. A\ndoctor may conduct an examination of a patient&#39;s body to see if it is\nsafe.\n\nAs you can see the generated text does not generates any unique text sequence but it generates the same sentence over and over again with minor changes.\nHow should we create long sequence using  PyTorch-Transformers?\n"", ""excerpt"": ""A doctor may conduct&quot;&quot;&quot;\n\nindexed_tokens = tokenizer.encode(text)\n\n# Convert indexed tokens in a PyTorch <span class=\""highlight\"">tensor</span>\ntokens_tensor = torch.tensor([indexed_tokens])\nseq_len = tokens_tensor.shape[1]\ntokens_tensor &hellip; The doctor may also examine a patient&#39;s body to see if it is <span class=\""highlight\"">safe</span>. A\ndoctor may conduct an examination of a patient&#39;s body to see if it is\n<span class=\""highlight\"">safe</span>. &hellip; "", ""title"": ""What is the right way to generate long sequence using PyTorch-Transformers?""}"

Different `grad_fn` for similar looking operations in Pytorch (1.0),Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""attention-model""], ""question_score"": 8, ""is_accepted"": false, ""answer_id"": 67371068, ""is_answered"": false, ""question_id"": 55835557, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1620053596, ""creation_date"": 1620053596, ""body"": ""\nAren&#39;t these two the same operations?\n\nNo. While they produce effectively the same tensor, the operations are not the same, and they are not guaranteed to have the same .\nTensorShape.cpp:\n\nNote this can produce an error if applied to complex inputs, but this is generally not yet fully supported in PyTorch and not unique to this function.\n"", ""excerpt"": ""While they produce effectively the same <span class=\""highlight\"">tensor</span>, the operations are not the same, and they are not guaranteed to have the same storage. &hellip; It&#39;s only <span class=\""highlight\"">safe</span> to use if the `self` <span class=\""highlight\"">tensor</span>\n// is temporary. &hellip; "", ""title"": ""Different `grad_fn` for similar looking operations in Pytorch (1.0)""}"

Why use Variable() in inference?,Stack Overflow,N/A,"{""tags"": [""python"", ""deep-learning"", ""computer-vision"", ""pytorch"", ""image-classification""], ""question_score"": 2, ""is_accepted"": true, ""answer_id"": 67152650, ""is_answered"": false, ""question_id"": 67152554, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1618774427, ""creation_date"": 1618774427, ""body"": ""You can safely omit it. Variables are a legacy component of PyTorch, now deprecated, that used to be required for autograd:\n\n (deprecated)\nWARNING\nThe  API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with  set to . Below please find a quick guide on what has changed:\n\n and  still work as expected, but they return Tensors instead of Variables.\n\n\n"", ""excerpt"": ""Below please find a quick guide on what has changed:\n\nVariable(<span class=\""highlight\"">tensor</span>) and Variable(<span class=\""highlight\"">tensor</span>, requires_grad) still work as expected, but they return Tensors instead of Variables. &hellip; "", ""title"": ""Why use Variable() in inference?""}"

What does .contiguous() do in PyTorch?,Stack Overflow,N/A,"{""tags"": [""python"", ""memory"", ""pytorch"", ""contiguous""], ""question_score"": 237, ""is_accepted"": true, ""answer_id"": 52229694, ""is_answered"": false, ""question_id"": 48915810, ""item_type"": ""answer"", ""score"": 370, ""last_activity_date"": 1618485277, ""creation_date"": 1536354835, ""body"": ""There are a few operations on Tensors in PyTorch that do not change the contents of a tensor, but change the way the data is organized. These operations include:\n\n, ,  and \n\nFor example: when you call , PyTorch doesn&#39;t generate a new tensor with a new layout, it just modifies meta information in the Tensor object so that the offset and stride describe the desired new shape. In this example, the transposed tensor and original tensor share the same memory:\n\nThis is where the concept of contiguous comes in. In the example above,  is contiguous but  is not because its memory layout is different to that of a tensor of same shape made from scratch. Note that the word &quot;contiguous&quot; is a bit misleading because it&#39;s not that the content of the tensor is spread out around disconnected blocks of memory. Here bytes are still allocated in one block of memory but the order of the elements is different!\nWhen you call , it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\nNormally you don&#39;t need to worry about this. You&#39;re generally safe to assume everything will work, and wait until you get a  where PyTorch expects a contiguous tensor to add a call to .\n"", ""excerpt"": ""in the <span class=\""highlight\"">Tensor</span> object so that the offset and stride describe the desired new shape. &hellip; You&#39;re generally <span class=\""highlight\"">safe</span> to assume everything will work, and wait until you get a RuntimeError: input is not contiguous where PyTorch expects a contiguous <span class=\""highlight\"">tensor</span> to add a call to contiguous(). &hellip; "", ""title"": ""What does .contiguous() do in PyTorch?""}"

In-place operations with PyTorch,Stack Overflow,N/A,"{""tags"": [""python"", ""neural-network"", ""deep-learning"", ""pytorch"", ""autograd""], ""question_score"": 19, ""is_accepted"": false, ""answer_id"": 66549988, ""is_answered"": false, ""question_id"": 51818163, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1616127322, ""creation_date"": 1615304204, ""body"": ""This may be not a direct answer to your question, but just for information.\nIn-place operations work for non-leaf tensors in a computational graph.\nLeaf tensors are tensors which are the &#39;ends&#39; of a computational graph.  Officially (from  attribute here),\n\nFor Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.\n\nExample which works without error:\n\nOn the other hand, in-place operations do not work for leaf tensors.\nExample which causes error:\n\nI suppose that  operation, in the first example above, is not really an in-place operation.  I suppose that it creates a new tensor with &quot;CopySlices&quot; operation.  The &#39;old b&#39; before the in-place operation might be kept internally (just its name being overwritten by the &#39;new b&#39;).  I found a nice figure here.\nold b  ---(CopySlices)----&gt;  new b\nOn the other hand, the tensor  is a leaf tensor.  After the CopySlices operation , it becomes an intermediate tensor.  To avoid such a complicated mixture between leaf tensors and intermediate tensors when back propagating, CopySlices operation on leaf tensors is prohibited from coexisting with backward.\nThis is merely my personal opinion, so please refer to official documents.\nNote:\nAlthough in-place operations work for intermediate tensors, it will be safe to use  clone and detach as much as possible when you do some in-place operations, to explicitly create a new tensor which is independent of the computational graph.\n"", ""excerpt"": ""&gt;)\nb[1] = 0\nprint(b)   # <span class=\""highlight\"">tensor</span>([ 9.,  0., 49.], grad_fn=&lt;CopySlices&gt;)\nc = torch.sum(2*b)\nprint(c)   # <span class=\""highlight\"">tensor</span>(116., grad_fn=&lt;SumBackward0&gt;)\nc.backward()\nprint(a.grad)  # <span class=\""highlight\"">tensor</span>([12.,  0., 28.]) &hellip; Note:\nAlthough in-place operations work for intermediate tensors, it will be <span class=\""highlight\"">safe</span> to use  clone and detach as much as possible when you do some in-place operations, to explicitly create a new <span class=\""highlight\"">tensor</span> which &hellip; "", ""title"": ""In-place operations with PyTorch""}"

Switch off CUDA in PyTorch without uninstalling,Stack Overflow,N/A,"{""tags"": [""python"", ""anaconda"", ""pytorch""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 66399048, ""is_answered"": false, ""question_id"": 65865854, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1614478432, ""creation_date"": 1614430673, ""body"": ""No, you have to resolve your dependencies ( might automatically set GPU to , maybe you could try to catch this error during import, but this is pointless and terrible thing to do as some parts of libraries might not be initialized or not work at all due to aforementioned). Recreate the environment or  both packages and follow below (I assume , you can also go for  (maybe with  inside))\nCPU only\nI would advise against that, use CUDA if you can. As mentioned, you can simply not use  calls on your tensors/models\nInstall PyTorch (as per this tutorial), via  it would be:\n\nSame for  (I assume this is your dependency, link here):\n\nPlease notice the same  version  in both commands\nGPU\nAlmost the same (default is ):\n\nAnd for :\n\nIt should work with PyTorch  not  as above, but you can follow these steps just to be on the safe side.\nAlso here are available pytorch-geometric versions you can use.\n"", ""excerpt"": ""torch_cluster:\npip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html\n\nIt should work with PyTorch 1.7.1 not 1.7.0 as above, but you can follow these steps just to be on the <span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""Switch off CUDA in PyTorch without uninstalling""}"

Cascade multiple RNN models for N-dimensional output,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""keras"", ""recurrent-neural-network"", ""tf.keras""], ""question_score"": 6, ""is_accepted"": true, ""answer_id"": 63222778, ""is_answered"": false, ""question_id"": 63157402, ""item_type"": ""answer"", ""score"": 5, ""last_activity_date"": 1612736235, ""creation_date"": 1596419737, ""body"": ""As this question has multiple major parts, I&#39;ve dedicated a Q&amp;A to the core challenge: stateful backpropagation. This answer focuses on implementing the variable output step length.\n\nDescription:\n\nAs validated in Case 5, we can take a bottom-up first approach. First we feed the complete input to  (A) - then, feed its outputs as input to  (B), but this time one step at a time.\nNote that we must chain B&#39;s output steps per A&#39;s input step, not between A&#39;s input steps; i.e., in your diagram, gradient is to flow between  and , but not between  and .\nFor computing loss it won&#39;t matter whether we use a ragged or padded tensor; we must however use a padded tensor for writing to TensorArray.\nLoop logic in code below is general; specific attribute handling and hidden state passing, however, is hard-coded for simplicity, but can be rewritten for generality.\n\nCode: at bottom.\n\nExample:\n\nHere we predefine the number of iterations for B per input from A, but we can implement any arbitrary stopping logic. For example, we can take a  layer&#39;s output from B as a hidden state and check if its L2-norm exceeds a threshold.\nPer above, if  is unknown to us, we can simply set it, which is common for NLP &amp; other tasks with a STOP token.\n\nAlternatively, we may write to separate  at every A&#39;s input with ; see &quot;point of uncertainty&quot; below.\n\n\nA valid concern is, how do we know gradients flow correctly? Note that we&#39;ve validate them for both vertical and horizontal in the linked Q&amp;A, but it didn&#39;t cover multiple output steps per an input step, for multiple input steps. See below.\n\n\nPoint of uncertainty: I&#39;m not entirely sure whether gradients interact between e.g.  and . I did, however, verify that gradients will not flow horizontally if we write to separate s for B&#39;s outputs per A&#39;s inputs (case 2); reimplementing for cases 4 &amp; 5, grads will differ for both models, including lower one with a complete single horizontal pass.\nThus we must write to a unified . For such, as there are no ops leading from e.g.  to , I can&#39;t see how TF would trace it as such - so it seems we&#39;re safe. Note, however, that in below example, using  will make gradient flow in the both model horizontally, as we&#39;re writing to a single  and passing hidden states.\nThe examined case is confounded, however, with B being stateful at all steps; lifting this requirement, we might not need to write to a unified  for all , , etc, but we must still test against something we know works, which is no longer as straightforward.\n\nExample [code]:\n\n\nMethods:\nNot the cleanest, nor most optimal code, but it works; room for improvement.\nMore importantly: I implemented this in Eager, and have no idea how it&#39;ll work in Graph, and making it work for both can be quite tricky. If needed, just run in Graph and compare all values as done in the &quot;cases&quot;.\n\n"", ""excerpt"": ""For computing loss it won&#39;t matter whether we use a ragged or padded <span class=\""highlight\"">tensor</span>; we must however use a padded <span class=\""highlight\"">tensor</span> for writing to TensorArray. &hellip; IR[1] to Out[0][1], I can&#39;t see how TF would trace it as such - so it seems we&#39;re <span class=\""highlight\"">safe</span>. &hellip; "", ""title"": ""Cascade multiple RNN models for N-dimensional output""}"

Why do I always get the same value as the result in a CNN in pytorch?,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""classification"", ""conv-neural-network""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 65926353, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1611776913, ""creation_date"": 1611776913, ""body"": ""Here is my code\n\nSo i am using pytorch CNN to classify 60k images in 2 classes. When i print the output after the model has trained, whatever the image as input, the ouput is always &quot;tensor([[0.6384]], grad_fn=)&quot;. Always the same value. So it predicts only 1 (because it&#39;s greater than 0.5). The thing is, when i print the ouput while training, the results vary (16, 1 , 0, 4 ,0.6 etc) but when i print the output (with the same model but not trained) the results don&#39;t vary that much (0.5, 0.51, 0.49 0.52, 0.55). So I think it&#39;s safe to say that it is converging to a single value. I just don&#39;t know why. what could i do differently?\n"", ""excerpt"": ""When i print the output after the model has trained, whatever the image as input, the ouput is always &quot;<span class=\""highlight\"">tensor</span>([[0.6384]], grad_fn=)&quot;. Always the same value. &hellip; So I think it&#39;s <span class=\""highlight\"">safe</span> to say that it is converging to a single value. I just don&#39;t know why. what could i do differently? &hellip; "", ""title"": ""Why do I always get the same value as the result in a CNN in pytorch?""}"

Prediction classes not showing in app (Warning: Each child in a list should have a unique &quot;key&quot; prop.),Stack Overflow,N/A,"{""tags"": [""javascript"", ""class""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 65278823, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1607956130, ""creation_date"": 1607882000, ""body"": ""I made a plant leaf diseases classification using transfer learning MobileNet, i saved my model as h5 the converted it into .js, in the app I changed classes.js to fit the SoftMax of my model which was 38 classes.\nActual Result : When I run the app it does not show the classes image2 , it jus shows the probabilities , but interestingly when I put classes.js with ImageNet classes , classes appear in app .\nwhere am I going wrong ? How do I get classes from my model to appear in the predictions .\nThe expected result: is to have the app show both classes(From my class disease softmax output) and probability % .\nI have attaches files below.\n[\nActual result\n[\nresult with imagenet classes.js\n\n\r\n\r\n\r\n\r\n\r\n\n"", ""excerpt"": ""isDownloadingModel: false\n    };\n  }\n\n  async componentDidMount() {\n    if ((&#39;indexedDB&#39; in window)) {\n      try {\n        this.model = await tf.loadLayersModel(&#39;indexeddb://&#39; + INDEXEDDB_KEY);\n\n        // <span class=\""highlight\"">Safe</span> &hellip; * @param logits <span class=\""highlight\"">Tensor</span> representing the logits from MobileNet. &hellip; "", ""title"": ""Prediction classes not showing in app (Warning: Each child in a list should have a unique &quot;key&quot; prop.)""}"

"WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2",Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""machine-learning"", ""keras"", ""deep-learning""], ""question_score"": 6, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 59400128, ""item_type"": ""question"", ""score"": 6, ""last_activity_date"": 1606756912, ""creation_date"": 1576704035, ""body"": ""Before my Tensorflow neural network starts training, the following warning prints out:\n\nWARNING:tensorflow:Layer my_model is casting an input tensor from\ndtype float64 to the layer&#39;s dtype of float32, which is new behavior\nin TensorFlow 2.  The layer has dtype float32 because it&#39;s dtype\ndefaults to floatx. If you intended to run this layer in float32, you\ncan safely ignore this warning.\n\n\nIf in doubt, this warning is likely\nonly an issue if you are porting a TensorFlow 1.X model to TensorFlow\n2. To change all layers to have dtype float64 by default, call .\n\n\nTo change just this layer,\npass dtype=&#39;float64&#39; to the layer constructor. If you are the author\nof this layer, you can disable autocasting by passing autocast=False\nto the base Layer constructor.\n\nNow, based on the error message, I am able to silence this error message by setting the backend to . But, I would like to get to the bottom of this and set the right  manually.\nFull code:\n\n"", ""excerpt"": ""Before my Tensorflow neural network starts training, the following warning prints out:\n\nWARNING:tensorflow:Layer my_model is casting an input <span class=\""highlight\"">tensor</span> from\ndtype float64 to the layer&#39;s dtype of float32, &hellip; "", ""title"": ""WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2""}"

extend array intervals of arrays by the same array,Stack Overflow,N/A,"{""tags"": [""python"", ""arrays"", ""numpy"", ""tensorflow"", ""data-science""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 64977132, ""is_answered"": false, ""question_id"": 64972152, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1606176707, ""creation_date"": 1606168763, ""body"": ""IIUC, you have an  tensor which comprises of  objects, some of which are interval objects. These interval objects each have 4 more features apart from the 6 features already available. The tensor that holds these 4 features is , where m&lt;n and m = the number of interval objects among n objects.\nAssuming that these intervals start from the 0th object AND they are repeating baseds on a given number of repeatitions (interval lengths) I can safely say that the structure is following -\n\nAssuming that you want to simply copy the additional information to the next few objects until the subsequent interval is reached, you basically will fill the gaps between the  intervals such that now you have  objects with additional info.\nYou can do this with an . You can calculate how many times to repeat each of the  objects from your intervals list and store it in . A detailed explanation about this is in the last paragraph.\n\n\nThis would do the following -\n\n\nEDIT: I have updated my answer based on your inputs. Since the intervals can be of variable length, you can still simply use  but just pass another parameter  which tells it how many times to repeat each element. This can be calculated from your list of intervals. The length of this  array needs to be equal to  objects so it can tell how many times each of these objects need to be repeated respectively. AND, the sum of  needs to be equal to  since the output array after repeating needs to be have same objects as the original array = \n\n"", ""excerpt"": ""The <span class=\""highlight\"">tensor</span> that holds these 4 features is (a,m,4), where m&lt;n and m = the number of interval objects among n objects. &hellip; #Total number of objects created after \n#repetitions = 10 = objects in original <span class=\""highlight\"">tensor</span>. &hellip; "", ""title"": ""extend array intervals of arrays by the same array""}"

"PyTorch BatchNorm1D, 2D, 3D and TensorFlow/Keras BatchNormalization",Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""keras"", ""pytorch""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 64630264, ""is_answered"": false, ""question_id"": 64629522, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1604221878, ""creation_date"": 1604221878, ""body"": ""It depends on your ordering of dimensions.\nPytorch does its batchnorms over axis=1.\nBut it also has tensors with axis=1 as channels for convolutions.\nTensorflow has has channels in the last axis in convolution. So its batchnorm puts them in axis=-1.\nIn most cases you should be safe with the default setting.\n"", ""excerpt"": ""In most cases you should be <span class=\""highlight\"">safe</span> with the default setting. &hellip; "", ""title"": ""PyTorch BatchNorm1D, 2D, 3D and TensorFlow/Keras BatchNormalization""}"

"TypeError: Expected any non-tensor type, got a tensor instead",Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""keras"", ""deep-learning"", ""chatbot"", ""transformer-model""], ""question_score"": 5, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 62608037, ""item_type"": ""question"", ""score"": 5, ""last_activity_date"": 1599553351, ""creation_date"": 1593249091, ""body"": ""I Was following a post on &#39;Training a transformer model for a chatbot with TensorFlow 2.0&#39;. I have encountered an error on my local machine although the code seems to work fine in colab. Below is the code snippet.\n\nI called above function with the following function call;\n\nBelow is the traceback of the error:\n\n"", ""excerpt"": ""isinstance(v, ops.Tensor)]\n    278 # pylint: enable=invalid-name\n\n~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in _check_failed(v)\n    247   # it is <span class=\""highlight\"">safe</span> &hellip; type, got a <span class=\""highlight\"">tensor</span> instead.&quot;)\n    329     else:\n    330       raise TypeError(&quot;Expected %s, got %s of type &#39;%s&#39; instead.&quot; %\n\nTypeError: Expected any non-<span class=\""highlight\"">tensor</span> type, got a <span class=\""highlight\"">tensor</span> instead. &hellip; "", ""title"": ""TypeError: Expected any non-tensor type, got a tensor instead""}"

Does it matter what dtype a numpy array is for input into a tensorflow/keras neural network?,Stack Overflow,N/A,"{""tags"": [""python"", ""numpy"", ""tensorflow"", ""machine-learning"", ""keras""], ""question_score"": 3, ""is_accepted"": true, ""answer_id"": 63424921, ""is_answered"": false, ""question_id"": 63424782, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1597487390, ""creation_date"": 1597486840, ""body"": ""You should cast your input to , that&#39;s the default dtype for Keras. Look it up:\n\n\nIf you give Keras input in , it will complain:\n\n\nWARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it&#39;s dtype defaults to floatx.\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\nTo change all layers to have dtype float64 by default, call . To change just this layer, pass dtype=&#39;float64&#39; to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nIt is possible to use Tensorflow for training with 8bit input, which is called quantization. But it is challenging and unnecessary in most cases (i.e., unless you need to deploy your models on edge devices).\ntl;dr keep your input in . See also this post.\n"", ""excerpt"": ""activation=&#39;linear&#39;)\n\n  def call(self, x):\n    x = self.d0(x)\n    x = self.d1(x)\n    x = self.d2(x)\n    return x\n\nmodel = MyModel()\n\n_ = model(X)\n\n\nWARNING:tensorflow:Layer my_model is casting an input <span class=\""highlight\"">tensor</span> &hellip; "", ""title"": ""Does it matter what dtype a numpy array is for input into a tensorflow/keras neural network?""}"

Saving the dictionary with str keys and a list of pytorch tensors as values,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""saving-data""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 63089147, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1596014193, ""creation_date"": 1595684997, ""body"": ""I want to save a list of dictionary in which keys are indices of queries (so the keys of the dictionary are strings). The values of this dictionary are list of sparse tensors which I store as follows :\n\nI&#39;ve not saved any such dictionary before. Generally for normal dictionaries, we pickle it. If I pickle this dictionary, I&#39;m getting a pytorch warning. This makes me doubt the method I&#39;m using to save the dictionary. What extensions should I use to save such dictionary? Is pickling such dictionary safe? What method should I use to save it? Thanks in advance.\n"", ""excerpt"": ""<span class=\""highlight\"">tensor</span>(indices=<span class=\""highlight\"">tensor</span>([[0, 1, 2]]),\n         values=<span class=\""highlight\"">tensor</span>([1., 1., 1.]),\n         size=(30000,), nnz=3, layout=torch.sparse_coo),\n  <span class=\""highlight\"">tensor</span>(indices=<span class=\""highlight\"">tensor</span>([[  36,   99,  243,  428,  513,  514,  741,  751 &hellip; Is pickling such dictionary <span class=\""highlight\"">safe</span>? What method should I use to save it? Thanks in advance. &hellip; "", ""title"": ""Saving the dictionary with str keys and a list of pytorch tensors as values""}"

Create Tensor with images and lable jpg vs png,Stack Overflow,N/A,"{""tags"": [""python"", ""arrays"", ""numpy"", ""tensorflow"", ""conv-neural-network""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 62992669, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1595238104, ""creation_date"": 1595238104, ""body"": ""I try to create a tensor out of images for an CNN.\nFirst i used jpg images and that works well. The Tensor includes the number of images, the shape and the channels. Therefor i take all images in a directory an put it in an np array, something like this:\n\n( also tried [np.array(bild),split_lable(img)] )For the CNN i split the lable and the images like this:\n\nthis works very well and i geht exactly what i want.\n\nX | Array of uint8 | (655,337,900,3)\n\nBut now i want to use some png images. I just change the direction and the ending to png but this will return an array of type object\n\nX | array ofobject | (610,)\n\nSo here the number of images is correct but i miss the size. I can&#39;t see any Differenz in how i safe the images in the array especially because the individualy images have the right type.\n"", ""excerpt"": ""I try to create a <span class=\""highlight\"">tensor</span> out of images for an CNN.\nFirst i used jpg images and that works well. The <span class=\""highlight\"">Tensor</span> includes the number of images, the shape and the channels. &hellip; I can&#39;t see any Differenz in how i <span class=\""highlight\"">safe</span> the images in the array especially because the individualy images have the right type. &hellip; "", ""title"": ""Create Tensor with images and lable jpg vs png""}"

Is adding values with tf.concat slow in for-loops?,Stack Overflow,N/A,"{""tags"": [""python"", ""performance"", ""tensorflow"", ""for-loop"", ""tensorflow2.0""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 62538267, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1592931369, ""creation_date"": 1592925827, ""body"": ""Im using tensorflow 2.0 and try to speed up my training by optimizing my code a little bit.\nI run my model batchwise and want to safe the results from each batch to have all results at the end of one epoch in one tensor.\nThis is how my code looks like:\n\nLets assume, one prediction is just a scalar value. So  is a tensor with shape=[batchsize,].\nThis way of doing the concaternation just works fine.\nNow my question is:\nDoes this  operation slow down my whole training? I also used for this purpose, but it seems like no difference in speed.\nI wonder, because once I worked with Matlab, adding new values to a Vector (and hence change its size) within a for-loop was extremly slow. Initializing the vector with zeros and then assign values in the loop was way more efficient regarding speed.\nIs this also true for tensorflow? Or is there another more &#39;proper&#39; way of doing something like adding tensors together in a for-loop which is more clean or faster?\nI did not find any alternative solution online.\nThanks for the help.\n"", ""excerpt"": ""I run my model batchwise and want to <span class=\""highlight\"">safe</span> the results from each batch to have all results at the end of one epoch in one <span class=\""highlight\"">tensor</span>. &hellip; So predictions_batch is a <span class=\""highlight\"">tensor</span> with shape=[batchsize,].\nThis way of doing the concaternation just works fine.\nNow my question is:\nDoes this tf.concat() operation slow down my whole training? &hellip; "", ""title"": ""Is adding values with tf.concat slow in for-loops?""}"

What does the copy_initial_weights documentation mean in the higher library for Pytorch?,Stack Overflow,N/A,"{""tags"": [""machine-learning"", ""deep-learning"", ""pytorch""], ""question_score"": 14, ""is_accepted"": false, ""answer_id"": 62476155, ""is_answered"": false, ""question_id"": 60311183, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1592589967, ""creation_date"": 1592589967, ""body"": ""I think it&#39;s more or less clear what this means now to me.\n\nFirst I&#39;d like to make some notation clear, specially with respect to indices wrt inner time step and outer time step (also known as episodes):\n\n\n\nAt the beginning of training a neural net has params:\n\n\n\nand are held inside it&#39;s module. For the sake of explanation the specific tensor (for the base model) will be denoted:\n\n\n\nand will be updated with with an in-place operation (this is important since  is the placeholder for all  for all outer step values during &quot;normal&quot; meta-learning) by the outer optimizer. I want to emphasize that  is the tensor for the normal Pytorch neural net base model. By changing this in-place with an outer optimizer (like Adam) we are effectively training the initialization. The outer optimizer will use the gradients wrt this tensor to do the update through the whole unrolled inner loop process.\n\nWhen we say  we mean that we will have a gradient path directly to  with whatever value it currently has. Usually the context manager is done before a inner loop after an outer step has been done so  will have  for the current step. In particular the code that does this is this one for :\n\n\n\nthis might look confusing if you&#39;re not familiar with clone but what it&#39;s doing is making a copy of the current weight of . The unusual thing is that clone also remembers the gradient history from the tensor it came from ( is as identity). It&#39;s main use it to add an extra layer of safety from the user doing dangerous in-place ops in it&#39;s differentiable optimizer. Assuming the user never did anything crazy with in-place ops one could in theory remove the . the reason this is confusing imho is because &quot;copying in Pytorch&quot; (clinging) does not automatically block gradient flows, which is what a &quot;real&quot; copy would do (i.e. create a 100% totally separate tensor). This is not what clone does and that is not what  does.\n\nWhen  what really happens is that the weights are cloned and detached. See the code it eventually runs (here and here):\n\n\n\nwhich runs copy tensor (assuming they are doing a safe copy i.e. doing the extra clone):\n\n\n\nNote that  does not allocate new memory. It shares the memory with the original tensor, which is why the  is needed to have this op be &quot;safe&quot; (usually wrt in-place ops).\n\nSo when  they are copying and detaching the current value of . This is usually  if it&#39;s doing usual meta-learning in the inner adaptation loop. So the intended semantics of  is that and the initial_weight they simply mean . The important thing to note is that the intermediate tensors for the net in the inner loop are not denoted in my notation but they are . Also if things are usually meta-learning we have  and it gets update in-place by the outer optimizer.\n\nNote that because of the outer optimizer&#39;s in-place op and the freeing of the graphs we never take the derivate  with respect to the initial value of . Which was something I initially thought we were doing. \n"", ""excerpt"": ""See the code it eventually runs (here and here):\n\nparams = [_copy_tensor(p, safe_copy, device) for p in module.parameters()]\n\n\nwhich runs copy <span class=\""highlight\"">tensor</span> (assuming they are doing a <span class=\""highlight\"">safe</span> copy i.e. doing the &hellip; It shares the memory with the original <span class=\""highlight\"">tensor</span>, which is why the .clone() is needed to have this op be &quot;<span class=\""highlight\"">safe</span>&quot; (usually wrt in-place ops). &hellip; "", ""title"": ""What does the copy_initial_weights documentation mean in the higher library for Pytorch?""}"

Is it possible to convert this numpy function to tensorflow?,Stack Overflow,N/A,"{""tags"": [""python"", ""numpy"", ""tensorflow"", ""implicit-conversion""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 62222662, ""is_answered"": false, ""question_id"": 61978603, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1591384542, ""creation_date"": 1591384542, ""body"": ""This code &#39;works&#39;, in that it only uses tensorflow functions, and does allow the model to train when used in a training loop:\n\n\n\nThis is where the &#39;convert image&#39; function is applied using tf.vectorize_map:\n\n\n\nIt is PAINFULLY slow, though - It does not appear to be using the GPU, and looks to be single threaded as well.  \n\nI&#39;m adding it as an answer to my own question because is clearly IS possible to do this numpy function entirely in tensorflow - it just probably shouldn&#39;t be done like this.  \n"", ""excerpt"": ""#Example <span class=\""highlight\"">tensor</span> cel (replace with (x))\nP = tf.cast(x, dtype=tf.float32)\n\n    #split out P.x and P.y, and fill a ragged <span class=\""highlight\"">tensor</span> to the same shape as Rx\n\nPx_value = tf.cast(x, dtype=tf.float32) - tf.cast( &hellip; Makes boolean bitwise mask\n\n    #calculate the intersection of the line with the y-value, assuming it intersects\n    #P.x &lt;= (G.x - R.x) * (P.y - R.y) / (G.y - R.y + R.x)   - use tf.divide_no_nan for <span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""Is it possible to convert this numpy function to tensorflow?""}"

"Colab TPU error - InvalidArgumentError: Unsupported data type for TPU: string, caused by output cond_8/Identity_1:0",Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""google-colaboratory"", ""tpu""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 61831465, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1589599257, ""creation_date"": 1589599257, ""body"": ""I get above error in colab TPU from the code below. Original model had epochs, steps_per_epoch and batch but removed that while debugging. Not sure what the issue is as I do not see a string.\nNone TPU version of code works. Most of the code is stock code with some modifications made. I tested the code to ensure images loaded properly.\n\n\n\n\n\n- \n\n\n\nError:\n\n\n"", ""excerpt"": ""tf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n\nlist_ds = tf.data.Dataset.list_files(str(gcs_pattern))\n\n# Reads an image from a file, decodes it into a dense <span class=\""highlight\"">tensor</span> &hellip; validation_batch_size, validation_freq, max_queue_size, workers,       use_multiprocessing)\n          853                 context.async_wait()\n          854               logs = tmp_logs  # No error, now <span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""Colab TPU error - InvalidArgumentError: Unsupported data type for TPU: string, caused by output cond_8/Identity_1:0""}"

Custom metric in keras with three parameters,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""keras"", ""time-series"", ""metrics""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 61738185, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1589227946, ""creation_date"": 1589227005, ""body"": ""Hi everyone I&#39;m building a deepNN using keras for forecasting time series, to be more specific I&#39;m using a Conv1D so inputs are tensors with this kind of shape:\n\nMy problem is that I&#39;d like to have a custom metric that needs to recieve 3 parameters instead of the 2 standar  and . This because my metric needs to be safely computed as it does not rely on divisions with values that could be equal to zero (e.g. as done in percentage errors when &#119884;&#119905; = 0).The complete formula of my metric is showed below:\n\nHow can I pass the  y_true_N and have a metric function like that?\n\nwhere the   correspond to all the target values in the training while  and  correspond to the target in the validation.\nI hope I&#39;ve been clear, if anyone has an example can please show me how to get it? Thanks in advance\n"", ""excerpt"": ""Hi everyone I&#39;m building a deepNN using keras for forecasting time series, to be more specific I&#39;m using a Conv1D so inputs are tensors with this kind of shape:\n[samples, window_size, 1]\ninput_2  = In &hellip; "", ""title"": ""Custom metric in keras with three parameters""}"

Convert tensorflow 2.0 estimator to tensorflow lite,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""tensorflow2.0"", ""tensorflow-lite"", ""tensorflow-estimator""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 61385083, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1587972182, ""creation_date"": 1587638102, ""body"": ""After exporting my saved_model.pb file, I would like to convert it in TfLite.\n\nI had the same error than this post.\nSo I tried the proposed answer :\n\n\n\nHowever, I have a new error:\n\nPrint Output:\n\n\n\nOutput Error on converter.convert():\n\n\n\nJust in case, I exported with no issue my DNNclassifier estimator thanks to this code:\n\n\n\nI am wondering if my error is because of a bad export of my .pb file, is it possible? Else do you have an idea to solve this error?\n\nThank you for your help.\n"", ""excerpt"": ""placeholder op: 20 \n2020-04-23 12:05:52.644632: F tensorflow/lite/toco/import_tensorflow.cc:2690] \nCheck failed: status.ok() \nInput_content string_val doesn&#39;t have the right dimensions for this string <span class=\""highlight\"">tensor</span> &hellip; Note this is a warning and probably <span class=\""highlight\"">safe</span> to ignore.\nto_proto not supported in EAGER mode. &hellip; "", ""title"": ""Convert tensorflow 2.0 estimator to tensorflow lite""}"

Implementing a trainable generalized Bump function layer in Keras/Tensorflow,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""machine-learning"", ""keras"", ""tf.keras""], ""question_score"": 4, ""is_accepted"": true, ""answer_id"": 61310695, ""is_answered"": false, ""question_id"": 60889308, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1587325797, ""creation_date"": 1587325797, ""body"": ""I am a bit surprised that no one has mentioned the main (and only) reason for the given warning! As it seems, that code is supposed to implement the generalized variant of Bump function; however, just take a look at the functions implemented again:\n\n\n\nThe error is evident: there is no usage of the trainable weight of the layer in these functions! So there is no surprise that you get the message saying that no gradient exist for that: you are not using it at all, so no gradient to update it! Rather, this is exactly the original Bump function (i.e. with no trainable weight).\n\nBut, you might say that: &quot;at least, I used the trainable weight in the condition of , so there must be some gradients?!&quot;; however, it&#39;s not like that and let me clear up the confusion:\n\n\nFirst of all, as you have noticed as well, we are interested in element-wise conditioning. So instead of  you need to use .\nThe other misconception is to claim that since  is used as the condition, and since it is not differentiable i.e. it has no gradient with respect to its inputs (which is true: there is no defined gradient for a function with boolean output w.r.t. its real-valued inputs!), then that results in the given warning!\n\n\nThat&#39;s simply wrong! The derivative here would be taken of the output of the layer w.r.t trainable weight, and the selection condition is NOT present in the output. Rather, it&#39;s just a boolean tensor which determines the output branch to be selected. That&#39;s it! The derivative of condition is not taken and will never be needed. So that&#39;s not the reason for the given warning; the reason is only and only what I mentioned above: no contribution of trainable weight in the output of layer. (Note: if the point about condition is a bit surprising to you, then think about a simple example: the ReLU function, which is defined as . If the derivative of condition, i.e. , is considered/needed, which does not exists, then we would not be able to use ReLU in our models and train them using gradient-based optimization methods at all!)\n\n\n\n\n\n(Note: starting from here, I would refer to and denote the threshold value as sigma, like in the equation).\n\nAll right! We found the reason behind the error in implementation. Could we fix this? Of course! Here is the updated working implementation:\n\n\n\nA few points regarding this implementation:\n\n\nWe have replaced  with  in order to do element-wise conditioning.\nFurther, as you can see, unlike your implementation which only checked for one side of inequality, we are using ,  and also  to find out whether the input values have magnitudes of less than  (alternatively, we could do this using just  and ; no difference!). And let us repeat it: using boolean-output functions in this way does not cause any problems and have nothing to do with derivatives/gradients.\nWe are also using a non-negativity constraint on the sigma value learned by layer. Why? Because sigma values less than zero does not make sense (i.e. the range  is ill-defined when sigma is negative).\nAnd considering the previous point, we take care to initialize the sigma value properly (i.e. to a small non-negative value).\nAnd also, please don&#39;t do things like ! It&#39;s redundant (and a bit weird) and it is equivalent to ; and both have a gradient of  (w.r.t. ). Multiplying zero with a tensor does not add anything or solve any existing issue, at least not in this case!\n\n\nNow, let&#39;s test it to see how it works. We write some helper functions to generate training data based on a fixed sigma value, and also to create a model which contains a single  with input shape of . Let&#39;s see if it could learn the sigma value which is used for generating training data:\n\n\n\nYes, it could learn the value of sigma used for generating data! But, is it guaranteed that it actually works for all different values of training data and initialization of sigma? The answer is: NO! Actually, it is possible that you run the code above and get  as the value of sigma after training, or  as the loss value! So what&#39;s the problem? Why this  or  values might be produced? Let&#39;s discuss it below...\n\n\n\nDealing with numerical stability\n\nOne of the important things to consider, when building a machine learning model and using gradient-based optimization methods to train them, is the numerical stability of operations and calculations in a model. When extremely large or small values are generated by an operation or its gradient, almost certainly it would disrupt the training process (for example, that&#39;s one of the reasons behind normalizing image pixel values in CNNs to prevent this issue).\n\nSo, let&#39;s take a look at this generalized bump function (and let&#39;s discard the thresholdeding for now). It&#39;s obvious that this function has singularities (i.e. points where either the function or its gradient is not defined) at  (i.e. when  or ). The animated diagram below shows the bump function (the solid red line), its derivative w.r.t. sigma (the dotted green line) and  and  lines (two vertical dashed blue lines), when sigma starts from zero and is increased to 5:\n\n\n\nAs you can see, around the region of singularities the function is not well-behaved for all values of sigma, in the sense that both the function and its derivative take extremely large values at those regions. So given an input value at those regions for a particular value of sigma, exploding output and gradient values would be generated, hence the issue of  loss value.\n\nEven further, there is a problematic behavior of  which causes the issue of  values for the sigma variable in the layer: surprisingly, if the produced value in inactive branch of  is extremely large or , which with the bump function results in extremely large or  gradient values, then the gradient of  would be , despite the fact that the  is in inactive branch and is not even selected (see this Github issue which discusses exactly this)!!\n\nSo is there any workaround for this behavior of ? Yes, actually there is a trick to somehow resolve this issue which is explained in this answer: basically we can use an additional  in order to prevent the function to be applied on these regions. In other words, instead of applying  on any input value, we filter those values which are NOT in the range  (i.e. the actual range which the function should be applied) and instead feed the function with zero (which is always produce safe values, i.e. is equal to ):\n\n\n\nApplying this fix would entirely resolve the issue of  values for sigma. Let&#39;s evaluate it on training data values generated with different sigma values and see how it would perform:\n\n\n\nIt could learn all the sigma values correctly! That&#39;s nice. That workaround worked! Although, there is one caveat: this is guaranteed to work properly and learn any sigma value if the input values to this layer are greater than -1 and less than 1 (i.e. this is the default case of our  function); otherwise, there is still the issue of  loss value which might happen if the input values have a magnitude of greater than 1 (see point #1 and #2, below).\n\n\n\nHere are some foods for thought for the curios and interested mind:\n\n\nIt was just mentioned that if the input values to this layer are greater than 1 or less than -1, then it may cause problems. Can you argue why this is the case? (Hint: use the animated diagram above and consider cases where  and the input value is between  and  (or between  and .)\nCan you provide a fix for the issue in point #1, i.e. such that the layer could work for all input values? (Hint: like the workaround for , think about how you can further filter-out the unsafe values which the bump function could be applied on and produce exploding output/gradient.)\nHowever, if you are not interested to fix this issue, and would like to use this layer in a model as it is now, then how would you guarantee that the input values to this layer are always between -1 and 1? (Hint: as one solution, there is a commonly-used activation function which produces values exactly in this range and could be potentially used as the activation function of the layer which is before this layer.)\nIf you take a look at the last code snippet, you will see that we have used . Why is that? Why large values of sigma need more epochs to be learned? (Hint: again, use the animated diagram and consider the derivative of function for input values between -1 and 1 as sigma value increases. What are their magnitude?)\nDo we also need to check the generated training data for any ,  or extremely large values of  and filter them out? (Hint: yes, if  and range of values, i.e.  and , fall outside of ; otherwise, no that&#39;s not necessary! Why is that? Left as an exercise!)\n\n"", ""excerpt"": ""Rather, it&#39;s just a boolean <span class=\""highlight\"">tensor</span> which determines the output branch to be selected. That&#39;s it! The derivative of condition is not taken and will never be needed. &hellip; Multiplying zero with a <span class=\""highlight\"">tensor</span> does not add anything or solve any existing issue, at least not in this case!\n\n\nNow, let&#39;s test it to see how it works. &hellip; "", ""title"": ""Implementing a trainable generalized Bump function layer in Keras/Tensorflow""}"

TypeError: can’t convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first (fastai),Stack Overflow,N/A,"{""tags"": [""numpy"", ""pytorch"", ""cpu""], ""question_score"": 2, ""is_accepted"": true, ""answer_id"": 61255656, ""is_answered"": false, ""question_id"": 61236178, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1587055988, ""creation_date"": 1587055988, ""body"": ""I&#39;m assuming that both  and  are CUDA tensors, that means that you need to bring them both to the CPU for the , not just one.\n\n\n\nCalling  on a tensor that is already on the CPU has no effect, so it&#39;s safe to use in any case.\n"", ""excerpt"": ""torch.tensor(cohen_kappa_score(torch.argmax(y_hat.cpu(),1), y.cpu(), weights=&#39;quadratic&#39;),device=&#39;cuda:0&#39;)\n    #                                                        ^^^         ^^^\n\n\nCalling .cpu() on a <span class=\""highlight\"">tensor</span> &hellip; that is already on the CPU has no effect, so it&#39;s <span class=\""highlight\"">safe</span> to use in any case. &hellip; "", ""title"": ""TypeError: can’t convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first (fastai)""}"

Any guarantees that Torch won&#39;t mess up with an already allocated CUDA array?,Stack Overflow,N/A,"{""tags"": [""cuda"", ""pytorch"", ""torch"", ""numba""], ""question_score"": 1, ""is_accepted"": true, ""answer_id"": 60596585, ""is_answered"": false, ""question_id"": 60593317, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1583740770, ""creation_date"": 1583740770, ""body"": ""\n  Will PyTorch, when allocating later GPU memory for some tensors, accidentally overwrite the memory space that is being used for our first CUDA array?\n\n\nNo.\n\n\n  are they automatically aware of memory regions used by other CUDA programs ...\n\n\nThey are not &quot;aware&quot;, but each process gets its own separate context ... \n\n\n  ... or does each one of them see the entire GPU memory as his own?\n\n\n.... and contexts have their own address spaces and isolation. So neither, but there is no risk of memory corruption.\n\n\n  If it&#39;s the latter, is there a way to make them aware of allocations by other CUDA programs?\n\n\nIf by &quot;aware&quot; you mean &quot;safe&quot;, then that happens automatically. If by &quot;aware&quot; you imply some sort of interoperability, then that is possible on some platforms, but it is not automatic.\n\n\n  ... assume that all allocations are done by the same process.\n\n\nThat is a different situation. In general, the same process implies a shared context, and shared contexts share a memory space, but all the normal address space protection rules and facilities apply, so there is not a risk of loss of safety. \n"", ""excerpt"": ""If by &quot;aware&quot; you mean &quot;<span class=\""highlight\"">safe</span>&quot;, then that happens automatically. &hellip; "", ""title"": ""Any guarantees that Torch won&#39;t mess up with an already allocated CUDA array?""}"

Tensorflow Lite inference memory allocation,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""memory"", ""tensorflow-lite""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 60555993, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1583455462, ""creation_date"": 1583455462, ""body"": ""Usually during the neural network inference, especially for forward NN, after the execution at one layer finishes, one can safely discard the result of the previous layer, since it won&#39;t be used any more (except for those with shortcut connections.). So memory can be saved during the inference. My question is whether TFlite is doing this at this moment. By looking at their C++ code, it seems that they now allocate all the tensors before hand (input, output and intermediate tensors). But I might be wrong. Can anyone confirm this?\n\nThank you!\n"", ""excerpt"": ""Usually during the neural network inference, especially for forward NN, after the execution at one layer finishes, one can safely discard the result of the previous layer, since it won&#39;t be used any m &hellip; "", ""title"": ""Tensorflow Lite inference memory allocation""}"

Set Vector of Eigen Matrices to 0,Stack Overflow,N/A,"{""tags"": [""c++"", ""eigen""], ""question_score"": 0, ""is_accepted"": true, ""answer_id"": 60322677, ""is_answered"": false, ""question_id"": 60309152, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1582210422, ""creation_date"": 1582210422, ""body"": ""First of all,  will by default be aligned to 16 bytes, which on 64bit systems or with C++17 will most likely work properly, but you may otherwise face some caveats.\nAn easy way to workaround any issues regarding alignment is to write\n\n\n\nNow the idiomatic way to write what you want would be to write\n\n\n\nHowever, this will result in a loop of  calls (at least for gcc and clang: https://godbolt.org/z/ULixBm).\n\nAlternatively, you could create a vector of uninitialized es and apply  on them:\n\n\n\nNote that for that to run without Eigen having to loop through all elements requires to have  not aligned (as shown at the beginning) or to disable alignment-assertions: https://godbolt.org/z/nDJqV5\n\nIf you don&#39;t actually need the  functionality (mostly the ability to copy and resize), you could just allocate some memory using  and  it after usage. To be leak-safe, this can be encapsulated into a :\n\n\n\nClang and gcc will optimize this into a single / pair:\nhttps://godbolt.org/z/m4rzRq\n\n\n\nA totally different approach would be to try using a 3D Tensor instead of a vector of matrices:\n\n\n\nLooking at the generated assembly this looks semi-optimal though, even with .\n\n\n\nOverall, note that benchmarking anything memory-related may be misleading. E.g., the call to  may return nearly instantaneous but on a lower level point to unallocated pages, which makes actually accessing them the first time more expensive.\n"", ""excerpt"": ""To be leak-<span class=\""highlight\"">safe</span>, this can be encapsulated into a std::unique_ptr:\n\n// unique_ptr with custom deallocator (use a typedef, if you need this more often):\nstd::unique_ptr&lt;HoughMatrix[], void(&amp;)(void*)&gt; hough_spaces &hellip; Eigen::<span class=\""highlight\"">Tensor</span>&lt;int, 3&gt; HoughSpace;\nHoughSpace hough_spaces(num_spaces,30,150);\nhough_spaces.setZero();\n\n\nLooking at the generated assembly this looks semi-optimal though, even with -O3 -DNDEBUG. &hellip; "", ""title"": ""Set Vector of Eigen Matrices to 0""}"

Custom TensorFlow Keras optimizer,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""deep-learning"", ""tf.keras"", ""tensorflow2.x""], ""question_score"": 36, ""is_accepted"": false, ""answer_id"": 60309569, ""is_answered"": false, ""question_id"": 58772846, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1582147960, ""creation_date"": 1582147960, ""body"": ""\nYes, this looks to be a documentation error. The preceding underscore names are the correct methods to override. Related is the non-Keras Optimizer which has these all defined, but not implemented in the base class https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py\n\n\n\n\n\nI don&#39;t know about . For one thing, if you do override it, the code mentions that a per-replica DistributionStrategy could be &quot;dangerous&quot;\n\n\n\n"", ""excerpt"": ""Args:\n      grad: a `<span class=\""highlight\"">Tensor</span>` representing the gradient.\n      handle: a `<span class=\""highlight\"">Tensor</span>` of dtype `resource` which points to the variable\n       to be updated. &hellip; We do\n    # allow creation per-replica optimizers however, because the\n    # compute_gradients()-&gt;apply_gradients() sequence is <span class=\""highlight\"">safe</span>. &hellip; "", ""title"": ""Custom TensorFlow Keras optimizer""}"

What&#39;s the difference between tf.placeholder and tf.Variable?,Stack Overflow,N/A,"{""tags"": [""tensorflow""], ""question_score"": 318, ""is_accepted"": false, ""answer_id"": 39177244, ""is_answered"": false, ""question_id"": 36693740, ""item_type"": ""answer"", ""score"": 66, ""last_activity_date"": 1581340758, ""creation_date"": 1472269226, ""body"": ""Since Tensor computations compose of graphs then it&#39;s better to interpret the two in terms of graphs. \n\nTake for example the simple linear regression \n\n\n\nwhere  and  stand for the weights and bias and  for the observations&#39; inputs and  for the observations&#39; outputs.\n\nObviously  and  are of the same nature (manifest variables) which differ from that of  and  (latent variables).  and  are values of the samples (observations) and hence need a place to be filled, while  and  are the weights and bias, Variables (the previous values affect the latter) in the graph which should be trained using different  and  pairs. We place different samples to the Placeholders to train the Variables.  \n\nWe only need to save or restore the Variables (at checkpoints) to save or rebuild the graph with the code. \n\nPlaceholders are mostly holders for the different datasets (for example training data or test data). However, Variables are trained in the training process for the specific tasks, i.e., to predict the outcome of the input or map the inputs to the desired labels. They remain the same until you retrain or fine-tune the model using different or the same samples to fill into the Placeholders often through the dict. For instance:\n\n\n\nPlaceholders are also passed as parameters to set models. \n\nIf you change placeholders (add, delete, change the shape etc) of a model in the middle of training, you can still reload the checkpoint without any other modifications. But if the variables of a saved model are changed, you should adjust the checkpoint accordingly to reload it and continue the training (all variables defined in the graph should be available in the checkpoint).   \n\nTo sum up, if the values are from the samples (observations you already have) you safely make a placeholder to hold them, while if you need a parameter to be trained harness a Variable (simply put, set the Variables for the values you want to get using TF automatically). \n\nIn some interesting models, like a style transfer model, the input pixes are going to be optimized and the normally-called model variables are fixed, then we should make the input (usually initialized randomly) as a variable as implemented in that link. \n\nFor more information please infer to this simple and illustrating doc.\n"", ""excerpt"": ""Since <span class=\""highlight\"">Tensor</span> computations compose of graphs then it&#39;s better to interpret the two in terms of graphs. &hellip; "", ""title"": ""What&#39;s the difference between tf.placeholder and tf.Variable?""}"

Error in my TensorFlow input function: &quot;TypeError: List of Tensors when single Tensor expected&quot;,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""machine-learning"", ""iris-dataset""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 60001853, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1580546567, ""creation_date"": 1580465528, ""body"": ""Python: 3.6.9\n\nTensorFlow: 1.15.0 \n\nDespite seeing answers to similar questions on SO, I have been unable to detect and resolve the bug in my code. So I have come here to ask you for your help.\n\nI am training a classifier on the Iris dataset and I get the following error:\n\n\n  TypeError: List of Tensors when single Tensor expected\n\n\nBut, before this error occurs I see another error in the stack trace:\n\n\n  ValueError: Tensor(&quot;IteratorGetNext:4&quot;, shape=(10,), dtype=string)\n\n\nwhere 10 is the batch size.\n\n\n\nRelevant Code:\n\n\n\nStack Trace:\n\n\n\n\n\nValues of the dictionaries  and  before they are passed to :\n\n\n"", ""excerpt"": ""trace:\n\n\n  ValueError: <span class=\""highlight\"">Tensor</span>(&quot;IteratorGetNext:4&quot;, shape=(10,), dtype=string)\n\n\nwhere 10 is the batch size. &hellip; if isinstance(v, ops.Tensor)]\n    278 # pylint: enable=invalid-name\n\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py in _check_failed(v)\n    247   # it is <span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""Error in my TensorFlow input function: &quot;TypeError: List of Tensors when single Tensor expected&quot;""}"

Use of torchvision.utils.save_image twice on the same image tensor makes the second save not work. What&#39;s going on?,Stack Overflow,N/A,"{""tags"": [""image"", ""image-processing"", ""pytorch"", ""tensor"", ""torchvision""], ""question_score"": 3, ""is_accepted"": true, ""answer_id"": 59056046, ""is_answered"": false, ""question_id"": 59054886, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1574793092, ""creation_date"": 1574788042, ""body"": ""It turns out  modifies the input tensor. A workaround to this is to add a line somewhere before calling  that&#39;s similar to this:\n\n\n\nThen you can safely save the perturbed image twice if on the second call you use perturbed_data_copy instead of the perturbed_data (which was modified by torchvision.utils.save_image). I will be submitting a bug report and tagging this post. Thanks @Mat for pointing this out!\n"", ""excerpt"": ""It turns out torchvision.utils.save_image modifies the input <span class=\""highlight\"">tensor</span>. &hellip; "", ""title"": ""Use of torchvision.utils.save_image twice on the same image tensor makes the second save not work. What&#39;s going on?""}"

keras model.summary() don&#39;t show all model layers,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""keras""], ""question_score"": 0, ""is_accepted"": true, ""answer_id"": 58942644, ""is_answered"": false, ""question_id"": 58942583, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1574196828, ""creation_date"": 1574196828, ""body"": ""You need to call each layer on the previous ouput tensor:\n\n\n\nIn general, the summary is good at showing all part of the model. If something is not shown in the summary, it&#39;s safe to assume that that means that it&#39;s not in the model.\n"", ""excerpt"": ""You need to call each layer on the previous ouput <span class=\""highlight\"">tensor</span>:\n\ntensor_input = Input(shape=(Xn.shape[1], Xn.shape[2]), name=&#39;main_inputs&#39;)\n\nxy = TimeDistributed(Conv1D(filters= 10, kernel_size= 3, &hellip; If something is not shown in the summary, it&#39;s <span class=\""highlight\"">safe</span> to assume that that means that it&#39;s not in the model. &hellip; "", ""title"": ""keras model.summary() don&#39;t show all model layers""}"

AttributeError: module &#39;tensorflow&#39; has no attribute &#39;placeholder&#39; with keras 2.2.4 tensorflow 1.14,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""keras""], ""question_score"": 0, ""is_accepted"": true, ""answer_id"": 58801230, ""is_answered"": false, ""question_id"": 58793607, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1573474837, ""creation_date"": 1573474837, ""body"": ""A placeholder is the initial tensor-like object you use to create a symbolic graph model. (Which is the standard Keras model and old Tensorflow model). \n\nIf it can&#39;t be found, either your installation is bad or your tensorflow version is 2.0.0 (and thus uses eager mode by default - eager mode doesn&#39;t support placeholders).   \n\nTo use Tensorflow 2.0.0, it&#39;s probably better to use  instead of . (But it may be an idea to test Keras 2.3 as proposed by Matias Valdenegro)    \n\nTo fix your installation, the safest way is to create a new environment.\n\nYou should search the internet on how to create a new &quot;environment&quot; in Anaconda and in this environment you install the versions you need. This is the only safe way to install/uninstall things without breaking your previous installations. After you created this environment and installed only the versions you need, then you run your code from this environment. Unfortunately these installation issues are not easy things to tackle. \n"", ""excerpt"": ""A placeholder is the initial <span class=\""highlight\"">tensor</span>-like object you use to create a symbolic graph model. (Which is the standard Keras model and old Tensorflow model). &hellip; This is the only <span class=\""highlight\"">safe</span> way to install/uninstall things without breaking your previous installations. &hellip; "", ""title"": ""AttributeError: module &#39;tensorflow&#39; has no attribute &#39;placeholder&#39; with keras 2.2.4 tensorflow 1.14""}"

how to write a generator for keras model for predict_generator,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""optimization"", ""keras"", ""generator""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 58202668, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1570024819, ""creation_date"": 1570023295, ""body"": ""I have a trained keras model, and I am trying to run predictions with CPU only. I want this to be as quick as possible, so I thought I would use  with multiple workers. All of the data for my prediction tensor are loaded into memory beforehand. Just for reference, array is a list of tensors, with the first tensor having shape [nsamples, x, y, nchannels]. I made a thread-safe generator following the instructions here (I followed this when using  as well).\n\nI run predictions with my model like so,\n\nbut I don&#39;t get any speed improvement over using , regardless of the number of workers. This seemed to work well when fitting my model (i.e., getting a speed-up using a generator with multiple workers). Am I missing something in my generator? Is there a more efficient way to optimize predictions (besides using GPU)?\n"", ""excerpt"": ""All of the data for my prediction <span class=\""highlight\"">tensor</span> are loaded into memory beforehand. Just for reference, array is a list of tensors, with the first <span class=\""highlight\"">tensor</span> having shape [nsamples, x, y, nchannels]. &hellip; I made a thread-<span class=\""highlight\"">safe</span> generator following the instructions here (I followed this when using fit_generator as well).\nclass DataGeneratorPredict(keras.utils.Sequence):\n    &#39;Generates data for Keras&#39;\n    def &hellip; "", ""title"": ""how to write a generator for keras model for predict_generator""}"

tensorflow 1.13 how to use tf.searchsort safe?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 57986086, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1568815865, ""creation_date"": 1568787038, ""body"": ""With code \n\n\n\nI got first error\n\n\n  InvalidArgumentError (see above for traceback): Reshape cannot infer\n  the missing input size for an empty tensor unless all specified input\n  sizes are non-zero\n\n\nIt also reminded me in line 3459 of \n\n\n  tensorflow/python/ops/array_ops.py\n  \n  searchsorted \n\n\ngot\n\n\n\nBut when tensor shape contain 0 dimension,  , it will return an error. see here\n\nI want to check the tensor shape before using , and I know the dimension is \n\nSo I use \n\n\n\nThen I got my second error, and I know that  will return bool tensor, which cannot be used like bool. But I don&#39;t know how to solve my first error.\n\n\n  ValueError: Tried to convert &#39;x&#39; to a tensor and failed. Error: Cannot\n  convert an unknown Dimension to a Tensor: ?\n\n\nMy question is how to use  safe if the first error trigered by  dimension\n"", ""excerpt"": ""ValueError: Tried to convert &#39;x&#39; to a <span class=\""highlight\"">tensor</span> and failed. Error: Cannot\n  convert an unknown Dimension to a <span class=\""highlight\"">Tensor</span>: ? &hellip; My question is how to use tf.searchsort <span class=\""highlight\"">safe</span> if the first error trigered by 0 dimension &hellip; "", ""title"": ""tensorflow 1.13 how to use tf.searchsort safe?""}"

Converting Keras model to multi label output,Stack Overflow,N/A,"{""tags"": [""python"", ""machine-learning"", ""keras"", ""deep-learning"", ""conv-neural-network""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 57661516, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1566900369, ""creation_date"": 1566836256, ""body"": ""I have a model which takes in a dataframe which looks like this \n\n\n\nwith model structure like this\n\n\n\nI realize that&#39;s a lot of code, but i what i want to do is take in a dataframe which look like this \n\n\n\nand in order to achive this i have made this following changes,\n\n\n\nbut when i run this i get, \n\n\n\nAny suggestions on how i could change this model to train this on multi label inputs.Thanks in advance.\n"", ""excerpt"": ""# Every element is Nx8 <span class=\""highlight\"">tensor</span>, where N is a batch size. &hellip; shuffle the whole dataset\n\n        next_batch = idg(files_ds).repeat().make_one_shot_iterator().get_next()\n        for i in range(max(in_len//32,1)):\n            # NOTE: if we loop here it is &#39;thread-<span class=\""highlight\"">safe</span>-ish &hellip; "", ""title"": ""Converting Keras model to multi label output""}"

TensorFlow: Is there a way to convert a frozen graph into a checkpoint model?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow""], ""question_score"": 17, ""is_accepted"": false, ""answer_id"": 57596363, ""is_answered"": false, ""question_id"": 45275141, ""item_type"": ""answer"", ""score"": 5, ""last_activity_date"": 1566417538, ""creation_date"": 1566407597, ""body"": ""There is a method for converting constants back to trainable variables in TensorFlow, via the Graph Editor. However, you will need to specify the nodes to convert, as I&#39;m not sure if there is a way to automatically detect this in a robust manner.\n\nHere are the steps:\n\nStep 1: Load frozen graph\n\nWe load our  file into a graph object.\n\n\n\nStep 2: Find constants that need conversion\n\nHere are 2 ways to list the names of nodes in your graph:\n\n\nUse this script to print them\n\n\n\nThe nodes you&#39;ll want to convert are likely named something along the lines of &quot;Const&quot;. To be sure, it is a good idea to load your graph in Netron to see which tensors are storing the trainable weights. Oftentimes, it is safe to assume that all const nodes were once variables.\n\nOnce you have these nodes identified, let&#39;s store their names into a list:\n\n\n\nStep 3: Convert constants to variables\n\nRun this code to convert your specified constants. It essentially creates corresponding variables for each constant and uses GraphEditor to unhook the constants from the graph, and hook the variables on.\n\n\n\nStep 4: Save result as \n\n\n\nAnd viola! You should be done at this point :) I was able to get this working myself, and verified that the model weights are preserved--the only difference is that the graph is now trainable. Please let me know if there are any issues.\n"", ""excerpt"": ""Oftentimes, it is <span class=\""highlight\"">safe</span> to assume that all const nodes were once variables. &hellip; (&#39;{}:0&#39;.format(name))\n        with tf.Session() as sess:\n            tensor_as_numpy_array = sess.run(<span class=\""highlight\"">tensor</span>)\n        var_shape = tensor.get_shape()\n        # Give each variable a name that doesn&#39;t already &hellip; "", ""title"": ""TensorFlow: Is there a way to convert a frozen graph into a checkpoint model?""}"

How to connect convolution layer with lstm layer to in seq2seq tasks?,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""conv-neural-network"", ""lstm"", ""seq2seq""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 57401878, ""is_answered"": false, ""question_id"": 57401286, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1565209636, ""creation_date"": 1565209636, ""body"": ""The RNN expects that the input is going to be sequential. Therefore, the input has the shape  or if you are processing a batch .\n\nIn your case, the input has a shape . Then, you use a convolutional layer, to learn spatial dependencies between the pixels in each video frame. Therefore, for each video frame, the convolutional layer is going to provide you a tensor with shape . Then, because you want to learn a context-dependent representation of the frames, you are safe to reshape everything that you learned for each frame in a 1D sequence.\n\nFinally, what you will provide the RNN is: .\n\nAs for the implementation, if we assume that you have 2 videos, and each video has 5 frames, where each frame has width and height of 10 and 3 channels, this is what you should do:\n\n\n\nPlease note that I have hardcoded some of the variables in the code for simplicity.\n\nI hope that this helps you!\n"", ""excerpt"": ""Therefore, for each video frame, the convolutional layer is going to provide you a <span class=\""highlight\"">tensor</span> with shape [activation_map_width, activation_map_height, number_of_filters]. &hellip; Then, because you want to learn a context-dependent representation of the frames, you are <span class=\""highlight\"">safe</span> to reshape everything that you learned for each frame in a 1D sequence. &hellip; "", ""title"": ""How to connect convolution layer with lstm layer to in seq2seq tasks?""}"

List not populated correctly unless use PyTorch clone(),Stack Overflow,N/A,"{""tags"": [""machine-learning"", ""deep-learning"", ""pytorch""], ""question_score"": 1, ""is_accepted"": true, ""answer_id"": 54853134, ""is_answered"": false, ""question_id"": 54852644, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1564767475, ""creation_date"": 1551020200, ""body"": ""\nFirst of all, I am going to reproduce your case. I will use very simple model:\n\nCode:\n\n\n\nOutput:\n\n\n\nOk, it works well. Let&#39;s now look at :\n\nCode:\n\n\n\nOutput:\n\n\n\nOk, it is not what we want, but actually it is expected behavior. If you look once again, you will see that values in the list correspond to weights values from second epoch. That means we were appending not new tensors, but assignments that point to real weights storage, and that&#39;s why we just have the same final results.\n\nIn other words, you are getting the same values when using regular append, because gradients still propagate to the original weights tensor. And appended &quot;weights tensors&quot; are pointing to the same tensor from model that changes during backprop. \n\nThat&#39;s why you need to use  to create a new tensor, BUT it is recommended to use  whereas  is recorded to computational graph, this means if you backprop through this cloned tensor, \n\n\n  Gradients propagating to the cloned tensor will propagate to the original tensor. clone docs\n\n\nSo, if you want to append your weights safely use this:\n\n\n"", ""excerpt"": ""And appended &quot;weights tensors&quot; are pointing to the same <span class=\""highlight\"">tensor</span> from model that changes during backprop. &hellip; <span class=\""highlight\"">tensor</span>, \n\n\n  Gradients propagating to the cloned <span class=\""highlight\"">tensor</span> will propagate to the original <span class=\""highlight\"">tensor</span>. clone docs\n\n\nSo, if you want to append your weights safely use this:\n\nweights.append(model.fc2.weight.data.clone &hellip; "", ""title"": ""List not populated correctly unless use PyTorch clone()""}"

How to fix &#39;Expected object of scalar type Float but got scalar type Double for argument #4 &#39;mat1&#39;&#39;?,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""lstm""], ""question_score"": 1, ""is_accepted"": true, ""answer_id"": 57081912, ""is_answered"": false, ""question_id"": 57076709, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1563386916, ""creation_date"": 1563386916, ""body"": "" does not need any initialization, as it&#39;s initialized to zeros by default (see documentation).\nFurthermore,  already has predefined  method, so one can move module to GPU simply, hence you can safely delete .\n\nYou have this error because your  is of type , while modules by default use  (as it&#39;s accurate enough, faster and smaller than ).\nYou can cast your input Tensors by calling , in your case it could look like that:\n\n\n\nFinally, there is no need for  argument if it&#39;s always zeroes, you can simply use:\n\n\n\nas  is zeroes by default as well.\n"", ""excerpt"": ""torch.nn.LSTM does not need any initialization, as it&#39;s initialized to zeros by default (see documentation).\nFurthermore, torch.nn.Module already has predefined cuda() method, so one can move module t &hellip; "", ""title"": ""How to fix &#39;Expected object of scalar type Float but got scalar type Double for argument #4 &#39;mat1&#39;&#39;?""}"

How to reshape a tensor in Eigen3?,Stack Overflow,N/A,"{""tags"": [""c++"", ""tensor"", ""eigen3""], ""question_score"": 2, ""is_accepted"": false, ""answer_id"": 57044397, ""is_answered"": false, ""question_id"": 56985731, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1563210761, ""creation_date"": 1563210761, ""body"": ""You can&#39;t mix different  for Tensor operations (that is the implicit 4th template parameter of ). This also means the type of the  must match the . By default,  uses  which is the same as . You can either write this instead of  (similar for )\n\n\n\nor you replace  by .\nOf course making s for both will safe you some typing and will simplify refactoring, if you ever need to do that.\n"", ""excerpt"": ""You can either write this instead of Eigen::<span class=\""highlight\"">Tensor</span>&lt;int,4&gt; (similar for <span class=\""highlight\"">Tensor</span>&lt;int,2&gt;)\n\nEigen::<span class=\""highlight\"">Tensor</span>&lt;int, 4, 0, int&gt;\n\n\nor you replace std::array&lt;int, 2&gt; by std::array&lt;Eigen::Index, 2&gt;. &hellip; Of course making typedefs for both will <span class=\""highlight\"">safe</span> you some typing and will simplify refactoring, if you ever need to do that. &hellip; "", ""title"": ""How to reshape a tensor in Eigen3?""}"

Why does model.losses return regularization losses?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""keras"", ""tensorflow2.0"", ""eager-execution""], ""question_score"": 10, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 56693863, ""item_type"": ""question"", ""score"": 10, ""last_activity_date"": 1561098102, ""creation_date"": 1561064685, ""body"": ""I have met a snippet of code of tensorflow 2.0, which is used for calculating the loss. The total loss is composed of two parts: 1) regularization loss, 2) prediction loss. My question is why  is regularization loss?  here is an instance of . I&#39;m kind of confused by the tensorflow official API documentation. tf.keras.Model, it says\n\nLosses which are associated with this Layer.\nVariable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a  will propagate gradients back to the corresponding variables.\n\nWhy could we get regularization loss via accessing  property? Also, what is eager safe? If  property is returning regularization loss, why is it named  instead of ?\n\n"", ""excerpt"": ""Variable regularization tensors are created when this property is accessed, so it is eager <span class=\""highlight\"">safe</span>: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. &hellip; Also, what is eager <span class=\""highlight\"">safe</span>? If losses property is returning regularization loss, why is it named losses instead of regularization_loss? &hellip; "", ""title"": ""Why does model.losses return regularization losses?""}"

How to set a fixed and proper Sequence Length in the Sentiment Analysis using LSTM?,Stack Overflow,N/A,"{""tags"": [""machine-learning"", ""text"", ""nlp"", ""lstm"", ""recurrent-neural-network""], ""question_score"": 1, ""is_accepted"": false, ""answer_id"": 56670760, ""is_answered"": false, ""question_id"": 56613410, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1560956513, ""creation_date"": 1560956513, ""body"": ""I will be assuming that you are implementing either a Keras or TensorFlow RNN in this answer, but really any other tool applies as well.\n\nFirst thing to mention is that you are not always forced to select a single sequence length. By defining the appropriate timesteps dimension of the input shape in your neural network as , you can have varying sequence lengths between different batches. The only restriction is that the sequence length within a single batch will have to be equivalent, and this is because of the predefined shape property of Tensors. Hence, you can feed in batches of say 50, 100, and 150 lengths of sequences to your neural network and see how it performs.\n\nHowever, setting a single sequence length is often preferred as it&#39;s more intuitive and easier to work with. You can simply train different models where the only changing variable is sequence length, and compare their validation accuracy. Different tasks will have different optimal sequence lengths: sentiment analysis may be performed with shorter sequence lengths whereas language models are generally performed with longer sequence lengths. A safe approach would be to take the median sequence length of your dataset.\n\nA perhaps safer approach is to either take the maximum sequence length, or a sequence length that doesn&#39;t result in a lot sentences getting truncated. However, this depends on your architecture. This approach will potentially work better with a concept like Convolutional RNNs (Kim et. al., 2014) where we use sliding windows to go over the sequence in short timesteps (3, 5, etc.) just like we would to a image with CNNs. Another potential direction is the local attention mechanism (Luong et. al., 2015), which adaptively focuses on a distinct subset of the hidden states derived from the input sequences. Then again, the sequence length is obviously a parameter to experiment with and we can&#39;t have a definite answer.\n"", ""excerpt"": ""A <span class=\""highlight\"">safe</span> approach would be to take the median sequence length of your dataset. &hellip; "", ""title"": ""How to set a fixed and proper Sequence Length in the Sentiment Analysis using LSTM?""}"

Can I remove gcc compiler after tensorflow installation?,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""gcc"", ""infrastructure""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 56258885, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1558536866, ""creation_date"": 1558534441, ""body"": ""I&#39;m sorry if this is not the right place to ask this.  If it is not, let me know and I&#39;ll close this question.\n\nI&#39;m trying to set up an environment for a bot we are working on, and part of that will require installing gcc on a server so we can install tensorflow.  However, my company has decided that compilers are unsafe, so part of the process for production will require removing the compiler from the server after a set amount of time. After tensor flow is installed, will it be safe to remove the gcc compiler from the server, or will it break my installation?\n"", ""excerpt"": ""After <span class=\""highlight\"">tensor</span> flow is installed, will it be <span class=\""highlight\"">safe</span> to remove the gcc compiler from the server, or will it break my installation? &hellip; "", ""title"": ""Can I remove gcc compiler after tensorflow installation?""}"

Extremely large gradient in last layer and small in the rest,Stack Overflow,N/A,"{""tags"": [""neural-network"", ""pytorch"", ""data-science"", ""backpropagation"", ""transformer-model""], ""question_score"": 4, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 56063210, ""item_type"": ""question"", ""score"": 4, ""last_activity_date"": 1557416837, ""creation_date"": 1557416837, ""body"": ""I’m very new to training neural nets but foolishly tried to implement my own novel architecture. It’s very similar to a transformer, in that the pipeline takes in a “batch of articles headlines” tensor, passes it through the encoder half of the Transformer, then through several transformations and versions of the transformer encoder. The output is then summed into a single vector and softmaxed. I also wrote a custom loss function.\n\nWhen I graph the average gradients per layer, it looks like this.\n\n(The last layer’s gradients are huge and the rest are barely existent. This is after clipping gradient to 0.25; before that the last layer&#39;s gradient was at around 1e7)\n\nHere’s my relevant code:\n\n\n\nThe layer with the large gradients is the coefficients and biases for a LayerNorm layer at the end of the transformer-esque architecture\n"", ""excerpt"": ""It’s very similar to a transformer, in that the pipeline takes in a “batch of articles headlines” <span class=\""highlight\"">tensor</span>, passes it through the encoder half of the Transformer, then through several transformations and &hellip; torch.Tensor(batch_size, len(assets))&gt;\n&quot;&quot;&quot;\ndef loss_f(model, XY):\n    examples, prices = XY\n    portfolios = torch.stack([model.forward(ex) for ex in examples], dim=0)\n    prices = Variable(prices)\n\n    # <span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""Extremely large gradient in last layer and small in the rest""}"

Restoring const-correctness for NN forward pass,Stack Overflow,N/A,"{""tags"": [""libtorch""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 55752123, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1555613311, ""creation_date"": 1555613311, ""body"": ""I am trying to implement a simple neural network using pytorch/libtorch. The following example is adapted from the libtorch cpp frontend tutorial.\n\n\n\nNote that the function  is declared . The code I am writing requires the evalutation of the NN to be a const function, which seems reasonable to me.\nThis code does not compile though. The compiler throws\n\n\n  error: no match for call to ‘(const torch::nn::Linear) (at::Tensor&amp;)’\n  x = linear1(x);\n\n\nI have found a way around this though, by defining the layers to be :\n\n\n\nSo my question is  \n\n\nWhy is the application of a layer on a tensor not \nIs using  the way to fix this and is it safe?\n\n\nMy intuition is that in the forward pass, the layers get assembled into a structure that can be used for the back-propagation, requiring some writing operation. If that&#39;s true, the question becomes how to assemble the layers in a first (non-) step and then evaluate the structure in a second () step.\n"", ""excerpt"": ""#include &lt;torch/torch.h&gt;\nstruct DeepQImpl : torch::nn::Module {\n    DeepQImpl(size_t N)\n        : linear1(2,5),\n          linear2(5,3) {}\n    torch::<span class=\""highlight\"">Tensor</span> forward(torch::<span class=\""highlight\"">Tensor</span> x) const {\n        x = &hellip; const\nIs using mutable the way to fix this and is it <span class=\""highlight\"">safe</span>? &hellip; "", ""title"": ""Restoring const-correctness for NN forward pass""}"

How to change retrain.py to take in a b64 image,Stack Overflow,N/A,"{""tags"": [""tensorflow""], ""question_score"": 0, ""is_accepted"": true, ""answer_id"": 55680492, ""is_answered"": false, ""question_id"": 55680023, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1555358146, ""creation_date"": 1555280019, ""body"": ""take a look at this post, it contains the info you need. If not, then reply and I&#39;ll help you prepare some code, you probbably need the url safe b64 variant though. \n\nEDIT\n\nyour code is a bit confusing, I don&#39;t think the input is already connected to your graph, have you looked at the graph  with tf.summary.FileWriter(&#39;output folder&#39;, sess.graph)?\n\nI&#39;m gonna gradually try to explain how you build some layers in front of your model, with some examples, this code should not be in retrain.py and can be ran after you trained the model.\n\n1) Load your tensorflow model in if it is build with savedModelBuilder or the simple save thing you can do it like this:\n\n\n\nThe tagconstants can be checked with the saved_model_cli tool, it is possible that this has to be empty [] in your case.\n\n2) add the layers/tensors you need, you need something that accept a byte string, or base 64 in this case, that decodes it and transforms it into a 3D image:\n\n\n\nThe other tensors like converting to float, dim_expanding and reshaping should already be in the graph if you got it from retrain.py.\n\n3) implement them into your graph by feeding them into it.\n\n\n\n4) create a saved model and check if everything is like you want it to be!\n\n\n\n5) if you get errors try to debug them with tensorboard\n\n\n\nI hoped I helped a bit, this code will not work from the first try, you need to puzzle with some parts. If you truly cannot succeed, then you should share the model, maybe I can do it then and share the code with you, if I have time.\n"", ""excerpt"": ""If not, then reply and I&#39;ll help you prepare some code, you probbably need the url <span class=\""highlight\"">safe</span> b64 variant though. &hellip; "", ""title"": ""How to change retrain.py to take in a b64 image""}"

How to get data from Tensor object in c++,Stack Overflow,N/A,"{""tags"": [""c++"", ""tensorflow"", ""eigen""], ""question_score"": 2, ""is_accepted"": false, ""answer_id"": 54647457, ""is_answered"": false, ""question_id"": 54641650, ""item_type"": ""answer"", ""score"": 7, ""last_activity_date"": 1549983986, ""creation_date"": 1549965902, ""body"": ""The  class allows you to access its contents through several methods. With  you get a flattened version of the array,  gives you a full Eigen tensor, and then there are a few other like / (like  with number of dimensions fixed to 1 or 2) and // (gives you a tensor with some dimensions collapsed). You can use the one that suits you best. In this case, for example if you want to print all the values in the tensor, you can use  and compute the corresponding offset or use  if you know that the number of dimensions is 4:\n\n\n\nNote that, although these methods create  objects, which are not really expensive, you may prefer to call them only once and then query the tensor object multiple times. For example:\n\n\n\nEDIT:\n\nIf you want to obtain a pointer to the data of the tensor (for example to build another object from the same buffer avoiding copies or iteration), you can also do that. One option is to use the  method, which returns a , which is in turn a , which is just a polyfill for . So the  method of this object will give you a pointer to the underlying byte buffer for the tensor (note the warning in the documentation of : &quot;the underlying tensor buffer is refcounted&quot;, so do not let the returned object be destroyed while you use the buffer). You can therefore do:\n\n\n\nThis however gives you a pointer to  so you would have to cast it to use it as float. It should be safe, but it looks ugly, so you can let Eigen do that for you. All Eigen objects have a  method that returns a pointer of its type to the underlying buffer. For example:\n\n\n"", ""excerpt"": ""For example:\n\n// Make <span class=\""highlight\"">tensor</span>\ntf::TTypes&lt;float_t, 4&gt;::<span class=\""highlight\"">Tensor</span> outputTensor0 = outputs[0].<span class=\""highlight\"">tensor</span>&lt;float_t, 4&gt;();\n// Query <span class=\""highlight\"">tensor</span> multiple times\nfor (...)\n{\n    std::cout &lt;&lt; outputTensor0(ni, hi, wi, ci) &lt;&lt; &hellip; It should be <span class=\""highlight\"">safe</span>, but it looks ugly, so you can let Eigen do that for you. All Eigen objects have a .data method that returns a pointer of its type to the underlying buffer. &hellip; "", ""title"": ""How to get data from Tensor object in c++""}"

Tensorflow Projected Gradient Descent with Box Constraints using native Optimizer&#39;s apply_gradients,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""gradient-descent""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 54263151, ""is_answered"": false, ""question_id"": 54186685, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1547859177, ""creation_date"": 1547858765, ""body"": ""In tf.get_variable there is a constraint parameter, I guess you can use it to do your box constraints on the weights variable, using  tf.clip_by_value as projection function.\n\n\n  constraint: An optional projection function to be applied to the variable after being updated by an Optimizer (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected Tensor representing the value of the variable and return the Tensor for the projected value (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training.\n\n"", ""excerpt"": ""The function must take as input the unprojected <span class=\""highlight\"">Tensor</span> representing the value of the variable and return the <span class=\""highlight\"">Tensor</span> for the projected value (which must have the same shape). &hellip; Constraints are not <span class=\""highlight\"">safe</span> to use when doing asynchronous distributed training. &hellip; "", ""title"": ""Tensorflow Projected Gradient Descent with Box Constraints using native Optimizer&#39;s apply_gradients""}"

"Is this declaration of an Eigen::Tensor in C++ safe, or buggy? And should I submit an issue for it?",Stack Overflow,N/A,"{""tags"": [""c++"", ""type-conversion"", ""eigen"", ""tensor""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 54233187, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1547719219, ""creation_date"": 1547718786, ""body"": ""Using Eigen&#39;s unsupported Tensor module, if I do:\n\n\n\nI get the following error:\n\n\n\nBut the code compiles OK if I explicitly cast the dimensions to long int:\n\n\n\nQuestions:\n\n\nFor what size variable will this become unsafe? If at all?\nSurely Eigen should be generally accepting a  type as a dimension argument? Should I file a bug report for this or is it intended behaviour here?\n\n\nI&#39;m using C++11, clang on Mac OSX (haven&#39;t tested other platforms).\n"", ""excerpt"": ""Using Eigen&#39;s unsupported <span class=\""highlight\"">Tensor</span> module, if I do:\n\n    size_t dim0 = 3;\n    size_t dim1 = 2;\n    size_t dim2 = 4;\n    Eigen::<span class=\""highlight\"">Tensor</span>&lt;double, 3&gt; var(dim0, dim1, dim2);\n\n\nI get the following error:\n\n/usr/ &hellip; local/include/eigen3/unsupported/Eigen/CXX11/src/<span class=\""highlight\"">Tensor</span>/TensorDimensions.h:287:167: error: non-constant-expression cannot be narrowed from type &#39;unsigned long&#39; to &#39;std::__1::array&lt;long, 3&gt;::value_type&#39; &hellip; "", ""title"": ""Is this declaration of an Eigen::Tensor in C++ safe, or buggy? And should I submit an issue for it?""}"

Return an expression containing ArrayWrapper,Stack Overflow,N/A,"{""tags"": [""eigen3""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 53712461, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1544560125, ""creation_date"": 1544470608, ""body"": ""I am writing C++/Python hybrid. The library that glue the two parts support Eigen matrix/array but not tensor.\n\nIs it safe to do something like this?\n\n\n\nOr, is it better to do this?\n\n\n"", ""excerpt"": ""The library that glue the two parts support Eigen matrix/array but not <span class=\""highlight\"">tensor</span>.\n\nIs it <span class=\""highlight\"">safe</span> to do something like this? &hellip; "", ""title"": ""Return an expression containing ArrayWrapper""}"

TensorFlow custom operation: Thread safety of output tensor,Stack Overflow,N/A,"{""tags"": [""c++"", ""multithreading"", ""tensorflow""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 52727882, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1539117614, ""creation_date"": 1539112415, ""body"": ""I have a custom operation for TensorFlow which benefits from doing a certain computation in parallel.\nHowever, one user reported a crash when using this parallel computation (on my computer everything works fine).\n\n\nIs there anything I have to take care in the Compute method when working with threads?\nIs reading/writing tensors already thread-safe or do I have to take care?\nEspecially, do I have to synchronize my threads when writing to the output tensor? See simplified code snippet below.\n\n\n\n    void Compute(OpKernelContext* context) override \n    {\n        // output tensor\n        Tensor* outputTensor = nullptr;\n        OP_REQUIRES_OK(context, context-&gt;allocate_output(0, TensorShape({maxI, maxJ}), &amp;outputTensor));\n        auto outputMapped = outputTensor-&gt;tensor();\n\n        // do some computations for index (i, j) and write result to index (i, j) of output tensor\n        // do this for each (i, j) combination\n        outputMapped(i, j) = result; \n\n        // computing the result for each index (i, j) in parallel would speed up the computation\n        // Question: is it allowed to write to outputMapped(i, j) from different threads without synchronization?\n    }\n\n\nThis is the relevant code:\n\n\nCompute(...) method: output tensor\nStart parallel work\nWrite to output tensor from each thread in parallel\n\n\nI looked through the TF docs, but it only mentions that TF itself may call Compute(...) in parallel. But I could not find any information about thread-safety of the tensors. Does anyone have more information about this? \n"", ""excerpt"": ""Is reading/writing tensors already thread-<span class=\""highlight\"">safe</span> or do I have to take care?\nEspecially, do I have to synchronize my threads when writing to the output <span class=\""highlight\"">tensor</span>? See simplified code snippet below. &hellip; void Compute(OpKernelContext* context) override \n    {\n        // output <span class=\""highlight\"">tensor</span>\n        <span class=\""highlight\"">Tensor</span>* outputTensor = nullptr;\n        OP_REQUIRES_OK(context, context-&gt;allocate_output(0, TensorShape({maxI, maxJ &hellip; "", ""title"": ""TensorFlow custom operation: Thread safety of output tensor""}"

TensorFlow: How to use &#39;tf.data&#39; instead of &#39;load_csv_without_header&#39;?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""deep-learning"", ""pycharm"", ""tensorflow-datasets""], ""question_score"": 2, ""is_accepted"": true, ""answer_id"": 52662295, ""is_answered"": false, ""question_id"": 51113982, ""item_type"": ""answer"", ""score"": 7, ""last_activity_date"": 1538731115, ""creation_date"": 1538731115, ""body"": ""Using  to work with a  file:\n\nFrom TensorFlow&#39;s official documentation:\n\n\n  The tf.data module contains a collection of classes that allows you to easily load data, manipulate it, and pipe it into your model.\n\n\nUsing the API,  is intended as the new standard of interfacing with data in TensorFlow. It represent &quot;a sequence of elements, in which each element contains one or more Tensor objects&quot;. For a CSV, an element is just a single row of training example, represented as a pair of tensor that correspond to the data (our ) and the label (&quot;target&quot;) respectively. \n\nUsing the API, the primary method of extracting each row (or each element more accurately) in a tensorflow dataset () is by consuming the Iterator and TensorFlow has an API named  for that. To return the next row, we can call  on the Iterator for example.\n\nNow onto the code to take  and transform that into our tensorflow dataset. \n\nMethod 1:  and \n\nWith more recent versions of TensorFlow&#39;s Estimator API, instead of , you&#39;d read your CSV or using the more generic  instead. You can chain that with  to skip the first row if there is a header row, but in your case, that wasn&#39;t necessary. \n\nYou can then use  to pack decode each line of your CSV into its own respective fields.\n\nThe code solution:\n\n\n\nYou would get:\n\n\n\nYou can verify the :\n\n\n\nYou can also use  to iterate through the iterator:\n\n\n\nMethod 2:  to construct a dataset object from numpy or pandas\n\n\n\nAnother (more elaborated) example:\n\n\n\n\n\nI also strongly suggest this article and this, both from the official documentation; Safe to say that should cover most if not all your use case and will help you migrate from the deprecated  function.\n"", ""excerpt"": ""It represent &quot;a sequence of elements, in which each element contains one or more <span class=\""highlight\"">Tensor</span> objects&quot;. &hellip; Construct a dataset, and configure batching/repeating.\nds = tf.data.Dataset.from_tensor_slices((features,targets))\n\n\n\n\nI also strongly suggest this article and this, both from the official documentation; <span class=\""highlight\"">Safe</span> &hellip; "", ""title"": ""TensorFlow: How to use &#39;tf.data&#39; instead of &#39;load_csv_without_header&#39;?""}"

How to grab one tensor from an existing model and use it in another one?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 51852089, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1534391452, ""creation_date"": 1534299633, ""body"": ""What I want to do is to grab some weights and biases from an existing trained model, and then use them in my customized op (model or graph).\n\nI can restore model with:\n\n\n\nAnd then grab tensor:\n\n\n\nMy question is, if I want to use that  in another context (model or graph), how to safely copy its value to the new graph, e.g.:\n\n\n\nAny help is welcome, thank you so much.\n\n\n\nThanks @Sorin for the inspiration, I found a simple and clean way to do this:\n\n\n"", ""excerpt"": ""restore model with:\n\n# Create context\nwith tf.Graph().as_default(), tf.Session() as sess:\n    # Create model\n    with tf.variable_scope(&#39;train&#39;):\n        train_model = MyModel(some_args)\n\n\nAnd then grab <span class=\""highlight\"">tensor</span> &hellip; "", ""title"": ""How to grab one tensor from an existing model and use it in another one?""}"

tensorflow object detection faster rcnn randomly fails,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""object-detection""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 4, ""is_answered"": true, ""question_id"": 45007328, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1531467534, ""creation_date"": 1499675774, ""body"": ""I am trying to use the new object detection api in tensorflow 1.2, and the example faster-rcnn config, to train on a custom dataset. The error I get is related to some tensor shapes, but it happens seemingly randomly during training, and the exact shape changes too.\n\n\n\nAs you can see, it runs for a variable number of steps correctly, and then gives me . What I dont understand is why this error is being triggered, and furthermore the where the incompatible shape comes from, as this changes as well between runs.\n\nAs I did convert my dataset into the TF format, I was unsure whether that was my issue. However, I have successfully trained for several days on the same dataset with their ssd implementation, so I think it is safe to say the data is formatted correctly.\n\nEDIT: The label map file is here. Again I would like to point out that this same dataset runs perfectly using ssd.\n"", ""excerpt"": ""The error I get is related to some <span class=\""highlight\"">tensor</span> shapes, but it happens seemingly randomly during training, and the exact shape changes too. &hellip; However, I have successfully trained for several days on the same dataset with their ssd implementation, so I think it is <span class=\""highlight\"">safe</span> to say the data is formatted correctly. &hellip; "", ""title"": ""tensorflow object detection faster rcnn randomly fails""}"

How could I limit the range of a variable in tensorflow,Stack Overflow,N/A,"{""tags"": [""variables"", ""tensorflow""], ""question_score"": 9, ""is_accepted"": false, ""answer_id"": 47672475, ""is_answered"": false, ""question_id"": 47005283, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1531297326, ""creation_date"": 1512557417, ""body"": ""The proper way to do this would be to pass the clipping function  as the  argument to the  constructor:\n\n\n\nFrom the docs of :\n\n\n  constraint: An optional projection function to be applied to the\n  variable after being updated by an Optimizer (e.g. used to implement\n  norm constraints or value constraints for layer weights). The function\n  must take as input the unprojected Tensor representing the value of\n  the variable and return the Tensor for the projected value (which must\n  have the same shape). Constraints are not safe to use when doing\n  asynchronous distributed training.\n\n\nOr you might want to consider simply adding a nonlinearity  on top of your variable.\n\n\n\nThis will transform your variable to range between 0 and 1. Read more about activation functions here.\n"", ""excerpt"": ""The function\n  must take as input the unprojected <span class=\""highlight\"">Tensor</span> representing the value of\n  the variable and return the <span class=\""highlight\"">Tensor</span> for the projected value (which must\n  have the same shape). &hellip; Constraints are not <span class=\""highlight\"">safe</span> to use when doing\n  asynchronous distributed training.\n\n\nOr you might want to consider simply adding a nonlinearity tf.sigmoid on top of your variable. &hellip; "", ""title"": ""How could I limit the range of a variable in tensorflow""}"

When is it safe to cache tf.Tensors?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 49550723, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1522313566, ""creation_date"": 1522309913, ""body"": ""Let&#39;s say we have some method  we call during graph construction time that returns some  or a nested structure of them every time is called, and multiple other methods that make use of &#39;s result. For efficiency and to avoid spamming the TF graph with unnecessary repeated operations, it might be tempting to make  cache its result (to reuse the subgraph it produces) the first time is called. However, that will fail if  is ever used in the context of a control flow, like ,  or .\n\nMy questions are:\n\n\nWhen is it safe to cache  objects in such a way that does not cause problems with control flows? Perhaps is there some way to retrieve the control flow under which a  was created (if any), store it and compare it later to see if a cached result can be reused?\nHow would the answer to the question above apply to ?\n\n\n(Question text updated to make clearer that  creates a new set of tensors every time is called)\n"", ""excerpt"": ""My questions are:\n\n\nWhen is it <span class=\""highlight\"">safe</span> to cache tf.Tensor objects in such a way that does not cause problems with control flows? &hellip; "", ""title"": ""When is it safe to cache tf.Tensors?""}"

Variadic Function,Stack Overflow,N/A,"{""tags"": [""c++"", ""c++11"", ""variadic-templates"", ""variadic-functions"", ""variadic""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 4, ""is_answered"": true, ""question_id"": 47849807, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1521710205, ""creation_date"": 1513457183, ""body"": ""I am trying to write a class to represent a tensor  and would like to provide the syntax  for a 2 dimensional tensor,  for a 3 dimensional tensor and so on.\n\nWhat I want to know is if there is a c++ type safe way to declare such  that accepts any number of  arguments (besides the C style with the macros ) and how to use said arguments inside the function.\n\nThanks for your time.\n"", ""excerpt"": ""I am trying to write a class to represent a <span class=\""highlight\"">tensor</span> <span class=\""highlight\"">Tensor</span> and would like to provide the syntax <span class=\""highlight\"">tensor</span>(i, j) for a 2 dimensional <span class=\""highlight\"">tensor</span>, <span class=\""highlight\"">tensor</span> (i, j, k) for a 3 dimensional <span class=\""highlight\"">tensor</span> and so on. &hellip; What I want to know is if there is a c++ type <span class=\""highlight\"">safe</span> way to declare such Tensor:operator()(int, int, ...) that accepts any number of int arguments (besides the C style with the macros va_start va_end) and &hellip; "", ""title"": ""Variadic Function""}"

How to test a function that load the model file (FrozenGraph like SavedModel) in TensorFlow C?,Stack Overflow,N/A,"{""tags"": [""c++"", ""tensorflow""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 49045081, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1519953441, ""creation_date"": 1519891126, ""body"": ""As we all known, the model file for TensorFlow can be divided into two classes, frozen graph or a SavedModel.\nthe structure of a saved model directory can be:\n\nthe frozen graph is a single file:\n\nI want to run a predefined graph in C. In c_api.cc contains the function\n\nTF_LoadSessionFromSavedModel\n\n\nbut not contains\n\nTF_LoadSessionFromFrozenGraph\n\nso I added the function to c_api.cc.\n\nNow I want to test the function TF_LoadSessionFromFrozenGraph.\nI found the code of testing TF_LoadSessionFromSavedModel is in c_api_test.cc\n\nHow can I imitate the code of savedmodel test to write code of frozengraph test?\nI am new to c, please forgive me if my question is stupid.\n"", ""excerpt"": ""This is <span class=\""highlight\"">safe</span> as long as Session\n  // extends using GraphDefs. &hellip; <span class=\""highlight\"">Tensor</span> input(tensorflow::DT_STRING, TensorShape({4}));\n  for (tensorflow::int64 i = 0; i &lt; input.NumElements(); ++i) {\n    tensorflow::Example example;\n    auto* feature_map = example.mutable_features( &hellip; "", ""title"": ""How to test a function that load the model file (FrozenGraph like SavedModel) in TensorFlow C?""}"

What is the best way to implement weight constraints in TensorFlow?,Stack Overflow,N/A,"{""tags"": [""tensorflow""], ""question_score"": 36, ""is_accepted"": false, ""answer_id"": 47189751, ""is_answered"": false, ""question_id"": 33694368, ""item_type"": ""answer"", ""score"": 24, ""last_activity_date"": 1517168040, ""creation_date"": 1510175407, ""body"": ""As of TensorFlow 1.4, there is a new argument to  that allows to pass a constraint function that is applied after the update of the optimizer. Here is an example that enforces a non-negativity constraint:\n\n\n\n\n  constraint: An optional projection function to be applied to the\n  variable\n          after being updated by an  (e.g. used to implement norm\n          constraints or value constraints for layer weights). The function must\n          take as input the unprojected Tensor representing the value of the\n          variable and return the Tensor for the projected value\n          (which must have the same shape). Constraints are not safe to\n          use when doing asynchronous distributed training.\n\n"", ""excerpt"": ""The function must\n          take as input the unprojected <span class=\""highlight\"">Tensor</span> representing the value of the\n          variable and return the <span class=\""highlight\"">Tensor</span> for the projected value\n          (which must have the same shape &hellip; Constraints are not <span class=\""highlight\"">safe</span> to\n          use when doing asynchronous distributed training. &hellip; "", ""title"": ""What is the best way to implement weight constraints in TensorFlow?""}"

How to design a neural network to recognize distinct features?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""neural-network"", ""artificial-intelligence"", ""shapes""], ""question_score"": 3, ""is_accepted"": true, ""answer_id"": 48284628, ""is_answered"": false, ""question_id"": 48283911, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1516116329, ""creation_date"": 1516116329, ""body"": ""\n  Naturally, after passing this tensor through several tf.layers.dense layers, the output would also be a 3D tensor while I would only need a single number as an output. \n\n\nYou would need a 1D tensor with  different outputs, correct? One output per full team in the batch.\n\n\n  On the other hand, If I put all team features into a single array, I believe the network wouldn&#39;t have any way of knowing that these are in fact features of 5 different people.\n\n\nThis is the most common solution, and probably also the best. Definitely the easiest. This solution only works if you are sure that every team always has 5 people, I suppose that is a safe assumption?\n\nThis solution is often going to be referred to as &quot;flattening&quot; (in many frameworks you can convert a  tensor into a  tensor using a function called ). Yes, you&#39;re correct that the neural network has no way of &#39;&#39;knowing&#39;&#39; that the age feature of team member X is somehow similarly important as the age feature of team member Y, or &#39;&#39;knowing&#39;&#39; that the age and gender features of team member X are somehow more closely related to each other than the age feature of team member X and the gender feature of team member Y... but that&#39;s generally fine. If it turns out to be important to learn something like that, it can still do so.\n\n\n\nOne additional tip: if the order in which different persons appear in the same team does not matter (e.g. if there is not some important, consistent position-based ordering or something like that), I&#39;d recommend augmenting your data by also including shuffled versions of the teams you already have to your data. For example, if you data contains a team [P1, P2, P3, P4, P5] (where every P is a sequence of features corresponding to one person), I&#39;d augment the dataset by also adding a team [P2, P1, P3, P4, P5], and a team [P3, P1, P2, P4, P5], etc. You can basically add all possible permutations.\n"", ""excerpt"": ""Naturally, after passing this <span class=\""highlight\"">tensor</span> through several tf.layers.dense layers, the output would also be a 3D <span class=\""highlight\"">tensor</span> while I would only need a single number as an output. &hellip; This solution only works if you are sure that every team always has 5 people, I suppose that is a <span class=\""highlight\"">safe</span> assumption? &hellip; "", ""title"": ""How to design a neural network to recognize distinct features?""}"

Error while clipping gradient,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""gradient""], ""question_score"": 3, ""is_accepted"": true, ""answer_id"": 46947231, ""is_answered"": false, ""question_id"": 46858506, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1508999257, ""creation_date"": 1508999257, ""body"": ""The key point of this type of problem is, trainable_variable list may contain multiple tensors who are not initialized or used in the graph. make sure you contain all the tensor safely in the trainable_variable list. Sometimes even they might contain NaN for gradient computation. This type of error may also introduce for unnatural value.\n"", ""excerpt"": ""The key point of this type of problem is, trainable_variable list may contain multiple tensors who are not initialized or used in the graph. make sure you contain all the <span class=\""highlight\"">tensor</span> safely in the trainable_variable &hellip; "", ""title"": ""Error while clipping gradient""}"

Visualizing output of convolutional layer in tensorflow,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""conv-neural-network""], ""question_score"": 38, ""is_accepted"": true, ""answer_id"": 33816991, ""is_answered"": false, ""question_id"": 33802336, ""item_type"": ""answer"", ""score"": 37, ""last_activity_date"": 1507749873, ""creation_date"": 1447979633, ""body"": ""I don&#39;t know of a helper function but if you want to see all the filters you can pack them into one image with some fancy uses of .\n\nSo if you have a tensor that&#39;s  x  x  x \n\n\n\nSo in this example , , \n\nfirst slice off 1 image, and remove the  dimension\n\n\n\nNext add a couple of pixels of zero padding around the image\n\n\n\nThen reshape so that instead of 32 channels you have 4x8 channels, lets call them  and .\n\n\n\nNow the tricky part.  seems to return results in C-order, numpy&#39;s default. \n\nThe current order, if flattened, would list all the channels for the first pixel (iterating over  and ), before listing the channels of the second pixel (incrementing ). Going across the rows of pixels () before incrementing to the next row ().\n\nWe want the order that would lay out the images in a grid.\nSo you go across a row of an image (), before stepping along the row of channels (), when you hit the end of the row of channels you step to the next row in the image () and when you run out or rows in the image you increment to the next row of channels (). so:\n\n\n\nPersonally I prefer  for fancy transposes, for readability, but it&#39;s not in  yet.\n\n\n\nanyway, now that the pixels are in the right order, we can safely flatten it into a 2d tensor:\n\n\n\ntry  on that, you should get a grid of little images. \n\nBelow is an image of  what one gets after following all the steps here.\n\n\n"", ""excerpt"": ""So if you have a <span class=\""highlight\"">tensor</span> that&#39;s images x ix x iy x channels\n\n&gt;&gt;&gt; V = tf.Variable()\n&gt;&gt;&gt; print V.get_shape()\n\nTensorShape([Dimension(-1), Dimension(256), Dimension(256), Dimension(32)])\n\n\nSo in this example &hellip; newtensor = np.einsum(&#39;yxYX-&gt;YyXx&#39;,oldtensor)\n\n\nanyway, now that the pixels are in the right order, we can safely flatten it into a 2d <span class=\""highlight\"">tensor</span>:\n\n# image_summary needs 4d input\nV = tf.reshape(V,(1,cy*iy, &hellip; "", ""title"": ""Visualizing output of convolutional layer in tensorflow""}"

How does tf.nn.moments calculate variance?,Stack Overflow,N/A,"{""tags"": [""tensorflow""], ""question_score"": 2, ""is_accepted"": true, ""answer_id"": 45504713, ""is_answered"": false, ""question_id"": 45504394, ""item_type"": ""answer"", ""score"": 6, ""last_activity_date"": 1501843288, ""creation_date"": 1501843288, ""body"": ""The problem is that  is an integer tensor and TensorFlow, instead of forcing a conversion, performs the computation as good as it can without changing the type (so the outputs are also integers). You can pass float numbers in the construction of  or specify the  parameter of :\n\n\n\nThen you get the expected result:\n\n\n\nAbout the  parameter, it seems to allow you specify a value to, well, &quot;shift&quot; the input. By shift they mean subtract, so if your input is  and you give a  of, say, , TensorFlow would first subtract that amount and compute the moments from . In general, it seems safe to just leave it as , which will perform a shift by the mean of the input, but I suppose there may be cases where giving a predetermined shift value (e.g. if you know or have an approximate idea of the mean of the input) may yield better numerical stability.\n"", ""excerpt"": ""The problem is that x is an integer <span class=\""highlight\"">tensor</span> and TensorFlow, instead of forcing a conversion, performs the computation as good as it can without changing the type (so the outputs are also integers). &hellip; In general, it seems <span class=\""highlight\"">safe</span> to just leave it as None, which will perform a shift by the mean of the input, but I suppose there may be cases where giving a predetermined shift value (e.g. if you know or have &hellip; "", ""title"": ""How does tf.nn.moments calculate variance?""}"

How to restore my loss from a saved meta graph?,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""scope"", ""restore""], ""question_score"": 2, ""is_accepted"": true, ""answer_id"": 44522505, ""is_answered"": false, ""question_id"": 44510024, ""item_type"": ""answer"", ""score"": 5, ""last_activity_date"": 1498994476, ""creation_date"": 1497359371, ""body"": ""To get your training step back, the documentation suggests you add it to a collection before saving it as a way to be able to point at it to after restoring your graph.\n\nSaving:\n\n\n\nRestore:\n\n\n\nWhy did your attempt at recovering the tensor by name fail?\n\nYou can indeed get the tensor by its name -- the catch is that you need the correct name. And notice that your  argument to  is a scope name, not the name of the returned operation. This can be confusing, as other operations, such as , accept a  argument.\n\nIn the end, the name of your  operation is , which you can use to get it by name.\n\nThat is, until it breaks again in the future when you update tensorflow.  does not give you any guarantee on the name of its output, so it very well may change for some reason.\n\nI think this is what motivates the use of collections: the lack of guarantee on the names of the operators you don&#39;t control yourself.\n\nAlternatively, if for some reason you really want to use names, you could rename your operator like this:\n\n\n\nThen you can rely on  safely.\n"", ""excerpt"": ""mlp_model-1000.meta&#39;)\nsaver.restore(sess, tf.train.latest_checkpoint(&#39;L:\\\\model\\\\&#39;))\n# recover op through collection\ntrain_op = tf.get_collection(&#39;train_op&#39;)[0]\n\n\nWhy did your attempt at recovering the <span class=\""highlight\"">tensor</span> &hellip; You can indeed get the <span class=\""highlight\"">tensor</span> by its name -- the catch is that you need the correct name. &hellip; "", ""title"": ""How to restore my loss from a saved meta graph?""}"

Type level programming to represent multidimensional arrays (Tensors),Stack Overflow,N/A,"{""tags"": [""arrays"", ""haskell"", ""multidimensional-array"", ""type-level-computation"", ""tensor""], ""question_score"": 3, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 43833191, ""item_type"": ""question"", ""score"": 3, ""last_activity_date"": 1494242915, ""creation_date"": 1494170449, ""body"": ""I would like to have a type to represent multidimensional arrays (Tensors) in a type safe way. so I could write for example: \nthat would represent a multidimensional array that has 5 element , each of which has 3 elements each of which have 2 elements, where all elements are s\n\nHow would you define this type using type level programming?\n\nEdit:\n\nAfter the wonderful answer by Alec, Which implemented this using s, \n\nI wonder if you could take this a step further, and support multiple implementations of a  and of the operations on tensors and serialization of tensors\n\nsuch that you could have for example:\n\n\n or  implementations using \npure  implementations\nimplementation that only prints the graph of computation and does not compute anything\nimplementation which caches results on disk\nparallel or distributed computation\netc...\n\n\nAll type safe and easy to use.\n\nMy intention is to make a library in Haskell much like  but type-safe and much more extensible, using automatic differentiation (ad library), and exact real arithmetic (exact-real library)\n\nI think a functional language like  is much more appropriate for these things (for all things in my opinion) than the python ecosystem which sprouted somehow.\n\n\nHaskell is purely functional, much more sutible for computational programming than python\nHaskell is much more efficient than python and can be compiled to binary\nHaskell&#39;s laziness (arguably) removes the need to optimize the computation graph, and makes code much simpler that way\nmuch more powerful abstractions in Haskell\n\n\nAlthough i see the potential, i&#39;m just not well versed enough (or smart enough) for this type-level programming, so i don&#39;t know how to implement such a thing in Haskell and get it to compile. \n\nThat&#39;s where I need your help.\n"", ""excerpt"": ""I would like to have a type to represent multidimensional arrays (Tensors) in a type <span class=\""highlight\"">safe</span> way. so I could write for example: zero :: <span class=\""highlight\"">Tensor</span> (5,3,2) Integer\nthat would represent a multidimensional array &hellip; All type <span class=\""highlight\"">safe</span> and easy to use. &hellip; "", ""title"": ""Type level programming to represent multidimensional arrays (Tensors)""}"

A fast way to iterate through multidimensional arrays with different shapes?,Stack Overflow,N/A,"{""tags"": [""arrays"", ""c++11"", ""multidimensional-array"", ""tensor""], ""question_score"": 3, ""is_accepted"": false, ""answer_id"": 43356890, ""is_answered"": false, ""question_id"": 43356889, ""item_type"": ""answer"", ""score"": 5, ""last_activity_date"": 1491947874, ""creation_date"": 1491947874, ""body"": ""After playing around for some time led to another approach in which we can combine C++11’s variadic templates and lambda functions with template metaprogramming to unroll the desired number of for loops:\n\n\n\nNote also that the tuple value and the shape can be safely declared __restrict, meaning that they point to distinct memory locations, because they will be constructed specifically for iteration and then deallocated. Values indexed by such pointers do not need to be re-read from memory when another pointer is dereferenced and changed (“pointer aliasing” problem). When invoking ForEachFixedDimension::template apply, the typename FUNCTION (possibly a lambda function) and the template parameter pack typename ...TENSORS (variadic support) can be inferred at compile time based on the contents of the tensor args... and the argument types to function. \n\nThe desired number of unrolled for loops can be looked up at runtime:\n\n\n\nNote that here, even though template recursion is used, the dimension need not be known until runtime. This is essentially achieved by using templates as a form of just-in-time (JIT) compilation, precomputing strategies for all dimensionalities of interest and then looking up the correct one at runtime. \n\nSo the methods were tested with Benchmarks. In benchmark 1, data is copied from a tensor of shape (210, 29, 28) to a tensor of shape (29, 29, 25). In benchmark 2, an inner product between two tensors of shape (210, 29, 28) and (29, 29, 25) is computed (visiting only tuple indices shared by both). The implementation with template recursion was compared with other alternative methods: tuple iteration; tuple iteration where the dimension is known at compile time; integer reindexing; integer reindexing where the axes are restricted to powers of 2; numpy; C-style for loops (hard coded); vectorised Fortran code; for loops in Go.\n\nIt turns out that the template recursion is faster than tuple indexing and the method used by boost:\n\n\n\n\n\nThe grey numbers represent the mean runtime and the error bars the min and max. Here are the ways those were implemented for benchmark 1 for each method:\n\n\n\nSurprisingly, integer reindexing (even when the axes were powers of 2) was substantially slower than making a tuple counter. And the version with template recursion was sometimes much faster (including 30% faster than boost, even though boost::multi_array must know the dimension at compile time). \n\nHere is a another examples of how you would use this nested for loop trick with template recursion:\n\n\n\nAnd an implementations of multidimensional convolution via tuple iteration, the version with template recursion and numpy were also compared by convolving two matrices, each with shape (28,23).\n\n\n\n \n\nThe benchmarks were timed on an 2.0 GHz Intel Core i7 chip with optimisations (-std=c++11 -Ofast -march=native - mtune=native -fomit-frame-pointer). All Fortran implementations use axes in reversed order and access data in a cache-optimised fashion, because Fortran uses column-major array format. \nDetails and source code (a simple multidimensional array library where the dimension does not need to be known at compile time) can be found in this small journal article.\n"", ""excerpt"": ""In benchmark 1, data is copied from a <span class=\""highlight\"">tensor</span> of shape (210, 29, 28) to a <span class=\""highlight\"">tensor</span> of shape (29, 29, 25). &hellip; <span class=\""highlight\"">Tensor</span>&lt;double&gt; triot_naive_convolve(const <span class=\""highlight\"">Tensor</span>&lt;double&gt; &amp; lhs, const <span class=\""highlight\"">Tensor</span>&lt;double&gt; &amp; rhs) { \n  assert(lhs.dimension() == rhs.dimension());\n\n  <span class=\""highlight\"">Tensor</span>&lt;double&gt; result(lhs.data_shape() + rhs.data_shape() &hellip; "", ""title"": ""A fast way to iterate through multidimensional arrays with different shapes?""}"

Threading in tensorflow&#39;s input pipeline,Stack Overflow,N/A,"{""tags"": [""multithreading"", ""machine-learning"", ""tensorflow"", ""neural-network""], ""question_score"": 6, ""is_accepted"": true, ""answer_id"": 43243034, ""is_answered"": false, ""question_id"": 43015025, ""item_type"": ""answer"", ""score"": 4, ""last_activity_date"": 1491433779, ""creation_date"": 1491433466, ""body"": ""All of your data loading operations are performed within the tensorflow graph, what you&#39;ll want to do is launch one or more threads to iterate over the reader/enqueue operations. Tensorflow provides a QueueRunner class that does exactly that. The Coordinator class allows you to manage the threads pretty trivially.\n\nhttps://www.tensorflow.org/programmers_guide/threading_and_queues\n\nThis is the example code from the link above:\n\n\n\nIf you were loading/preprocessing samples outside of the graph (in your own code, not using TF operations), then you wouldn&#39;t use QueueRunner, instead you would use your own class to enqueue data using a  command in a loop.\n\nQ1: Number of threads is handled with: \n\nQ2: TF sessions are thread safe, each call to  sees a consistent snapshot of the current variables as of when it begin. Your QueueRunner enqueue ops can run any number of threads. They&#39;ll queue up in a thread-safe manner.\n\nQ3: I haven&#39;t used  myself, but I think you&#39;d have to request a tensor later in the graph that  the data, just add that tensor to your list of requests in \n"", ""excerpt"": ""They&#39;ll queue up in a thread-<span class=\""highlight\"">safe</span> manner. &hellip; Q3: I haven&#39;t used tf.train.string_input_producer myself, but I think you&#39;d have to request a <span class=\""highlight\"">tensor</span> later in the graph that dequeued the data, just add that <span class=\""highlight\"">tensor</span> to your list of requests in sess.run &hellip; "", ""title"": ""Threading in tensorflow&#39;s input pipeline""}"

Haskell GADTs - making a type-safe Tensor types for Riemannian geometry,Stack Overflow,N/A,"{""tags"": [""haskell"", ""type-safety"", ""gadt"", ""data-kinds""], ""question_score"": 5, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 43156781, ""item_type"": ""question"", ""score"": 5, ""last_activity_date"": 1491054293, ""creation_date"": 1491048991, ""body"": ""I want to make a type safe implementation of Tensor calculus in Haskell using GADT&#39;s, so the rules are:\n\n\nTensors are n-dimentional metrices with indecies that can be &#39;upstairs&#39; or &#39;downstairs&#39; eg:  - is a Tensor with no indecies (a scalar),  is a Tensor with one &#39;upstairs&#39; index,  is a tensor with a bunch of &#39;upstairs&#39; and &#39;downstairs&#39; indecies\nYou can ADD tensor of the same type, meaning they have the same indecies signature. the 0th index of the first tensor is of the same type(upstairs or downstairs) as the 0th index of the second tensor and so on... \n\n  ~~~~ OK\n\n  ~~~~ NOT OK\nYou can MULTIPLY tensors and get bigger tensors, with the indecies concatenated: \n\n\nSo I want that the type-checker of Haskell wouldn&#39;t allow me to write code that doesn&#39;t follow those rules, It wouldn&#39;t compile otherwise.\n\nHere is my attempt using GADTs:\n\n\n\nBut i&#39;m getting: \n\n\n\nWhat is the problem? \n"", ""excerpt"": ""I want to make a type <span class=\""highlight\"">safe</span> implementation of <span class=\""highlight\"">Tensor</span> calculus in Haskell using GADT&#39;s, so the rules are:\n\n\nTensors are n-dimentional metrices with indecies that can be &#39;upstairs&#39; or &#39;downstairs&#39; eg:  - &hellip; is a <span class=\""highlight\"">Tensor</span> with no indecies (a scalar),  is a <span class=\""highlight\"">Tensor</span> with one &#39;upstairs&#39; index,  is a <span class=\""highlight\"">tensor</span> with a bunch of &#39;upstairs&#39; and &#39;downstairs&#39; indecies\nYou can ADD <span class=\""highlight\"">tensor</span> of the same type, meaning they have &hellip; "", ""title"": ""Haskell GADTs - making a type-safe Tensor types for Riemannian geometry""}"

TensorFlow 0.12 tutorials produce warning: &quot;Rank of input Tensor should be the same as output_rank for column,Stack Overflow,N/A,"{""tags"": [""python-3.x"", ""tensorflow""], ""question_score"": 11, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 41273182, ""item_type"": ""question"", ""score"": 11, ""last_activity_date"": 1490948099, ""creation_date"": 1482360313, ""body"": ""I have some experience with writing machine learning programs in python, but I&#39;m new to TensorFlow and am checking it out. My dev environment is a lubuntu 14.04 64-bit virtual machine. I&#39;ve created a python 3.5 conda environment from miniconda and installed TensorFlow 0.12 and its dependencies. I began trying to run some example code from TensorFlow&#39;s tutorials and encountered this warning when calling  in the boston.py example for input functions: source.\n\n\n  WARNING:tensorflow:Rank of input Tensor (1) should be the same as\n  output_rank (2) for column. Will attempt to expand dims. It is highly\n  recommended that you resize your input, as this behavior may change.\n\n\nAfter some searching in Google, I found other people encountered this same warning:\n\n\nhttps://github.com/tensorflow/tensorflow/issues/6184\nhttps://github.com/tensorflow/tensorflow/issues/5098\nTensorflow - Boston Housing Data Tutorial Errors\n\n\nHowever, they also experienced errors which prevent code execution from completing. In my case, the code executes with the above warning. Unfortunately, I couldn&#39;t find a single answer in those links regarding what caused the warning and how to fix the warning. They all focused on the error. How does one remove the warning? Or is the warning safe to ignore? \n\nCheers!\n\nExtra info, I also see the following warnings when running the aforementioned boston.py example.  \n\n\n  WARNING:tensorflow:*******************************************************\n  WARNING:tensorflow:TensorFlow&#39;s V1 checkpoint format has been\n  deprecated. WARNING:tensorflow:Consider switching to the more\n  efficient V2 format: WARNING:tensorflow:\n  &#39;tf.train.Saver(write_version=tf.train.SaverDef.V2)&#39;\n  WARNING:tensorflow:now on by default.\n  WARNING:tensorflow:*******************************************************\n\n\nand\n\n\n  WARNING:tensorflow:From\n  /home/kade/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py:1053\n  in predict.: calling BaseEstimator.predict (from\n  tensorflow.contrib.learn.python.learn.estimators.estimator) with x is\n  deprecated and will be removed after 2016-12-01. Instructions for\n  updating: Estimator is decoupled from Scikit Learn interface by moving\n  into separate class SKCompat. Arguments x, y and batch_size are only\n  available in the SKCompat class, Estimator will only accept input_fn.\n  Example conversion:   est = Estimator(...) -&gt; est =\n  SKCompat(Estimator(...))\n\n\nUPDATE (2016-12-22):\nI&#39;ve tracked the warning to this file:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column_ops.py\n\nand this code block:\n\n\n\nNote the line: \n\nThe method signature is: \n\nThe value  is hardcoded as the value of output_rank, but the boston.py example is passing in an  of rank 1. I will continue to investigate.\n"", ""excerpt"": ""Or is the warning <span class=\""highlight\"">safe</span> to ignore? \n\nCheers!\n\nExtra info, I also see the following warnings when running the aforementioned boston.py example. &hellip; , name=&#39;matmul&#39;)\n\n\nNote the line: <span class=\""highlight\"">tensor</span> = fc. &hellip; "", ""title"": ""TensorFlow 0.12 tutorials produce warning: &quot;Rank of input Tensor should be the same as output_rank for column""}"

TensorFlow: How to get pointer to data contents of ConstTensor?,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""eigen""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 42707600, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1489102720, ""creation_date"": 1489102720, ""body"": ""My team is working on developing a new backend for TensorFlow.  Generally, tensorflow opkernels are passed as arguments &quot;Tensor&quot; types which use memory allocated with our architecture:\n\n\n\nHowever, we are having more trouble with porting the &quot;CrossOp&quot; type, because part of the preconditioning involves converting the datatype to Eigen types:\n\n\n\n assumes it is run on a  and not a .  Is it safe to follow the above operations with the ones below, or is does the process of  change the contents of the underlying data such that the result would be invalid or not read by TensorFlow?\n\n\n"", ""excerpt"": ""Grab the input tensors\n  const <span class=\""highlight\"">Tensor</span>&amp; A = context-&gt;input(0);\n  const <span class=\""highlight\"">Tensor</span>&amp; B = context-&gt;input(1);\n\n  // ...input validation... &hellip; Is it <span class=\""highlight\"">safe</span> to follow the above operations with the ones below, or is does the process of flat_inner_dims() change the contents of the underlying data such that the result would be invalid or not read by &hellip; "", ""title"": ""TensorFlow: How to get pointer to data contents of ConstTensor?""}"

How to access weight variables in Keras layers in tensor form for clip_by_weight?,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""deep-learning"", ""keras""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 42530216, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1488371321, ""creation_date"": 1488366799, ""body"": ""I&#39;m implementing WGAN and need to clip weight variables.\n\nI&#39;m currently using Tensorflow with Keras as high-level API. Thus building layers with Keras to avoid manually creation and initialization of variables.\n\nThe problem is WGAN need to clip weight varibales, This can be done using  once I got those weight variable tensors, but I don&#39;t know to how to get them safely.\n\nOne possible solution maybe using  to get all trainable variables. But I don&#39;t know how to get only weight variable without bias variables.\n\nAnother solution is , but it get  arrays, although I can clip them with  APIs and set them using , but this may need CPU-GPU corporation, and may not be a good choice since clip operation needs to be performed on each train step.\n\nThe only way I know is access them directly using exact variable names which I can get from TF lower level APIs or TensorBoard, but this is may not be safe since naming rule of Keras is not guaranteed to be stable.\n\nIs there any clean way to perform  only on those s with Tensorflow and Keras?\n"", ""excerpt"": ""The only way I know is access them directly using exact variable names which I can get from TF lower level APIs or TensorBoard, but this is may not be <span class=\""highlight\"">safe</span> since naming rule of Keras is not guaranteed &hellip; "", ""title"": ""How to access weight variables in Keras layers in tensor form for clip_by_weight?""}"

Can I change Inv operation into Reciprocal in an existing graph in Tensorflow?,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""deep-learning""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 42007638, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1486117398, ""creation_date"": 1486053633, ""body"": ""I am working on an image classification problem with tensorflow. I have 2 different CNNs trained separately (in fact 3 in total but I will deal with the third later), for different tasks and on a AWS (Amazon) machine. One tells if there is text in the image and the other one tells if the image is safe for work or not. Now I want to use them in a single script on my computer, so that I can put an image as input and get the results of both networks as output.\n\nI load the two graphs in a single tensorflow Session, using the  API and the import_scope argument and putting each subgraph in a separate scope. Then I just use the  method of the created saver, giving it the common Session as argument.\n\nThen, in order to run inference, I retrieve the placeholders and final output with  and  before using it in  (I think I could just have put &#39;name&#39; in  instead of fetching the output tensor and putting it in a variable, but this is not my problem).\n\nMy problem is the text CNN works perfectly fine, but the nsfw detector always gives me the same output, no matter the input (even with ). I have tried both separately and same story: text works but not nsfw. So I don&#39;t think the problem comes from using two networks simultaneaously.\n\nI also tried on the original AWS machine I trained it on, and this time the nsfw CNN worked perfectly.\n\nBoth networks are very similar. I checked on Tensorboard if everything was fine and I think it is ok. The differences are in the number of hidden units and the fact that I use batch normalization in the nsfw model and not in the text one. Now why this title ? I observed that I had a warning when running the nsfw model that I didn&#39;t have when using only the text model:\n\n\n  W tensorflow/core/framework/op_def_util.cc:332] Op Inv is deprecated. It will cease to work in GraphDef version 17. Use Reciprocal.\n\n\nSo I thougt maybe this was the reason, everything else being equal. I checked my GraphDef version, which seems to be 11, so Inv should still work in theory. By the way the AWS machine use tensroflow version 0.10 and I use version 0.12.\n\nI noticed that the text network only had one Inv operation (via a filtering on the names of the operations given by ), and that the nsfw model had the same operation plus multiple Inv operations due to the batch normalization layers. As precised in the release notes,  has simply been renamed to , so I tried to change the names of the operations to Reciprocal with , as proposed here, but it didn&#39;t work. I have seen that using  and changing the name could also work, but from what I understand, tensorflow graphs are an append-only structure, so we can&#39;t really modify its operations (which seems to be immutable anyway).\n\nThe thing is:\n\n\nas I said, the Inv operation should still work in my GraphDef version;\nthis is only a warning;\nthe Inv operations only appear under name scopes that begin with &#39;gradients&#39; so, from my understanding, this shouldn&#39;t be used for inference;\nthe text model also have an Inv operation.\n\n\nFor these reasons, I have a big doubt on my diagnosis. So my final questions are:\n\n\ndo you have another diagnosis?\nif mine is correct, is it possible to replace Inv operations with Reciprocal operations, or do you have any other solution?\n\n"", ""excerpt"": ""One tells if there is text in the image and the other one tells if the image is <span class=\""highlight\"">safe</span> for work or not. &hellip; with graph=tf.get_default_graph() and my_var=graph.get_operation_by_name(&#39;name&#39;).outputs[0] before using it in sess.run (I think I could just have put &#39;name&#39; in sess.run instead of fetching the output <span class=\""highlight\"">tensor</span> &hellip; "", ""title"": ""Can I change Inv operation into Reciprocal in an existing graph in Tensorflow?""}"

Tensorflow--how to limit epochs with evaluation only?,Stack Overflow,N/A,"{""tags"": [""tensorflow""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 40702236, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1479631937, ""creation_date"": 1479631937, ""body"": ""Given that I train a model; save it off with metagraph/save.Saver, and the load that graph into a new script/process to test against test data, what is the best way to make sure I only iterate over the test data once?  \n\nWith my training data, I want to be able to iterate over the entire data set for an arbitrary number of iterations.  I use \n\n\n\nto drive a queue of loading files for training, so I can safely leave num_epochs as default (=None) and let other controls drive training termination.   \n\nHowever, when I run the graph for evaluation, I just want to the evaluate the test set once (and gather the appropriate statistics).  \n\nInitial attempted solution: \n\nMake a tensor for Epochs, and pass that into tf.train.string_input_producer, and then tf.Assign it to the appropriate value based on test/train.\n\nBut:\n\ntf.train.string_input_producer only takes integers as num_epochs, so this isn&#39;t possible...unless I&#39;m missing something.\n\nFurther notes: I use \n\n\n\nto read-in test/train data that has been serialized into protocol buffers (https://www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html#file-formats), so I have minimal visibility into how the data is loaded and how far along it is.\n\ntf.train.batch apparently will throw tf.errors.OutOfRangeError, but I&#39;m not clear how to catch that successfully, or if that is even what I really want to do.  I tried a very naive \n\n\n\n(like in https://www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html#creating-threads-to-prefetch-using-queuerunner-objects), which didn&#39;t catch the error from tf.train.batch.  \n"", ""excerpt"": ""Initial attempted solution: \n\nMake a <span class=\""highlight\"">tensor</span> for Epochs, and pass that into tf.train.string_input_producer, and then tf.Assign it to the appropriate value based on test/train. &hellip; "", ""title"": ""Tensorflow--how to limit epochs with evaluation only?""}"

TensorFlow python tests failing,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 40675340, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1479466658, ""creation_date"": 1479466658, ""body"": ""We are working on adding support for big-endian in TensorFlow.\nAfter making changes for &#39;kLittleEndian&#39; flag and adding support in few files, we are still facing 3 test failures(in TensorFlow v0.10.0 ), namely:\nsparse_split_op_test, sparse_matmul_op_test, string_to_hash_bucket_op_test.\n\nWhile debugging these tests by comparing the op/ret values on our system and x86(tests pass here), it was observed that the values are getting populated after   self._add_op(ret)  present in create_op function.\nhttps://github.com/tensorflow/tensorflow/blob/v0.10.0/tensorflow/python/framework/ops.py#L2241\n\nThe definition of _add_op includes with self._lock: which suggests it is thread safe.\nAny pointers on how to debug further? Regarding which thread populates output values of the Tensor?\n\nWe are executing bazel test //tensorflow/python/sparse_split_op_test and debugging using pdb.\n"", ""excerpt"": ""_lock: which suggests it is thread <span class=\""highlight\"">safe</span>.\nAny pointers on how to debug further? Regarding which thread populates output values of the <span class=\""highlight\"">Tensor</span>? &hellip; "", ""title"": ""TensorFlow python tests failing""}"

Do subsequent calls to sess.run() erase state?,Stack Overflow,N/A,"{""tags"": [""tensorflow""], ""question_score"": 1, ""is_accepted"": true, ""answer_id"": 39198355, ""is_answered"": false, ""question_id"": 39198213, ""item_type"": ""answer"", ""score"": 4, ""last_activity_date"": 1472445247, ""creation_date"": 1472443717, ""body"": ""In general, unless you assign it to a  or enqueue it in a queue, TensorFlow will discard the value of all intermediate tensors used when computing the result of  as soon as they are no longer needed, to free up memory.\n\nOne exception to this rule is that, if in your program  is a constant tensor, and  is a stateless operation (i.e. a pure function), TensorFlow may cache the result of  as part of a constant folding optimization. This optimization can make subsequent executions of  faster, at the expense of some extra work on the first run. However, if  is a a variable, or  is a stateful operation, then TensorFlow cannot safely cache the result, and it will be re-evaluated on each  call. (Also, TensorFlow currently performs constant folding anew for each distinct set of arguments to , so the value for  computed when you run  only would not be reused in the constant folding for . This optimisation could be added in the future, however.)\n"", ""excerpt"": ""One exception to this rule is that, if in your program z is a constant <span class=\""highlight\"">tensor</span>, and Node1() is a stateless operation (i.e. a pure function), TensorFlow may cache the result of Node1() as part of a constant &hellip; "", ""title"": ""Do subsequent calls to sess.run() erase state?""}"

How to do a factory of const objects,Stack Overflow,N/A,"{""tags"": [""c++"", ""c++11""], ""question_score"": 3, ""is_accepted"": true, ""answer_id"": 39169539, ""is_answered"": false, ""question_id"": 39168434, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1472224455, ""creation_date"": 1472224455, ""body"": "" cannot safely derive from .\n\nInstead it owns a tensor and exposes an implicit .\n\nThis may not work perfectly in some narrow contexts involving SFINAE template magic code, but for most cases it will work seemlessly.\n\nIt may require glue code within  to replicate the entire const API of  and forward it to the owned tensor.  Or you can force users to call a  method or  or , which enforces DRY.\n\nMyself, I&#39;d be tempted to make it a pseudo-pointer type with  and  overloads rather than duplicate the const API.\n"", ""excerpt"": ""alias_const_tensor_instance cannot safely derive from <span class=\""highlight\"">tensor</span>.\n\nInstead it owns a <span class=\""highlight\"">tensor</span> and exposes an implicit operator <span class=\""highlight\"">tensor</span> const&amp;(). &hellip; Or you can force users to call a <span class=\""highlight\"">tensor</span> const&amp; get() const method or <span class=\""highlight\"">tensor</span> const* operator-&gt;() const or <span class=\""highlight\"">tensor</span> const&amp; operator*() const, which enforces DRY. &hellip; "", ""title"": ""How to do a factory of const objects""}"

Idris non-trivial type computation for tensor indexing,Stack Overflow,N/A,"{""tags"": [""dependent-type"", ""idris""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 37402279, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1464266781, ""creation_date"": 1464047222, ""body"": ""I&#39;ve been messing around with a simple tensor library, in which I have defined the following type.\n\n\n\nThe vector parameter of the type describes the tensor&#39;s &quot;dimensions&quot; or &quot;shape&quot;. I am currently trying to define a function to safely index into a . I had planned to do this using s but I ran into an issue. Because the  is of unknown order, I could need any number of indices, each of which requiring a different upper bound. This means that a  of indices would be insufficient, because each index would have a different type. That drove me to look at using tuples (called &quot;pairs&quot; in Idris?) instead. I wrote the following function to compute the necessary type.\n\n\n\nThis function worked as I expected, calculating the appropriate index type from a dimension vector.\n\n\n\nBut when I tried to define the actual  function...\n\n\n\n...Idris raised the following error on the second case (oddly enough it seemed perfectly okay with the first).\n\n\n\nThe error seems to imply that instead of treating  as an extremely convoluted type synonym and evaluating it like I had hoped it would, it treated it as though it were defined with a  declaration; a &quot;black-box type&quot; so to speak. Where does Idris draw the line on this? Is there some way for me to rewrite  so that it works the way I want it to? If not, can you think of any other way to write the  function?\n"", ""excerpt"": ""I&#39;ve been messing around with a simple <span class=\""highlight\"">tensor</span> library, in which I have defined the following type. &hellip; data <span class=\""highlight\"">Tensor</span> : Vect n Nat -&gt; Type -&gt; Type where\n  Scalar : a -&gt; <span class=\""highlight\"">Tensor</span> [] a\n  Dimension : Vect n (<span class=\""highlight\"">Tensor</span> d a) -&gt; <span class=\""highlight\"">Tensor</span> (n :: d) a\n\n\nThe vector parameter of the type describes the tensor&#39;s &quot;dimensions&quot; &hellip; "", ""title"": ""Idris non-trivial type computation for tensor indexing""}"

python3.4 install theano in ubuntu,Stack Overflow,N/A,"{""tags"": [""python""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 36215580, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1458892040, ""creation_date"": 1458889818, ""body"": ""I install theano in ubuntu 14.04, using built-in system python 3.4.3,\ninstall pip3 for installing numpy、scipy、theano. I use nose to test above.\n\nNumpy and Scipy are no error, but theano gives the error below message.\n\n\n\nHow can I fix it ? thanks !\n"", ""excerpt"": ""in isclose\n&gt;     xfin = isfinite(x) TypeError: ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any\n&gt; supported types according to the casting rule &#39;&#39;<span class=\""highlight\"">safe</span> &hellip; in isclose\n&gt;     xfin = isfinite(x) TypeError: ufunc &#39;isfinite&#39; not supported for the input types, and the inputs could not be safely coerced to any\n&gt; supported types according to the casting rule &#39;&#39;<span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""python3.4 install theano in ubuntu""}"

How to add dynamically created input images to a RandomShuffleQueue using a QueueRunner in tensor flow,Stack Overflow,N/A,"{""tags"": [""python"", ""numpy"", ""neural-network"", ""tensorflow"", ""conv-neural-network""], ""question_score"": 2, ""is_accepted"": true, ""answer_id"": 34892643, ""is_answered"": false, ""question_id"": 34890899, ""item_type"": ""answer"", ""score"": 5, ""last_activity_date"": 1453269895, ""creation_date"": 1453269895, ""body"": ""The problem can be traced to this line, which defines the :\n\n\n\nThis creates four () ops that, each time any of them runs, will enqueue a tensor filled with 1.0 to the  queue.\nStepping through what happens in the list comprehension should make this clearer. The following happens four times:\n\n\nA  object is constructed.\nIt is passed to .\n calls  once, which increments the counter, and returns a tensor \n passes this tensor to  and returns the resulting op.\n\n\nThe  call creates one thread per enqueue op, and these run in an infinite loop (blocking when the queue reaches capacity).\n\nTo have the desired effect, you should use the feed mechanism and a placeholder to pass a different value for the grabbed screen each time you enqueue an experience. It depends on how your  class is implemented, but you probably also want to initialize a single instance of that class as well. Finally, it&#39;s not clear whether you want multiple enqueuing threads, but let&#39;s assume that  is thread-safe and permits some concurrency. Given all this, a plausible version looks like the following (note that you&#39;ll need to create a custom thread rather than a  to use feeding):\n\n\n"", ""excerpt"": ""It is passed to perceive().\nperceive() calls game.grab_screen() once, which increments the counter, and returns a <span class=\""highlight\"">tensor</span> 1 * tf.ones(self.screen_size)\npercieve() passes this <span class=\""highlight\"">tensor</span> to experience.enqueue &hellip; Finally, it&#39;s not clear whether you want multiple enqueuing threads, but let&#39;s assume that game.grab_screen() is thread-<span class=\""highlight\"">safe</span> and permits some concurrency. &hellip; "", ""title"": ""How to add dynamically created input images to a RandomShuffleQueue using a QueueRunner in tensor flow""}"

python/numpy: Using own data structure with np.allclose() ? Where to look for the requirements / what are they?,Stack Overflow,N/A,"{""tags"": [""python"", ""numpy"", ""magic-methods"", ""type-coercion""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 9801235, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1432731639, ""creation_date"": 1332319723, ""body"": ""I&#39;m implementing a Matrix Product State class, which is some kind of special tensor decomposition scheme in python/numpy for fast algorithm prototyping.\n\nI don&#39;t think that there already is such a thing out there, and I want to do it myself to get a proper understanding of the scheme.\n\nWhat I want to have is that, if I store a given tensor T in this format as T_mps, I can access the reconstructed elements by T_mps[ [i0, i1, ..., iL] ]. This is achieved by the getitem(self, key) method and works fine.\n\nNow I want to use numpy.allclose(T, mps_T) to see if my decomposition is correct.\n\nBut when I do this I get a type error for my own type:\n\n\n  TypeError: function not supported for these types, and can&#39;t coerce safely to supported types\n\n\nI looked at the documentation of allclose and there it is said, that the function works for &quot;array like&quot; objects. Now, what is this &quot;array like&quot; concept and where can I find its specification ?\n\nMaybe I&#39;m better off, implementing my own allclose method ? But that would somewhat be reinventing the wheel, wouldn&#39;t it ?\n\nAppreciate any help\nThanks in advance\n"", ""excerpt"": ""I&#39;m implementing a Matrix Product State class, which is some kind of special <span class=\""highlight\"">tensor</span> decomposition scheme in python/numpy for fast algorithm prototyping. &hellip; What I want to have is that, if I store a given <span class=\""highlight\"">tensor</span> T in this format as T_mps, I can access the reconstructed elements by T_mps[ [i0, i1, ..., iL] ]. &hellip; "", ""title"": ""python/numpy: Using own data structure with np.allclose() ? Where to look for the requirements / what are they?""}"

C++ Return temporary values and objects that cannot be copied,Stack Overflow,N/A,"{""tags"": [""c++"", ""reference"", ""return-value""], ""question_score"": 5, ""is_accepted"": false, ""answer_id"": 21567580, ""is_answered"": false, ""question_id"": 21565030, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1391568132, ""creation_date"": 1391567619, ""body"": ""As others already pointed out you missed () here:\n\n\n\nAnyway this declaration means that return value is copied. If you want to avoid copying just return reference for an object:\n\n\n\nAs I understand &#39;tensor&#39; is container of &#39;views&#39; so it manages there lifetime and its safe to return reference. For the same reason tensor::at should return reference to mutable_view:\n\n\n\nAnother question is about default constructor of &#39;view&#39; - it looks like &#39;tensor&#39; has to be a friend of &#39;view&#39; to be able to create its instances\n\nBy the way - prefer using &#39;size_t&#39; as index type instead of just &#39;int&#39;\n\nMy overall feeling of this code - you are trying to implement kind of domain language. Maybe it&#39;s better to focus on concrete calculation task?\n"", ""excerpt"": ""If you want to avoid copying just return reference for an object:\n\nconst view&amp; operator()(int ndx) const;\n\n\nAs I understand &#39;<span class=\""highlight\"">tensor</span>&#39; is container of &#39;views&#39; so it manages there lifetime and its <span class=\""highlight\"">safe</span> to &hellip; For the same reason <span class=\""highlight\"">tensor</span>::at should return reference to mutable_view:\n\nmutable_view&amp; at(int ndx);\n\n\nAnother question is about default constructor of &#39;view&#39; - it looks like &#39;<span class=\""highlight\"">tensor</span>&#39; has to be a friend &hellip; "", ""title"": ""C++ Return temporary values and objects that cannot be copied""}"

Haskell OpenGl and GLUT Ambiguous occurence,Stack Overflow,N/A,"{""tags"": [""opengl"", ""haskell"", ""glut"", ""haskell-platform""], ""question_score"": 3, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 15286187, ""item_type"": ""question"", ""score"": 3, ""last_activity_date"": 1362715834, ""creation_date"": 1362713158, ""body"": ""I am trying an OpenGL tutorial. I am running macosx with Haskell Platform installed\n\n\n\nThis is the error I am getting, the following is repeated for &quot;clear&quot;,&quot;ColorBuffer&quot; and &quot;Flush&quot;\n\n\n\nThis code was working before, since then I have installed some packages via cabal install and by homebrew.\n\nSince I think it might have something to do with packages here is &quot;ghc-pkg list&quot; (sorry for the block, just extra info that might help)\n\n\n"", ""excerpt"": ""xhtml-3000.2.1\n   zlib-0.5.4.0\n/Users/james/.ghc/x86_64-darwin-7.4.2/package.conf.d\n   Cabal-1.16.0.3\n   GLURaw-1.3.0.0\n   ObjectName-1.0.0.0\n   OpenGL-2.6.0.1\n   OpenGLRaw-1.3.0.0\n   StateVar-1.0.0.0\n   <span class=\""highlight\"">Tensor</span> &hellip; 0.7.0.3\n   haskell-lexer-1.0\n   haskell-src-exts-1.13.5\n   hostname-1.0\n   language-ecmascript-0.10\n   language-haskell-extract-0.2.4\n   optparse-applicative-0.5.2.1\n   pango-0.12.4\n   pretty-show-1.5\n   <span class=\""highlight\"">safe</span> &hellip; "", ""title"": ""Haskell OpenGl and GLUT Ambiguous occurence""}"

How to index and assign elements in a tensor using identical call signatures?,Stack Overflow,N/A,"{""tags"": [""c++"", ""arrays"", ""vector"", ""matrix"", ""variable-assignment""], ""question_score"": 4, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 3, ""is_answered"": true, ""question_id"": 13679195, ""item_type"": ""question"", ""score"": 4, ""last_activity_date"": 1355406520, ""creation_date"": 1354521415, ""body"": ""OK, I&#39;ve been googling around for too long, I&#39;m just not sure what to call this technique, so I figured it&#39;s better to just ask here on SO. Please point me in the right direction if this has an obvious name and/or solution I&#39;ve overlooked.\n\nFor the laymen: a tensor is the logical extension of the matrix, in the same way a matrix is the logical extension of the vector. A vector is a rank-1 tensor (in programming terms, a 1D array of numbers), a matrix is a rank-2 tensor (a 2D array of numbers), and a rank-N tensor is then simply an N-D array of numbers. \n\nNow, suppose I have something like this Tensor class:\n\n\n\nWith these definitions of , indexing/assign individual elements then has the same call signature: \n\n\n\nIt is fairly trivial to extend this up to arbitrary tensor rank. But what I&#39;d like to be able to implement is a more high-level way of indexing/assigning elements: \n\n\n\nI am aware that  (and many others for that matter) does a very similar thing already, but it&#39;s not my objective to just accomplish the behavior; my objective here is to learn how to elegantly, efficiently and safely add the following functionality to my  class:\n\n\n\nNote that it&#39;s my intention to use  for non-checked versions of . Alternatively, I&#39;ll stick more to the  approach of using  methods for checked versions of . Anyway, this is a design choice and besides the issue right now.\n\nI&#39;ve conjured up the following incomplete &quot;solution&quot;. This method is only really manageable for vectors/matrices (rank-1 or rank-2 tensors), and has many undesirable side-effects:\n\n\n\nBut this just feels wrong...\n\nFor each of the indexing/assignment methods outlined above, I&#39;d have to define a separate  constructor, a new , and a new  for each and every such operation. Moreover, the  would need to contain much of the same stuff that&#39;s already in the corresponding , and making everything suitable for a  of arbitrary rank makes this approach quite ugly, way too verbose and more importantly, completely unmanageable. \n\nSo, I&#39;m under the impression there is a much more effective approach to all this. \n\nAny suggestions? \n"", ""excerpt"": ""A vector is a rank-1 <span class=\""highlight\"">tensor</span> (in programming terms, a 1D array of numbers), a matrix is a rank-2 <span class=\""highlight\"">tensor</span> (a 2D array of numbers), and a rank-N <span class=\""highlight\"">tensor</span> is then simply an N-D array of numbers. &hellip; // Indexing/assigning with <span class=\""highlight\"">Tensor</span>&lt;bool&gt;\nB( B&gt;0 ) += 1.0;   \n\n// Indexing/assigning arbitrary amount of dimensions, each dimension indexed \n// with either <span class=\""highlight\"">Tensor</span>&lt;bool&gt;, size_t, <span class=\""highlight\"">Tensor</span>&lt;size_t&gt;, or Slice( &hellip; "", ""title"": ""How to index and assign elements in a tensor using identical call signatures?""}"

How to solve the following error “DistStoreError: Timed out after 61 seconds waiting for clients. 1/2 clients joined.” when train a model with 2 GPUs,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""distributed""], ""question_score"": 3, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78310490, ""item_type"": ""question"", ""score"": 3, ""last_activity_date"": 1726185820, ""creation_date"": 1712840239, ""body"": ""When I try to train on a single machine with two GPUs using the PyTorch framework, the program gets stuck at the  step. Single-step debugging shows that the program actually gets stuck at\n\nHere, if I set world_size=1, the program can run normally and successfully initialize the entire process group. However, when world_size=2, the program gets stuck and triggers the error \nI have tried the following to solve this problem:\n\nAdjusting the MASTER_PORT value.\nChanging the firewall configuration (allowing port 8090 and the port corresponding to the MASTER_PORT environment variable to be used for communication).\nDisabling the entire firewall.\nRunning the script using the python -m terminal command instead of the python terminal command.\nHowever, none of these attempts have been successful.\n\nExpected output:\n\nThe script should be able to normally return a TCPStore object.\nThe script should be able to normally train a model on a single machine with two GPUs.\n\nAdditional information:\n\nI have tried setting MASTER_PORT to different values, including 23455, 8090, and 12345.\nI have tried disabling the firewall using the sudo ufw disable command.\nI have tried running the script using the python -m torch.distributed.launch command.\n\nI am hoping to find a solution that will allow me to train on a single machine with two GPUs without encountering this error.\nWhen I was trying to reproduce this bug in Jupyter Notebook, I found that if I call the TCPStore constructor by\n\nBoth lines of code can be executed normally. But if I call the TCPStore constructor by\n\nThe timeout error would be triggered.\nHere is the minimum reproducible example (To make it easier to reproduce the problem, I set default_pg_timeout = timedelta(seconds=60) to set the wait time to 60 seconds. This variable is 300 seconds by default. If necessary, please adjust this statement directly.)\nIn my environment, the output is as follows:\noutput 1\noutput 2\nEntire output 2:\n\nCode:\n\nEnvironment:\n\n"", ""excerpt"": ""0.11.1\ninvisible_watermark==0.1.5\nbasicsr==1.4.2\neinops==0.6.0\nomegaconf==2.3.0\npytorch_lightning==1.5.9\ngradio\nopencv-python\npudb\nimageio\nimageio-ffmpeg\nk-diffusion\nwebdataset\nopen-clip-torch\nkornia\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""How to solve the following error “DistStoreError: Timed out after 61 seconds waiting for clients. 1/2 clients joined.” when train a model with 2 GPUs""}"

Consistent error message when using transformers,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""huggingface-transformers""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78954987, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1725574077, ""creation_date"": 1725574077, ""body"": ""I have been getting this error message for days on different projects and still can&#39;t understand where its from. This seems to only happen when I use tensorflow and transformers together. Can&#39;t really find anything for this specific error so help would be greatly appreciated. Am pretty new to this stuff.\n\nAm trying to use tensorflow and transformers pipelines in my multi-agent-system projects. I am very new to these libraries.\nDependencies:\n\n"", ""excerpt"": ""0.4.0\nnamex==0.0.8\nnltk==3.9.1\nnumpy==1.26.4\nopt-einsum==3.3.0\noptree==0.12.1\npackaging==24.1\npillow==10.4.0\nprotobuf==4.25.4\nPygments==2.18.0\nPyYAML==6.0.2\nregex==2024.7.24\nrequests==2.32.3\nrich==13.8.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Consistent error message when using transformers""}"

"Flux.1 Schnell image generator issue, GPU resources getting exhausted after 1 prompt",Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""diffusers""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78936279, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1725293000, ""creation_date"": 1725145495, ""body"": ""So, I tried to train a prompt based image generation model using FLUX.1-schnell. I used Lightning AI Studio (an alternate to Google Colab), that helped me to access to L40 GPU, that came with 48gb VRAM (since the minimum requirement is 24 GB.\nI trained it with a LoRA, which was setup with the help of &quot;ai-toolkit&quot; cloned from a GitHub repo &quot;click here&quot;, at 1200 steps with 35 images as training dataset. reference link\nThe first prompt worked pretty fine and generated a desired result, but when I went to generate a second image it said:\n\nCUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 71.25 MiB is free. Process 26865 has 44.45 GiB memory in use. Of the allocated memory 43.89 GiB is allocated by PyTorch, and 56.25 MiB is reserved by PyTorch but unallocated.\n\nI tried to run the code on the Jupyter Notebook, which was successful and was able to create  after training the model.\nNow, for the first prompt, I ran the cell with the following code:\n\nIt gave me a good result, but for the second image, I kept everything the same and changed the prompt and then ran the cell, but got the above error.\nHow to fix this, so that I can atleast generate three images at a time.\nAnother thing, since I have already save the output files such as:\n image,\nWhere should I make changes to not entirely download the model when once the GPU and codes are initiated.\n"", ""excerpt"": ""I tried to run the code on the Jupyter Notebook, which was successful and was able to create <span class=\""highlight\"">safetensors</span> after training the model. &hellip; "", ""title"": ""Flux.1 Schnell image generator issue, GPU resources getting exhausted after 1 prompt""}"

Matmul-free LLM ternary weights?,Stack Overflow,N/A,"{""tags"": [""large-language-model""], ""question_score"": -1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 78932079, ""item_type"": ""question"", ""score"": -1, ""last_activity_date"": 1725025009, ""creation_date"": 1725022179, ""body"": ""I&#39;ve been reading &quot;Scalable MatMul-free Language Modeling&quot;  that uses ternary weights.\nI wanted to inspect the weights, so I did the following:\n\nThe first few lines are :\n\nNext I did :\n\nThe output was :\n\nI thought that the weights were supposed to be ternary.\nCompletely confused.\n"", ""excerpt"": ""I wanted to inspect the weights, so I did the following:\nfrom <span class=\""highlight\"">safetensors</span> import safe_open\nfrom huggingface_hub import hf_hub_download\n\n# Change here to your model name\nname = &#39;ridger/MMfreeLM-370M&#39;  # &hellip; replace with your model name\n\n# Download the <span class=\""highlight\"">safetensors</span> file\nfile_path = hf_hub_download(repo_id=name, filename=&quot;model.safetensors&quot;)\n\n# Open the <span class=\""highlight\"">safetensors</span> file and inspect its contents\nwith safe_open &hellip; "", ""title"": ""Matmul-free LLM ternary weights?""}"

"Is it necessary for torch_dtype when loading a model and the precision for trainable weights to be different? If so, why?",Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""nlp"", ""huggingface-transformers"", ""large-language-model""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78870698, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1723635568, ""creation_date"": 1723635568, ""body"": ""According to this comment in the huggingface/peft package, if a model is loaded in fp16, the trainable weights must be cast to fp32. From this comment, I understand that generally, the  used when loading a model and the precision used for training must be different. Why is it necessary to change the precision? Also, does this principle apply to both fine-tuning and continual pretraining?\nAs a minimal working example, I&#39;m attempting to perform a continual pretraining on microsoft/Phi-3-mini-128k-instruct, whose default  is bfloat16. When loading the model with , training commenced when the precision for trainable weights was set to  (i.e. different precision for model loading and trainable weights). However, when the precision for trainable weights was set to  (i.e. same precision for model loading and trainable weights), an error  occurred, preventing the start of training. The execution environment was an NVIDIA RTX3060 with only 12GB of vRAM. For continual pretraining of the Phi-3 model, how should the  be set when loading the model and for trainable weights to minimize vRAM usage? For instance, should the model be loaded with  and the precision for trainable weights set to , or should the model be loaded with  and the precision for trainable weights set to ? I would like to know effective and feasible combinations.\nMWE\ntrain_deepspeed.py\n\ntrain_base.yaml\n\npyproject.toml\n\n"", ""excerpt"": ""2.7.3&quot;\npydantic-core = &quot;^2.18.4&quot;\npygments = &quot;^2.18.0&quot;\npynvml = &quot;^11.5.0&quot;\npython-dateutil = &quot;^2.9.0.post0&quot;\npytz = &quot;^2024.1&quot;\npyyaml = &quot;^6.0.1&quot;\nregex = &quot;^2024.5.15&quot;\nrequests = &quot;^2.32.3&quot;\nrich = &quot;^13.7.1&quot;\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Is it necessary for torch_dtype when loading a model and the precision for trainable weights to be different? If so, why?""}"

torch.load() with error _pickle.UnpicklingError: could not find MARK,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""pickle""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 78838204, ""is_answered"": false, ""question_id"": 77563935, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1722935611, ""creation_date"": 1722935611, ""body"": ""Not sure if that&#39;s your issue, but if you are loading a  ckpt, you should use  instead of .\n"", ""excerpt"": ""Not sure if that&#39;s your issue, but if you are loading a .<span class=\""highlight\"">safetensors</span> ckpt, you should use safetensors.torch.load_file() instead of torch.load(). &hellip; "", ""title"": ""torch.load() with error _pickle.UnpicklingError: could not find MARK""}"

Android app using Chaquopy to run Qwen2-0.5B-Instruct model,Stack Overflow,N/A,"{""tags"": [""python"", ""android"", ""huggingface-transformers"", ""large-language-model"", ""onnx""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78834466, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1722861293, ""creation_date"": 1722861293, ""body"": ""I&#39;m developing an Android app using Chaquopy to run the Qwen2-0.5B-Instruct model. I&#39;m facing persistent dependency issues, particularly with transformers and tokenizers.\nCurrent setup:\nAndroid app with Chaquopy\nPython 3.8\nTrying to use Qwen2-0.5B-Instruct model\nGradle configuration:\n\nPython code:\n\nIssues:\nDependency conflicts between transformers, tokenizers, and packaging\nLatest error: InvalidVersion: Invalid version: &#39;0.10.1,&lt;0.11&#39;\nAttempted solutions:\nTried various versions of transformers and tokenizers\nAttempted to use pre-built wheels and build from source\nExperimented with different Python package combinations\nONNX isn&#39;t an option as it doesn&#39;t support Qwen models currently.\nI&#39;m looking for suggestions on how to resolve these dependency issues or alternative approaches to run this model in an Android environment. Any insights or workarounds would be greatly appreciated.\nThanks in advance!\n"", ""excerpt"": ""0.5B-Instruct model\nGradle configuration:\ngradleCopypython {\n    version &quot;3.8&quot;\n    pip {\n        install &quot;torch==1.8.1&quot;\n        install &quot;numpy&quot;\n        install &quot;transformers==4.41.2&quot;\n        install &quot;<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Android app using Chaquopy to run Qwen2-0.5B-Instruct model""}"

What does the &quot;AttributeError: &#39;NoneType&#39; object has no attribute &#39;cget_managed_ptr&#39;&quot; mean?,Stack Overflow,N/A,"{""tags"": [""machine-learning"", ""pytorch"", ""huggingface-transformers"", ""huggingface"", ""huggingface-trainer""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78827974, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1722711866, ""creation_date"": 1722665526, ""body"": ""I&#39;m trying to train a model with very standard HF code I&#39;ve used before:\n\nbut no matter what I do e.g.,\n\nI&#39;ve forced every possible way I can to have cpu enabled to force it to train at all\nuse a HF dataset from the internet I&#39;ve used before\nupdated pytorch \nDisabled MPS\ntried making sure cpu was used\ntraining_args = TrainingArguments(\n...\nno_cuda=True,\nuse_mps_device=True if torch.backends.mps.is_available() else False,\n...\n)\n&quot; 1. Verify data types: Ensure that your model and data are using compatible data types. MPS might have issues with certain data types.&quot; but it&#39;s obvious it should work cuz the HF trainer does this on it&#39;s own by fetching the device from my model. I&#39;ve checked this code before.\nyes I did \n\nbut it doesn&#39;t work and I get a very cryptic error I&#39;ve never seen before and nothing on google shows up:\n\nwhat is going on? How do I debug this?\n\nRelated to this issue I also have this odd warning, wonder if it&#39;s related:\n\n\nMy conda env (locally, in server I&#39;m using venv):\n\n\nrefs:\n\nhttps://discuss.huggingface.co/t/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed-ptr-mean/100674\nhttps://discord.com/channels/879548962464493619/1269175711420125215\nWhat does the &quot;AttributeError: &#39;NoneType&#39; object has no attribute &#39;cget_managed_ptr&#39;&quot; mean?\n\n"", ""excerpt"": ""44.0\nregex                   2024.5.15\nrequests                2.32.3\nrequests-toolbelt       1.0.0\nrfc3986                 2.0.0\nrich                    13.7.1\ns3transfer              0.10.2\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""What does the &quot;AttributeError: &#39;NoneType&#39; object has no attribute &#39;cget_managed_ptr&#39;&quot; mean?""}"

Python error when installing Stable Diffusion,Stack Overflow,N/A,"{""tags"": [""python-3.x"", ""windows-11"", ""stable-diffusion""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78805308, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1722671160, ""creation_date"": 1722223536, ""body"": ""I was recently trying to install StableDiffusion natively on my SSD that is not my main C drive, everything was working until I tried running webui-user.bat and it said &quot;Getting requirements to build wheel did not run successfully.&quot;\n\nModuleNotFoundError: No module named &#39;setuptools.command.test&#39;\n\nThis is my output:\n\nI tried installing setuptools from here: https://github.com/pypa/setuptools/releases/tag/v72.0.0, but it did not work.\nI then tried installing setuptools v69.5.1 because that was claimed to be the version that works better with Stable Diffusion, but it gives the same error.\n"", ""excerpt"": ""1.9.4-py3-none-any.whl.metadata (22 kB)\nCollecting resize-right==0.0.2 (from -r requirements_versions.txt (line 24))\n  Using cached resize_right-0.0.2-py3-none-any.whl.metadata (551 bytes)\nCollecting <span class=\""highlight\"">safetensors</span> &hellip; ==0.4.2 (from -r requirements_versions.txt (line 25))\n  Using cached <span class=\""highlight\"">safetensors</span>-0.4.2-cp310-none-win_amd64.whl.metadata (3.9 kB)\nCollecting scikit-image==0.21.0 (from -r requirements_versions.txt (line &hellip; "", ""title"": ""Python error when installing Stable Diffusion""}"

UnknownServiceError: Unknown service: &#39;bedrock-runtime&#39;,Stack Overflow,N/A,"{""tags"": [""docker"", ""aws-lambda"", ""dockerfile"", ""boto3"", ""amazon-bedrock""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 77842380, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1722443157, ""creation_date"": 1705611715, ""body"": ""I executed this by indexing already on AOSS, and i was able to query the prompt as well. When i tried implementing this on docker, It would give an unknown service error. This error could be attributed to boto3. If it is not please let me know.\nPlease help me resolve the issues with the code here and the dockerfile below.\nThis is my code :\n\nThis code is working locally, it is giving me accurate responses, but somehow it is giving me the above mentioned error when executing the docker container.\nThis is my dockerfile:\n\nI tried using multiple boto3 version, nothing would work though.\n"", ""excerpt"": ""python:3.10\n\n# Copy requirements.txt\nCOPY requirements.txt ${LAMBDA_TASK_ROOT}\n\n# Install the specified packages\nRUN pip3 install llama-index &amp;&amp; \\\n    pip3 install opensearch-py &amp;&amp; \\\n    pip3 install <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""UnknownServiceError: Unknown service: &#39;bedrock-runtime&#39;""}"

How to reduce python Docker image size,Stack Overflow,N/A,"{""tags"": [""python"", ""docker"", ""dockerfile""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 78105348, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1722366870, ""creation_date"": 1709613529, ""body"": ""My Python based Docker image is 6.35GB. I tried multi stage build and several other options found while searching( like cache cleanup) Nothing helped.\nI might be missing something really important.\n\nrequirements.txt\n\napp folder contains multiple .py which does language model processing\n to check size of individual packages\n\n"", ""excerpt"": ""lib/python3.11/site-packages/pydantic_core\n5.3M    /app/venv/lib/python3.11/site-packages/watchfiles\n5.1M    /app/venv/lib/python3.11/site-packages/mpmath\n5.0M    /app/venv/lib/python3.11/site-packages/<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""How to reduce python Docker image size""}"

(GCP-Cloud build)Docker: COPY failed: file not found in build context (Dockerfile),Stack Overflow,N/A,"{""tags"": [""docker"", ""ubuntu"", ""google-cloud-platform""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78807626, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1722262635, ""creation_date"": 1722262635, ""body"": ""I am currently struggling with COPY instruction in Dockerfile.\n\nI have not excluded in .dockerignore\nHow can I do it?\nFile .env is stored in the same dir as Dockerfile.\ni dont no\nDockerfile\n\n"", ""excerpt"": ""Step #0: Installing collected packages: tbb, python-decouple, intel-openmp, urllib3, tzdata, typing_extensions, tqdm, threadpoolctl, sqlparse, six, <span class=\""highlight\"">safetensors</span>, regex, PyYAML, PyJWT, psycopg2-binary, pillow &hellip; huggingface-hub-0.24.3 idna-3.7 intel-openmp-2021.4.0 joblib-1.4.2 networkx-3.3 packaging-24.1 pillow-10.4.0 psycopg2-binary-2.9.9 python-dateutil-2.9.0.post0 python-decouple-3.8 regex-2024.5.15 requests-2.32.3 <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""(GCP-Cloud build)Docker: COPY failed: file not found in build context (Dockerfile)""}"

Encountering ModuleNotFoundError for mask2former after successful installation and dependency setup,Stack Overflow,N/A,"{""tags"": [""python""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78768048, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1721384374, ""creation_date"": 1721373777, ""body"": ""When I using this instructions from text to install Mask2Former.\nAnd I&#39;m pretty sure detectron2 has been installed successfully.\n\nBut I got this log, in the last.\n\nIt seems I already successful install the dependencies MultiScaleDeformableAttention==1.0 for Mask2Former.\nBut when I try improt mask2former, it still &quot;ModuleNotFoundError: No module named &#39;mask2former&#39;&quot;\nI dont know hot to deal with it.\nMy environment\n\n"", ""excerpt"": ""4.25.3\npycocotools                   2.0.8\npyparsing                     3.1.2\npython-dateutil               2.9.0.post0\nPyYAML                        6.0.1\nrequests                      2.32.3\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Encountering ModuleNotFoundError for mask2former after successful installation and dependency setup""}"

"A module that was compiled using NumPy 1.x cannot be run in NumPy 2.0.0 as it may crash, however I have NumPy 1.26.4 installed",Stack Overflow,N/A,"{""tags"": [""python"", ""conda"", ""torch"", ""miniconda""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78726800, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1720543310, ""creation_date"": 1720543310, ""body"": ""I have read all of the other posts about this, but I am already using numpy 1.26.4.  I have a conda venv and here is the error:\n\nHere is the conda listing:\n\nDoes anyone have any suggestions on how to correct this issue?  In addition, pybind is already version 2.13.1  Is there something I am missing or are there some other modules I need to upgrade/downgrade?  The listing above is all that is installed in this environment.\nThank You!\n"", ""excerpt"": ""2024.5.15       py310hc51659f_0    conda-forge\nrequests                  2.32.2          py310h06a4308_0  \ns2n                       1.4.15               he19d79f_0    conda-forge\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""A module that was compiled using NumPy 1.x cannot be run in NumPy 2.0.0 as it may crash, however I have NumPy 1.26.4 installed""}"

"Mixtral 8x7b, am I running it wrong?",Stack Overflow,N/A,"{""tags"": [""gpu"", ""artificial-intelligence"", ""vllm"", ""mixtral-8x7b""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78691343, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1720298233, ""creation_date"": 1719825214, ""body"": ""as many companies today we&#39;re experiencing running local LLM on our own internal servers. With choosed Mixtral 8x7b as it appeared to be the best value for money for a &quot;french speaking&quot; model. The model itself works pretty well but in terms of performance I have to say i&#39;m quite disapppointed : passed 10 concurrent requests the token generation speed drops significantly making the experience quite unpleasant.\nI&#39;m using vllm to run the model on a double NVIDIA H100 PCIe setup, you can see the booting logs below :\n\nHere is the result of nvidia-smi command, it looks like the model is loaded into GPU memory :\n\nIs it normal or is there a problem with my setup ? Does someone have experienced running this model in production and has feedback regarding to the hardware requirements for a given &quot;concurrent request target&quot; ?\nEDIT after Jakub&#39;s answer\nUsing the quantized model didn&#39;t help although it does run on a single GPU, the throughput is very bad even with a single request.\nMoreover adding dtype half or not with the legacy model does not seem to make any difference, I did a few tests with theses options, you can see the results below.\n1 request test\n\nThroughput is really bad, despite only one GPU is used, it is not an option for me.\n\ndtype parameter does not impact throughput.\n50 requests test\n\nHaving 50 running requests regardless of dtype parameter make the token throughput quite low, degrading user experience. I guess I expected too much for this setup.\n"", ""excerpt"": ""INFO 07-01 06:28:22 weight_utils.py:218] Using model weights format [&#39;*.<span class=\""highlight\"">safetensors</span>&#39;]\n(VllmWorkerProcess pid=3627) INFO 07-01 06:28:22 weight_utils.py:218] Using model weights format [&#39;*.<span class=\""highlight\"">safetensors</span>&#39;] &hellip; command: --model mistralai/Mixtral-8x7B-Instruct-v0.1 --host 0.0.0.0 --tensor-parallel-size 2 --load-format <span class=\""highlight\"">safetensors</span> --max-num-seqs 64 --dtype half\n\nchat    | INFO 07-02 09:23:52 metrics.py:341] Avg &hellip; "", ""title"": ""Mixtral 8x7b, am I running it wrong?""}"

"gpt-4o retrieved context difference in langchain, it changes, but stable for gpt-4 or gpt-3.5 models",Stack Overflow,N/A,"{""tags"": [""python-3.x"", ""langchain"", ""large-language-model"", ""information-retrieval""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78692295, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1719986879, ""creation_date"": 1719837641, ""body"": ""\nWhen I perform RAG in the relevant conversation chain, I have a fixed question, and when I use GPT-4 or GPT-3.5, the retrieved contexts within the prompt in debug mode contain the contexts that include the answer to my question. However, when I select the GPT-4o language model, the context containing the correct answer to the same question does not appear. In this case, the entire chain, code, and retriever remain the same; only the LLM model is changed. The context for RAG within the prompt changes when using GPT-4o.\n\n"", ""excerpt"": ""0.2.2\nrapidfuzz 2.13.7\nreferencing 0.33.0\nregex 2023.12.25\nrequests 2.31.0\nrequests-mock 1.12.1\nrequests-oauthlib 1.3.1\nresponses 0.18.0\nrich 13.7.0\nrpds-py 0.17.1\nrsa 4.9\nruff 0.1.14\ns3transfer 0.10.1\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""gpt-4o retrieved context difference in langchain, it changes, but stable for gpt-4 or gpt-3.5 models""}"

Error installing Fooocus using pip (AttributeError: cython_sources),Stack Overflow,N/A,"{""tags"": [""python"", ""windows"", ""installation"", ""pip""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78646247, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1719065882, ""creation_date"": 1718871381, ""body"": ""I&#39;m having troubles during the installation of Fooocus using pip, while processing the requirements_versions.txt\nI&#39;m on Windows 10, Python 3.12\nI got the error both in standard cmd and windows power shell, run as administrator.\nCommand:\npip install -r requirements_versions.txt\n\nThis is the content in requirements_versions.txt\n\n"", ""excerpt"": ""This is the content in requirements_versions.txt\ntorchsde==0.2.5\neinops==0.4.1\ntransformers==4.30.2\n<span class=\""highlight\"">safetensors</span>==0.3.1\naccelerate==0.21.0\npyyaml==6.0\nPillow==9.2.0\nscipy==1.9.3\ntqdm==4.64.1\npsutil==5.9.5 &hellip; "", ""title"": ""Error installing Fooocus using pip (AttributeError: cython_sources)""}"

Error RuntimeError: CUDA error: operation not supported when tried to locate something into CUDA,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""huggingface-transformers""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78638576, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1718917858, ""creation_date"": 1718727328, ""body"": ""Here is my code:\n\nWhen I run, I obtain the next error:\n\nRuntimeError: CUDA error: operation not supported CUDA kernel errors\nmight be asynchronously reported at some other API call, so the\nstacktrace below might be incorrect. For debugging consider passing\nCUDA_LAUNCH_BLOCKING=1. Compile with TORCH_USE_CUDA_DSA to enable\ndevice-side assertions.\n\nHowever, when I check if CUDA is available I obtain:\n\n\nTrue 1 0 &lt;torch.cuda.device object at 0x7f8bf6d4a9b0&gt; GRID T4-16Q\nMemory Usage: Allocated: 0.0 GB Cached: 0.0 GB\n\nI run this code on Colab, and I do not have any issues. I also run the code on another machine with another GPU, and it runs as expected.\nThe configuration of the machine where I need to run it fails.\n\nAnd the libraries:\n\nI do not know if this affect you, but the machine is a virtual machine with wmware under a vgpu. Also, I tried to run a simple nn, just for check if the problem was with the transformers library, but I obtained the same error when I tried to locate info on the GPU.\n\n\nTraceback (most recent call last): File\n“/home/admin/llm/ModelsService/test.py”, line 14, in t2 =\ntorch.randn(1,2).to(dev) RuntimeError: CUDA error: operation not\nsupported CUDA kernel errors might be asynchronously reported at some\nother API call, so the stacktrace below might be incorrect. For\ndebugging consider passing CUDA_LAUNCH_BLOCKING=1. Compile with\nTORCH_USE_CUDA_DSA to enable device-side assertions.\n\nby the way here info about my cuda\n\n(test310) admin@appdev-llm-lnx1:~/llm/ModelsService$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA\nCorporation Built on Sun_Jul_28_19:07:16_PDT_2019 Cuda compilation\ntools, release 10.1, V10.1.243\n\nregards\n"", ""excerpt"": ""nvidia-nvtx-cu12 12.1.105 packaging 24.1 pandas 2.2.2 pip 24.0 psutil 5.9.8 pyarrow 16.1.0 pyarrow-hotfix 0.6 python-dateutil 2.9.0.post0 pytz 2024.1 PyYAML 6.0.1 quanto 0.2.0 regex 2024.5.15 requests 2.32.3 <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Error RuntimeError: CUDA error: operation not supported when tried to locate something into CUDA""}"

How to run a local Open Source LLM in llama-index in a restricted environment?,Stack Overflow,N/A,"{""tags"": [""large-language-model"", ""huggingface"", ""llama-index"", ""retrieval-augmented-generation""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78471692, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1718789881, ""creation_date"": 1715596863, ""body"": ""I have a question on how to best run a local LLM (all Open Source) with llama-index for a RAG in a relatively restricted environment (absolutely no API calls, no installing from external GitRepos and also no Ollama or vLLM - which basically covers all I have experience with so far and all the examples I have come across...)\nMy approach is now to just load the quantized model with AWQ and then pass it to the query_engine, however, HuggingfaceLLM does not seem to support a locally stored model?\nMy specific question is, if I load a model like this:\n\nthen how do i proceed to further integrating this to llama-index? Do I need to write a custom LLM class?\nIs there any other option on how to achieve this? Hardware won&#39;t be a problem, my question is on the best method to include the model.\nThanks in advance!\n"", ""excerpt"": ""transformers import AutoTokenizer\n\nmodel_name_or_path = &quot;local path to folder/model&quot;\n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,trust_remote_code=False, <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""How to run a local Open Source LLM in llama-index in a restricted environment?""}"

Pythonnet not using virtual environment,Stack Overflow,N/A,"{""tags"": [""c#"", ""python.net""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78603246, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1718252867, ""creation_date"": 1718033731, ""body"": ""I&#39;m following the guide Using Python.NET with Virtual Environments and unable to get Python.NET to use a virtual environment I&#39;ve created.\nI&#39;ve built a simple  that initializes Python.net, loads a script that enumerates the installed python packages, then configure the appropriate paths and variables to enter a pre-defined virtual environment, and re-run that same script.  The output shows the same for both enumerations.\n:\n\n:\n\nOutput:\n\nThe virtual environment is present and defined:\n\nWhat changes would I need to make in order to have the second call to  use the virtual environment?\n"", ""excerpt"": ""mypy-extensions==1.0.0\nnetworkx==3.3\nnltk==3.8.1\nnumpy==1.26.4\norjson==3.10.3\npackaging==23.2\npillow==10.3.0\npip==24.0\npydantic==2.6.4\npydantic_core==2.16.3\nPyYAML==6.0.1\nregex==2024.5.15\nrequests==2.32.3\n<span class=\""highlight\"">safetensors</span> &hellip; mypy-extensions==1.0.0\nnetworkx==3.3\nnltk==3.8.1\nnumpy==1.26.4\norjson==3.10.3\npackaging==23.2\npillow==10.3.0\npip==24.0\npydantic==2.6.4\npydantic_core==2.16.3\nPyYAML==6.0.1\nregex==2024.5.15\nrequests==2.32.3\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Pythonnet not using virtual environment""}"

Issues with Generating Text from Fine-Tuned Mistral 7B Model on Georgian Dataset,Stack Overflow,N/A,"{""tags"": [""nlp"", ""huggingface"", ""language-model"", ""fine-tuning"", ""text-generation""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": false, ""question_id"": 78261781, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1718209781, ""creation_date"": 1712065589, ""body"": ""I&#39;ve fine-tuned the Mistral 7B model using a Georgian dataset with approximately 100,000 articles, including custom tokenizer fine-tuning. The fine-tuning process took about 9 hours. However, when I try to generate text, the output is not as expected; it consistently returns the input as the output, regardless of the input provided.\nHere&#39;s the code I used for fine-tuning:\n\nFor testing the fine-tuned model, I used the following code:\n\nDuring testing, the model just echoes the prompt without generating new text. Below are the logs observed:\n\nI am unsure if the issue lies in how I am loading and testing the model or if it is related to the fine-tuning process. The model should generate text based on the input prompt, but it returns the input as the output.\nHas anyone experienced similar issues, or can someone spot what might be wrong with my approach?\n"", ""excerpt"": ""it/s]\nmodel-00001-of-00002.<span class=\""highlight\"">safetensors</span>: 0%| | 0.00/9.94G [00:00&lt;?, ?B/s]\nmodel-00002-of-00002.<span class=\""highlight\"">safetensors</span>: 0%| | 0.00/4.54G [00:00&lt;?, ?B/s]\nLoading checkpoint shards: 0%| | 0/2 [00:00&lt;?, ? &hellip; "", ""title"": ""Issues with Generating Text from Fine-Tuned Mistral 7B Model on Georgian Dataset""}"

No Module Found:&quot;Requests&quot; In azure function app,Stack Overflow,N/A,"{""tags"": [""python"", ""python-requests"", ""azure-functions""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 78539068, ""is_answered"": false, ""question_id"": 78520952, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1716812732, ""creation_date"": 1716812732, ""body"": ""The error could be due to the incompatibility of module versions.\n\nAdd the packages without including the versions in .\n\n\nI have created a python Azure function and used the below packages in :\n\n\n\n\nDeploy the function using  :\n\n\n\n\nPortal:\n\n\n"", ""excerpt"": ""and used the below packages in requirements.txt:\nazure-functions\nrequests\nmpmath\nnetworkx\nnumpy\nopenai\nopenpyxl\npackaging\npdfkit\npillow\npycparser\npydantic\npydantic-core\npymongo\npyYaml\nregex\nrequests\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""No Module Found:&quot;Requests&quot; In azure function app""}"

"How to run pytorch lightning with multiple GPUs, with Apptainer and SLURM?",Stack Overflow,N/A,"{""tags"": [""pytorch"", ""slurm"", ""pytorch-lightning"", ""lightning"", ""apptainer""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78507603, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1716459604, ""creation_date"": 1716220741, ""body"": ""When using 2 GPUs on a single node, or multiple nodes on multiple nodes the training does not start while the job keeps running. I use a container (Apptainer) to deploy the environment and then submit the script to SLURM. The job starts but then stalls. I also tried .\nPyTorch version (env below)\npytorch v2.2.1\nHow to reproduce\nscript.py\n\nsbatch submission script\n\nError messages and logs\n\nThis is the output and then nothing happens any more.\nEnvironment\n\n"", ""excerpt"": ""2023.09.01\n  - readline=8.2\n  - referencing=0.35.1\n  - regex=2024.5.10\n  - requests=2.31.0\n  - rfc3339-validator=0.1.4\n  - rfc3986-validator=0.1.1\n  - rich=13.7.1\n  - rpds-py=0.18.1\n  - s2n=1.4.13\n  - <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""How to run pytorch lightning with multiple GPUs, with Apptainer and SLURM?""}"

Semaphore Leakage During MultiProcessing in the context of NLP,Stack Overflow,N/A,"{""tags"": [""python"", ""memory-leaks"", ""multiprocessing"", ""semaphore"", ""huggingface-transformers""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78510328, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1716277591, ""creation_date"": 1716277591, ""body"": ""I set up some stack traceback and multiprocessing log statements\n\nzsh: bus error  python3.10 -W &#39;default&#39; circuit_gpt2.py\n&gt; /opt/homebrew/Cellar/python@3.10/3.10.13_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224:\n&gt; UserWarning: resource_tracker: There appear to be 1 leaked semaphore\n&gt; objects to clean up at shutdown   warnings.warn(&#39;resource_tracker:\n&gt; There appear to be %d &#39;\n\nI use clean up at the end of program using the following code\n\nand use cpu on my m1 (8 core, 16GM RAM)device.\nAlso can&#39;t seem to find where it creates semlock with handle 8.\n\nHere&#39;s the full stack trace.\n\nFor more information, here is the full pip list\n\n"", ""excerpt"": ""rfc3339-validator          0.1.4\nrfc3986-validator          0.1.1\nrich                       13.7.1\nrpds-py                    0.18.1\nsae-lens                   2.1.1\nsae-vis                    0.2.18\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Semaphore Leakage During MultiProcessing in the context of NLP""}"

torch.cuda.is_available() returns &#39;False&#39;,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""anaconda"", ""gpu"", ""nvidia"", ""torch""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78503026, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1716131426, ""creation_date"": 1716131426, ""body"": ""I am working on Azure Machine Learning Studio Jupyter Notebook, I have created a conda virtual environment with python 3.8.19. My compute instance has NVIDIA Tesla T4 GPU. I have installed Pytorch with . But when I print  it returns .\n returns\n\n returns:\n\n returns:\n\nHow will I be able to activate GPU torch? Please help.\nI tried to install Torch with GPU support with following command:  but did not work.\n"", ""excerpt"": ""conda-forge\nrich                      13.7.1                   pypi_0    pypi\nrpds-py                   0.18.1                   pypi_0    pypi\nrsa                       4.9                      pypi_0    pypi\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""torch.cuda.is_available() returns &#39;False&#39;""}"

ImportError: libcudart.so.12: cannot open shared object file: No such file or directory,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""gpu"", ""large-language-model""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 3, ""is_answered"": true, ""question_id"": 77518719, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1715976455, ""creation_date"": 1700512320, ""body"": ""I am very new to LLM serving and quantization. Any leads would be much appreciated. I am trying to quantize my model using Autoawq. I have installed the following packages:\n\nI am trying run the sample code from https://github.com/casper-hansen/AutoAWQ:\n\nHowever I get the following error:\n\nThis is my nvidia config:\n\nHere is the nvcc --version output:\n\n"", ""excerpt"": ""python-dateutil    2.8.2\npytz               2023.3.post1\nPyYAML             6.0.1\nregex              2023.10.3\nrequests           2.28.1\nrootpath           0.1.1\nrouge-score        0.1.2\nsacrebleu          1.5.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""ImportError: libcudart.so.12: cannot open shared object file: No such file or directory""}"

Error installing scikit-fda in Github Codespace (fdasrsf build issue),Stack Overflow,N/A,"{""tags"": [""python"", ""scikit-learn"", ""datagrid"", ""github-codespaces""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78395786, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1715686413, ""creation_date"": 1714238054, ""body"": ""I am trying to import a library in a script on a github codespace, namely skfda.\nthe package wasn&#39;t present so I I tried to pip install, I tried conda and I tried to clone the the repo from github but the result is always the same:\n\nthe funny thing is that I write in a google colab notebook :\n! pip install scikit-fda\nthe package is installed correctly\nso the problem must be the environment.\nso I did this\n! pip freeze &gt; requirements.txt\n! cat requirements.txt\nand the result was:\n\nI am pretty new to the whole git, venv and requirement part so I don&#39;t know how to procede from here or if I&#39;m onto something at all so any suggestion would be nice.\nalso I don&#39;t know what a wheel is\n"", ""excerpt"": ""qudida==0.0.4\nratelim==0.1.6\nrdata==0.11.2\nreferencing==0.34.0\nregex==2023.12.25\nrequests==2.31.0\nrequests-oauthlib==1.4.0\nrequirements-parser==0.5.0\nrich==13.7.1\nrpds-py==0.18.0\nrpy2==3.4.2\nrsa==4.9\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Error installing scikit-fda in Github Codespace (fdasrsf build issue)""}"

OSError: meta-llama/Llama-2-7b-chat-hf is not a local folder,Stack Overflow,N/A,"{""tags"": [""python"", ""huggingface-transformers"", ""huggingface"", ""llama""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 77006745, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1715288762, ""creation_date"": 1693388079, ""body"": ""I&#39;m trying to replied the code from this Hugging Face blog. At first I installed the transformers and created a token to login to hugging face hub:\n\nAfter that it is said to use  when you have set a token. Unfortunately after running the code I get an error:\n\nError:\n\nIt says that the model cannot be found, but you can find it in the list of models on hugging face here.\nThis is the version of the  package I&#39;m using:\n\nDoes anyone know how to fix this error?\n"", ""excerpt"": ""transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /Users/quinten/opt/miniconda3/lib/python3.9/site-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""OSError: meta-llama/Llama-2-7b-chat-hf is not a local folder""}"

Create Apptainer container with environment.yml without creating a new conda environment within container,Stack Overflow,N/A,"{""tags"": [""anaconda"", ""containers"", ""apptainer""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 78373172, ""is_answered"": false, ""question_id"": 76146763, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1713882762, ""creation_date"": 1713882762, ""body"": ""Apptainer.def\n\nenvironment.yml\n\nrequirements.txt\n\n"", ""excerpt"": ""- pip:\n    - -r requirements.txt\n\nrequirements.txt\n--extra-index-url https://download.pytorch.org/whl/cu123\ntorch\ntorchvision\ntorchaudio\naccelerate\ntransformers\nscipy\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Create Apptainer container with environment.yml without creating a new conda environment within container""}"

Version requirements for Deep Learning Libraries of ArcGIS in Google Colab,Stack Overflow,N/A,"{""tags"": [""python"", ""google-colaboratory"", ""arcgis"", ""named-entity-recognition""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78355338, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1713547480, ""creation_date"": 1713547480, ""body"": ""I have been using ArcGIS&#39;s EntityRecognizer for some time now. I train the model on Google Colab as it provides faster runtime(GPU enabled).\nAs you know Google Colab now has updated its runtime to Python 3.10 from Python 3.7.\nHence the deep learning libraries that was working for 3.7 are not working for 3.10.\nFor example in Python 3.7 we were using   and  etc.\nBut now arcgis version has also updated for 3.10 to . Correspondingly it has different deep learning libraries version.\nI have tried doing\n\nin order to download individually all the libraries but there seems to be some mismatch as it is giving me the following error:\n\nHere is my libraries. As you can see fastai is already installed.\n\nI have asked the arcgis team also the same question a year back and yet to receive any response.\nhttps://github.com/Esri/arcgis-python-api/issues/1634\nMy question is\n\nHas anyone used the EntityRecognizer in the current Colab environment?\nWhat are the deep learning libraries versions you have used?\n\nAdded info: Since anyone might ask why not use Python 3.7 on local CPU to do the task - I am doing that currently but with more added data it is getting quite difficult.\nAlso I have visited the page they have mentioned in the error syntax. They also have not updated the page to provide the relevant libraries.\n"", ""excerpt"": ""0.1.4\nrfc3986-validator                0.1.1\nrich                             13.6.0\nrpds-py                          0.12.0\nrpy2                             3.4.2\nrsa                              4.9\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Version requirements for Deep Learning Libraries of ArcGIS in Google Colab""}"

LibMambaUnsatisfiableError: Encountered problems while solving: nothing provides requested bzip2 ==1.0.8 he774522_0,Stack Overflow,N/A,"{""tags"": [""python"", ""anaconda"", ""conda"", ""miniconda""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78353034, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1713522746, ""creation_date"": 1713522746, ""body"": ""I am trying to recreate a conda environment from an . The  looks like this.\n\nI tried to recreate this environment in an AWS ubuntu machine with\n\nConda can&#39;t solve the environment and gives this error message.\n\nI have tried it setting  to flexible. It doesn&#39;t work still. Is there anything else I can try?\n"", ""excerpt"": ""- pip==24.0\n      - protobuf==4.25.1\n      - pydantic==2.7.0\n      - pydantic-core==2.18.1\n      - pytz==2024.1\n      - pyyaml==6.0.1\n      - regex==2023.12.25\n      - requests==2.31.0\n      - <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""LibMambaUnsatisfiableError: Encountered problems while solving: nothing provides requested bzip2 ==1.0.8 he774522_0""}"

Timm throwing HuggingFace Hub not installed error when HFhub is installed,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""huggingface-transformers"", ""huggingface-hub""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78347092, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1713463371, ""creation_date"": 1713440018, ""body"": ""I&#39;m trying to create the convit_base.fb_in1k model via timm. When I call , I get a  error. I have huggingface_hub installed.\nThere seem to be various suggestions around the web to update other packages, all of which I have updated. I&#39;m running python 3.8 with , , , , , , . The full pip freeze list can be found here.\nI&#39;m not sure how to fix this error.\n"", ""excerpt"": ""I&#39;m running python 3.8 with huggingface-hub==0.22.2, timm==0.9.16, tokenizers==0.15.2, transformers==4.39.3, <span class=\""highlight\"">safetensors</span>==0.4.3, torch==2.2.2, torchvision==0.17.2. &hellip; "", ""title"": ""Timm throwing HuggingFace Hub not installed error when HFhub is installed""}"

How to install pip install torch==2.1.2+cu118 in linux?,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""huggingface-transformers"", ""dspy"", ""vllm""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78331842, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1713289579, ""creation_date"": 1713234970, ""body"": ""I see some people can install  e.g.,:\nhttps://github.com/stanfordnlp/dspy/discussions/818\n\nwhy can&#39;t I? I know it says I can&#39;t do it:\n\nI know it&#39;s possible. What&#39;s my issue?\nThis command also fails:\n\npython version\n\nMy gpu info:\n\nI will give this other info but I&#39;ve seen it be inconsistent in the past...so take it with a grain of salt:\n\n\nrelated:\n\nIs DSPy compatible with vllm and how good is DSPy with vllm? #818\n: https://github.com/stanfordnlp/dspy/discussions/818\nvllm not compatible with 2.2.x if that worked then this wouldn&#39;t be an issue for me https://github.com/vllm-project/vllm/issues/2747\n\n"", ""excerpt"": ""python-dateutil==2.9.0.post0\npython-dotenv==1.0.1\npytz==2024.1\nPyYAML==6.0.1\npyzmq==25.1.2\nquantile-python==1.1\nray==2.9.3\nreferencing==0.34.0\nregex==2023.12.25\nrequests==2.31.0\nrpds-py==0.18.0\nrpyc==6.0.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""How to install pip install torch==2.1.2+cu118 in linux?""}"

CUDA error: device-side assert triggered Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions. NLP - Transformers Roberta,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""huggingface-transformers"", ""bert-language-model""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78334988, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1713277830, ""creation_date"": 1713274930, ""body"": ""I met this error in loss function. An example below:\nMy code was working normally, but something got lost, I can’t find the problem. I tried to update the libraries, test previous versions, checked tensors, indices… Can someone please help me, at the end of the page contains the link to my repository with the files I used.\nLibraries version:\n\nVocabulary size:\n\nInput ID size:\n\nExample output input_ids:\n\nCode:\n\nError:\n\nGithub Repositories: https://github.com/DonatoReis/bert-new\n"", ""excerpt"": ""datasets==2.18.0\nCudatoolkit==12.1\nCuDNN==8801\n\ntransformers-cli env:\n- `transformers` version: 4.39.3\n- Platform: Windows-11-10.0.22631-SP0\n- Python version: 3.12.3\n- Huggingface_hub version: 0.22.2\n- <span class=\""highlight\"">Safetensors</span> &hellip; "", ""title"": ""CUDA error: device-side assert triggered Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions. NLP - Transformers Roberta""}"

How do I resolve this LoRA loading error?,Stack Overflow,N/A,"{""tags"": [""python"", ""huggingface-transformers"", ""huggingface""], ""question_score"": 5, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 78056541, ""item_type"": ""question"", ""score"": 5, ""last_activity_date"": 1712973356, ""creation_date"": 1708874816, ""body"": ""I&#39;m trying to run through the &#129303; LoRA tutorial. I&#39;ve gotten the dataset pulled down, trained it and have checkpoints on disk (in the form of several subdirectories and  files).\nThe last part is trying to run inference. In particular,\n\nHowever, on my local when I try to run that  line, I get\n\nI have PEFT installed, but there don&#39;t seem to be instructions calling for me to do anything else about it in order to load a LoRA.\nWhat am I doing wrong here? If the answer is &quot;nothing, this is the &#39;it&#39;s an experimental API&#39; note coming back to bite you&quot;, are there any workarounds?\n"", ""excerpt"": ""I&#39;ve gotten the dataset pulled down, trained it and have checkpoints on disk (in the form of several subdirectories and .<span class=\""highlight\"">safetensors</span> files).\nThe last part is trying to run inference. &hellip; "", ""title"": ""How do I resolve this LoRA loading error?""}"

Unable to locally save some models locally with SentenceTransformers,Stack Overflow,N/A,"{""tags"": [""python"", ""nlp"", ""huggingface-transformers"", ""large-language-model"", ""sentence-transformers""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78311910, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1712902869, ""creation_date"": 1712854218, ""body"": ""Tried many times to have SentenceTransformers and having stripped down my code to the following:\n\nalways fails with timeouts(?) while downloading the model, not always at the same exact spot. Here follows an example:\n\nSame happens with other largish models such as intfloat/e5-mistral-7b-instruct\nI have a good 200MBit fiber connection. Can I do anything else to save these models locally?\nPython 3.11.7 - sentence-transformers 2.6.1 - Mac M1 16GB with Sonoma 14.4.1 - 55GB free disk space\n"", ""excerpt"": "": 100%|██████████████████████████████████████████████████████████████████████████████████| 4.94G/4.94G [00:46&lt;00:00, 106MB/s]\nmodel-00002-of-00003.<span class=\""highlight\"">safetensors</span>: 100%|████████████████████████████████████ &hellip; ██████████████████████████████████████████████| 5.00G/5.00G [00:46&lt;00:00, 107MB/s]\nmodel-00003-of-00003.<span class=\""highlight\"">safetensors</span>: 100%|███████████████████████████████████████████████████████████████████████████████ &hellip; "", ""title"": ""Unable to locally save some models locally with SentenceTransformers""}"

TypeError: unhashable type: &#39;AddedToken&#39;,Stack Overflow,N/A,"{""tags"": [""python""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77606584, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1712898552, ""creation_date"": 1701783036, ""body"": ""I have a test.py file and I want to package this file using pyinstaller, but running the generated exe file produces the following error.\n\nNone of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won&#39;t be available and only tokenizers, configuration and file/data utilities can be used.\nTraceback (most recent call last):\nFile &quot;test.py&quot;, line 25, in \nFile &quot;transformers\\tokenization_utils_base.py&quot;, line 2024, in from_pretrained\nFile &quot;transformers\\tokenization_utils_base.py&quot;, line 2256, in _from_pretrained\nFile &quot;transformers\\models\\bert\\tokenization_bert.py&quot;, line 217, in init\nFile &quot;transformers\\tokenization_utils.py&quot;, line 367, in init\nFile &quot;transformers\\tokenization_utils.py&quot;, line 507, in _add_tokens\nFile &quot;transformers\\tokenization_utils.py&quot;, line 512, in _update_trie\nTypeError: unhashable type: &#39;AddedToken&#39;\n[3492] Failed to execute script &#39;test&#39; due to unhandled exception!\n\ntest.spec\n\nI guess there is some conflict between transformers and pyinstaller, because I run the py program directly with no problem, but I don&#39;t know how to solve it\n"", ""excerpt"": ""datas += copy_metadata(&#39;requests&#39;)\ndatas += copy_metadata(&#39;packaging&#39;)\ndatas += copy_metadata(&#39;filelock&#39;)\ndatas += copy_metadata(&#39;numpy&#39;)\ndatas += copy_metadata(&#39;huggingface-hub&#39;)\ndatas += copy_metadata(&#39;<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""TypeError: unhashable type: &#39;AddedToken&#39;""}"

Huggingface Trainer Finetuning While Using Multi-GPUs Get CUDA Loss Warnings and End up with CUDA Error. [../aten/src/ATen/native/cuda/Loss.cu:250],Stack Overflow,N/A,"{""tags"": [""linux"", ""pytorch"", ""huggingface""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78308203, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1712892277, ""creation_date"": 1712809922, ""body"": ""I&#39;m following from this script and try to adapt by setting  for using multiple GPUs in Docker container. Below is the server setting:\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=22.04\n\nSome Python packages version shows below:\n\nAll GPUs are running well while only using 1 GPU by the command:\n\nHowever, when calling all GPUs by the command:\n\nI got the CUDA Loss warnings and end up with CUDA error.\n\n\nI know the error seems caused by the number of labels is not match to the outputs, referred from this forum, but the totally same script is also running well with using multiple GPUs on the other server with the below setting:\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=20.04\n\nSome Python packages version shows below:\n\nAlso, I&#39;ve referred to this doc but stuck by the command:\n\nI&#39;m not sure if there are some CUDA installing, setting or others difference between both servers that I missed\nFor UBUNTU, CUDA, Python packages version difference, I tried to install the same environment on the first server but got the same warnings and error.\nI also tried to build the environment on host got the same warnings and error, too.\n"", ""excerpt"": ""nvidia-cusolver-cu12      11.4.5.107\nnvidia-cusparse-cu12      12.1.0.106\nnvidia-nccl-cu12          2.19.3\nnvidia-nvjitlink-cu12     12.4.127\nnvidia-nvtx-cu12          12.1.105\npeft                      0.10.0\n<span class=\""highlight\"">safetensors</span> &hellip; 11.4.5.107  \nnvidia-cusparse-cu12      12.1.0.106  \nnvidia-nccl-cu12          2.18.1      \nnvidia-nvjitlink-cu12     12.3.52     \nnvidia-nvtx-cu12          12.1.105    \npeft                      0.5.0      \n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Huggingface Trainer Finetuning While Using Multi-GPUs Get CUDA Loss Warnings and End up with CUDA Error. [../aten/src/ATen/native/cuda/Loss.cu:250]""}"

Python error InvalidHeaderDeserialization when loading model from huggingface,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""huggingface-transformers"", ""large-language-model""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78273341, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1712230465, ""creation_date"": 1712227969, ""body"": ""I fine-tuned google/gemma-7b model based on this jupyter notebook, with one minor change, that I&#39;m saving model at the end to huggingface using trainer.push_to_hub(). But when I&#39;m trying to load my model from hugging face using this code:\n\nI get error &#39;safetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization&#39;\n\nI have no problems load it from saved checkpoint using  with code\n\nbut I&#39;m not able to load it from huggingface using . Also when I tried to retrain another model without using:\n\nit loads just fine. Also I tried to &quot;reupload&quot; model to huggingface using\n\nbut it has the some outcome as previous. Is there any other option how to load this model from huggingface?\n"", ""excerpt"": ""326, in load_peft_weights\n    adapters_weights = safe_load_file(filename, device=device)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;pip_packages/lib/python3.11/site-packages/<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Python error InvalidHeaderDeserialization when loading model from huggingface""}"

TorToiSe TTS Wav2Vec2 model training every run,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""text-to-speech"", ""huggingface""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77387888, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1712108849, ""creation_date"": 1698661905, ""body"": ""I&#39;m trying to use tortoise tts lib to synthesize human speech. Here is my code:\n\nEvery run I gets this warning:\nSome weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: [&#39;wav2vec2.encoder.pos_conv_embed.conv.weight_g&#39;, &#39;wav2vec2.encoder.pos_conv_embed.conv.weight_v&#39;]\n\nThis IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\nThis IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: [&#39;wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1&#39;, &#39;wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0&#39;]\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nand this model (jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli) starts training, that took about half an hour. This model is used inside tortoise library in . How can I fix it? Can I train it model once and use than? Sorry if my question is stupid, I&#39;m backend engineer.\nHere is my spec:\nMacBook Air m1 16gb ram\nRequirements:\n\nI&#39;ve tried to search for a solve of this problem, but I expect I&#39;m the only guy with this problem\n"", ""excerpt"": ""Quart==0.19.3\nquart-cors==0.7.0\nreferencing==0.30.2\nregex==2023.10.3\nrequests==2.31.0\nrequests-oauthlib==1.3.1\nresize-right==0.0.2\nrfc3986==1.5.0\nrotary-embedding-torch==0.3.5\nrpds-py==0.10.6\nrsa==4.9\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""TorToiSe TTS Wav2Vec2 model training every run""}"

Error loading Hugging Face model: SafeTensorsInfo.__init__() got an unexpected keyword argument &#39;sharded&#39;,Stack Overflow,N/A,"{""tags"": [""python"", ""nlp"", ""huggingface-transformers"", ""huggingface"", ""llama""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 78262292, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1712080264, ""creation_date"": 1712070617, ""body"": ""I have been using Hugging Face transformers quantized Llama2 model. Suddenly, code I was able to run earlier today is throwing an error when I try to load the model.\nThis code is straight from the docs:\n\nThis suddenly generates this error:\n\nThe bizarre thing is that I was able to run this code earlier today on the same machine. It is the same model. I haven&#39;t installed or updated any new Python (or other) packages.\nThis is using Ubuntu 20.04. Here are the relevant package versions:\n\nHow can an error suddenly arise when I haven&#39;t changed anything? It might make sense of my cached version of the model was no longer available, and it had downloaded a newer version. The way that  downloads models is a bit mysterious to me, but the model files page doesn&#39;t indicate anything has changed. What can I do to return to the position I was in this morning when this code would run?\n"", ""excerpt"": ""AutoTokenizer\n\nmodel_name_or_path = &quot;TheBloke/Llama-2-7b-Chat-AWQ&quot;\n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(\n    model_name_or_path,\n    fuse_layers = True,\n    trust_remote_code = False,\n    <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Error loading Hugging Face model: SafeTensorsInfo.__init__() got an unexpected keyword argument &#39;sharded&#39;""}"

meta-llama/Llama-2-7b-hf returning tensor instead of ModelOutput,Stack Overflow,N/A,"{""tags"": [""huggingface-transformers"", ""llama""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78231239, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1712066868, ""creation_date"": 1711537984, ""body"": ""I am using the Llama-2-7b-hf model with the  function from the transformers library (v4.38.2) and it&#39;s returning the outputs as a single tensor, instead of the ModelOutput I expected.\nI have a copy of the model stored locally:\n\nThis is the code where the model is initialized and then called:\n\nI have used this code with several other models like Mistral and Occiglot and they return ModelOutput objects with these attributes,  and , but not Llama. Can anyone help me understand what is wrong?\n"", ""excerpt"": ""I have a copy of the model stored locally:\n[Llama-2-7b-hf]$ ls -1\nconfig.json\ngeneration_config.json\nLICENSE.txt\nmodel-00001-of-00002.<span class=\""highlight\"">safetensors</span>\nmodel-00002-of-00002.<span class=\""highlight\"">safetensors</span>\nmodel.safetensors.index.json &hellip; "", ""title"": ""meta-llama/Llama-2-7b-hf returning tensor instead of ModelOutput""}"

Pyarrow: ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28&#39; not found,Stack Overflow,N/A,"{""tags"": [""linux"", ""machine-learning"", ""gpu"", ""pyarrow"", ""state-space""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78248485, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1711812654, ""creation_date"": 1711812654, ""body"": ""I&#39;m trying to reproduce the results of the liquid s4 paper, but I ran into this problem of not being able to import Pyarrow. The entire error message read:\n\nIt seems like something to do with my linux system. But I have no idea where to get started.  Below are my current environment setup and information about my gpu.\n\nI&#39;m using a RTX3060 gpu.\n\nDoes anyone know what could go wrong and how I can fix them? Any help is appreciated. Please let me know if you need further information.\n"", ""excerpt"": ""=0.3.1\npydub==0.25.1\nPygments==2.17.2\npykeops==2.2.2\npyparsing==3.1.2\npython-dateutil==2.9.0.post0\npytorch-lightning==1.5.10\npytz==2024.1\nPyYAML==6.0.1\nregex==2023.12.25\nrequests==2.31.0\nrich==13.7.1\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Pyarrow: ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28&#39; not found""}"

Tortoise tts is not using GPU,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""text-to-speech""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77815428, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1711726711, ""creation_date"": 1705242666, ""body"": ""i have sucesfully installed tortoise TTs and my env looks like this:\n\ntorch.cuda.is available returns true but when i run tortoise tts it uses only 0.1 percent gpu and 5 percent cpu. this causes it to be unbearably slow even with cuda. why is it not using the gpu??\ni tried several things like reinstalling torch and forcefully changing tortoise tts device to cuda:0 but nothing made a difference.\n"", ""excerpt"": ""pypi\nregex                     2023.12.25               pypi_0    pypi\nrequests                  2.28.1                   pypi_0    pypi\nrotary-embedding-torch    0.5.3                    pypi_0    pypi\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Tortoise tts is not using GPU""}"

ValueError: 4.39.0.dev0 is not valid SemVer string,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""huggingface-transformers"", ""large-language-model"", ""huggingface""], ""question_score"": -1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78135361, ""item_type"": ""question"", ""score"": -1, ""last_activity_date"": 1710552833, ""creation_date"": 1710063146, ""body"": ""I got the following error  in my code:\n\nComplete error message:\n\nThat&#39;s the transformer version:\n\nI don&#39;t know, what the cause is and I hope you can help to find the cause.\nThanks in advance.\n"", ""excerpt"": ""Author-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""ValueError: 4.39.0.dev0 is not valid SemVer string""}"

RuntimeError: CUDA error: device-side assert triggered in Automatic1111,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""stable-diffusion""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78101854, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1709592383, ""creation_date"": 1709561560, ""body"": ""I have been using Automatic1111 and animatediff + controlnet + Adetailer for txt2img generation. I have been using it for a project for a week and nothing wrong with it. And suddenly few days ago it started to get error, it stopped generating after the Adetailer process. I found out it doesn&#39;t work because of Adetailer, it work alright when I disable it but I needed Adetailer so I have been trying to fix it. I tried updating it, doesn&#39;t work.\nI even tried formating my computer and reinstall everything. Still the same. I am a newbie so please help me out.. thank you very much. This is the error I got:\n\nformating my computer, reinstall everything\nAdd &#39;--skip-torch-cuda-test&#39; to COMMANDLINE_ARGS\n"", ""excerpt"": ""│\n    │ │     hr_upscaler │ Latent                                │                                                      │\n    │ │      checkpoint │ Dreamshaper8.<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""RuntimeError: CUDA error: device-side assert triggered in Automatic1111""}"

Exception: CUDA SETUP: Setup Failed! in Google Collab after training stable diffusion model using train_dreambooth.py,Stack Overflow,N/A,"{""tags"": [""python"", ""google-colaboratory"", ""stable-diffusion"", ""dreambooth""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78085859, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1709274031, ""creation_date"": 1709274031, ""body"": ""I&#39;m training a stable diffusion model in google collab, then I encountered an error, before that, here is my full code:\n\nAfter running this lines of codes than i met the error like this:\n\nPlease, maybe anyone know how to resolve this so the training will be success and I can load the weight of the model. Thank you in advanced\n"", ""excerpt"": ""convert_diffusers_to_original_stable_diffusion.py\n%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n%pip install -q -U --pre triton\n%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Exception: CUDA SETUP: Setup Failed! in Google Collab after training stable diffusion model using train_dreambooth.py""}"

Fooocus WebUI always wants to download a file to default folder,Stack Overflow,N/A,"{""tags"": [""json"", ""artificial-intelligence"", ""stable-diffusion""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 78081871, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1709228495, ""creation_date"": 1709213938, ""body"": ""please I have problem with Fooocus WebUI for Stable Diffusion. It always trying download file  or  into folder \nI&#39;m not sure why, because I have all in SDXL subfolder (checkpoints and loras). My directory structure looks like this:\n\nAnd  looks like this:\n\nJSON file is correct, no errors and default model which i want to use in  or  looks like this\n\nAlso  preset looking for right folder\n\nThanks for any advice wehere could be a problem\nI have solution, if I set, in  the path checkpoints to\n\nThen is it OK. But this is not correct solution because I have also SDXL_Turbo models or Lightning models or classic SD15 models.\nAny possible how to disable download it or check the paths with subfolders when I have it in --preset json?\nThanks for any advice\nMartin\n"", ""excerpt"": ""/c/AI/models/loras/SDXL/anima_pencil-XL-v1.5.0.<span class=\""highlight\"">safetensors</span>\n/c/AI/models/loras/SDXL/SDXL_FILM_PHOTOGRAPHY_STYLE_BetaV0.4.<span class=\""highlight\"">safetensors</span>\n/c/AI/models/loras/SDXL/sd_xl_offset_example-lora_1.0.<span class=\""highlight\"">safetensors</span>\n\nAnd &hellip; .<span class=\""highlight\"">safetensors</span>&quot;: &quot;https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/SDXL_FILM_PHOTOGRAPHY_STYLE_BetaV0.4.<span class=\""highlight\"">safetensors</span>&quot;\n    },\n    &quot;previous_default_models&quot;: [&quot;realisticStockPhoto_v10.<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Fooocus WebUI always wants to download a file to default folder""}"

Databricks ImportError: cannot import name &#39;override&#39; from &#39;typing_extensions&#39;,Stack Overflow,N/A,"{""tags"": [""dependencies"", ""databricks"", ""importerror"", ""llama""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 77483735, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1709048007, ""creation_date"": 1699994747, ""body"": ""I am trying to use a databricks notebook to finetune the Llama2 model. The code for this is here. I&#39;m running into an error at lines 219-231:\n\nI am getting the error\n\nImportError: cannot import name &#39;override&#39; from &#39;typing_extensions&#39; (/databricks/python/lib/python3.10/site-packages/typing_extensions.py)\n\nThe full stack trace is below.\n\nI have tried installing multiple versions of typing_extensions, both the most updated version (4.8.0), as well as (4.7.1) as was suggested in this stackoverflow post. I also tried the solution posted here, as well as installing the dependencies with &#39;%&#39; instead of &#39;!&#39;, as suggested here. None of this has worked.\nHere&#39;s a full list of my installed packages:\n\nIf anyone has any idea how to fix this, please let me know!\n"", ""excerpt"": ""requests-oauthlib            1.3.1\nresponses                    0.18.0\nrich                         13.6.0\nrope                         1.7.0\nrsa                          4.9\ns3transfer                   0.6.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Databricks ImportError: cannot import name &#39;override&#39; from &#39;typing_extensions&#39;""}"

nameError &#39;_C&#39; is not defined,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""gpu""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 78058520, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1708981770, ""creation_date"": 1708918233, ""body"": ""I&#39;m writing code in jupyter notebook. Tried a whole bunch of staff, but didn&#39;t succeeded. Here&#39;s my full setup and errors:\nSetup: Windows 10, python 3.9, venv\n\nAll the tests:\n\n\n\nProblem that lead to nameError:\n\nFrom:\n\nAnd the topic one:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNameError: name &#39;_C&#39; is not defined\nFrom:\n\nSOS!\nI tried the following:\nAdding environmental arg:\n\nCuda and torch+cuxxx don&#39;t need to bee matched, but cuxxx &lt;= CUDAxxx. I tried matched ones, nothing.\nTried all the packages that I could find in other topics:\n\nImporting torch to empty folder:\n\nAfter every action I was restarting my kernel, even venv\nAll the code was running perfectly fine using cpu.\nMain issue: I can&#39;t find direct solution neither for this:\n\nNor for this:\nNameError: name &#39;_C&#39; is not defined\nBut in .py file, where _C is imports, it throws this:\nUserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\nBecause _C isn&#39;t imported, that&#39;s the thing. If I can resolve any problem, gpu&#39;ll be set.\nP.S. Google Collab works perfectly fine. How did they achieve this? Who knows.\n"", ""excerpt"": ""referencing               0.33.0\nregex                     2023.12.25\nrequests                  2.31.0\nrfc3339-validator         0.1.4\nrfc3986-validator         0.1.1\nrpds-py                   0.18.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""nameError &#39;_C&#39; is not defined""}"

RuntimeError: Failed to import transformers.models.clip.processing_clip because of the following error,Stack Overflow,N/A,"{""tags"": [""python"", ""scikit-learn"", ""pyinstaller"", ""sentence-transformers""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 77726926, ""is_answered"": false, ""question_id"": 77528037, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1708970356, ""creation_date"": 1703766937, ""body"": ""I am using this command to solve this\n\n"", ""excerpt"": ""_utils&quot; --hidden-import=huggingface_hub --hidden-import=<span class=\""highlight\"">safetensors</span> --copy-metadata=<span class=\""highlight\"">safetensors</span> --copy-metadata huggingface_hub --hidden-import=yaml --copy-metadata=pyyaml --hidden-import=transformers &hellip; "", ""title"": ""RuntimeError: Failed to import transformers.models.clip.processing_clip because of the following error""}"

At least one of TensorFlow 2.0 or PyTorch should be installed,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""huggingface-transformers""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78041062, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1708606258, ""creation_date"": 1708606258, ""body"": ""I would like to use this Image to text model from HuggingFace. When I run the code from the example I get an error:\n\nError:\n\nThis error happens at loading the model in the . I understand that we have to use  or . So I installed it using :\n\nSo I&#39;m using tensorflow  version. I&#39;m using  on a Mac intel. I followed this question: Neither PyTorch nor TensorFlow &gt;= 2.0 have been found.Models won&#39;t be available and only tokenizers, configuration and file/data utilities can be used but installing the  with conda doesn&#39;t work. I followed the installation guide on HuggingFace but also doesn&#39;t work. I can&#39;t find a workaround for this. So I was wondering if anyone knows how to fix this issue?\n"", ""excerpt"": ""6.0.1\npyzmq                        24.0.1\nregex                        2023.12.25\nrequests                     2.31.0\nrequests-oauthlib            1.3.1\nrsa                          4.9\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""At least one of TensorFlow 2.0 or PyTorch should be installed""}"

Why do I get such a big Docker image (6.9GB)?,Stack Overflow,N/A,"{""tags"": [""python"", ""docker"", ""streamlit""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 78039672, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1708594094, ""creation_date"": 1708593369, ""body"": ""My Docker image is too big and I don&#39;t know how to get it smaller.\nI want to build a Docker image for my streamlit app with the following dockerfile:\n\nThe problem is definitely the layer installing the requirements:\n\nAnd here is my requirements.txt file\n\nMy system is a MacBook M1 and this is my Docker build command:\n\n"", ""excerpt"": ""2024.2.7\npython-magic==0.4.27\npython-multipart==0.0.9\npytz==2024.1\nPyYAML==6.0.1\nqdrant-client==1.7.2\nrapidfuzz==3.6.1\nreferencing==0.33.0\nregex==2023.12.25\nrequests==2.31.0\nrich==13.7.0\nrpds-py==0.18.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Why do I get such a big Docker image (6.9GB)?""}"

ValueError: torch.cuda.is_available() should be True but is False,Stack Overflow,N/A,"{""tags"": [""pytorch""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77809154, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1708565526, ""creation_date"": 1705091202, ""body"": ""I’m trying to train a custom model using kohya. I have been running into the same error for over 1 week now. I’ve attached photos of my error message as well as some information about my computer and the applications on it. I’m thinking I have an issue with my setup, but I’m not sure what it is or how to fix it. Any help would be much appreciated!!!\nError Message:\n\nnvcc --version:\n\npython -m torch.utils.collect_env:\n\nnvidia-smi:\nImage of NVIDIA-SMI\nValueError: torch.cuda.is_available() should be True but is False. I&#39;ve been trying to figure out how to fix this error message. I&#39;ve looked at multiple forums and tried to implement their solutions on github, pytorch, and stackoverflow with no success.\n"", ""excerpt"": ""1024, 1024&#39;, &#39;--output_dir=C:/Users/Allison/Downloads/jrm/jrm_lora\\\\model&#39;, &#39;--logging_dir=C:/Users/Allison/Downloads/jrm/jrm_lora\\\\log&#39;, &#39;--network_alpha=64&#39;, &#39;--training_comment=jrm&#39;, &#39;--save_model_as=<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""ValueError: torch.cuda.is_available() should be True but is False""}"

How to resolve conflicts with conda packages,Stack Overflow,N/A,"{""tags"": [""python"", ""docker"", ""pytorch""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 77992011, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1707889552, ""creation_date"": 1707883169, ""body"": ""I have this Dockerfile and I want to install all the Python dependencies which include opencv-python 4.7.0.68 in the requirements.txt file\n\nHowever, I am getting this error and I can&#39;t seem to resolve the issue except not installing the opencv-python 4.7.0.68\n\nAny idea?\n"", ""excerpt"": ""to resolve the issue except not installing the opencv-python 4.7.0.68\n55.09 Installing collected packages: voluptuous, tensorboard-plugin-wit, sentencepiece, library, easygui, tensorboard-data-server, <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""How to resolve conflicts with conda packages""}"

Olive to convert Whisper Model to Onnx,Stack Overflow,N/A,"{""tags"": [""python"", ""c#"", ""onnx"", ""olive""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77964944, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1707436725, ""creation_date"": 1707428948, ""body"": ""I am trying to convert OpenAi Whisper model to Onnx with Olive, to merge the Model Files into one file, using:\n\nGithub: https://github.com/microsoft/Olive\nOlive Documentation: https://microsoft.github.io/Olive/overview/quicktour.html\nI am getting error:\n\nTraceback (most recent call last):   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\runpy.py&quot;, line 196, in\n_run_module_as_main\nreturn _run_code(code, main_globals, None,   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\runpy.py&quot;, line 86, in\nrun_code\nexec(code, run_globals)   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\workflows\\run_main.py&quot;,\nline 17, in \nrun(**vars(args))   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\workflows\\run\\run.py&quot;,\nline 187, in run\nreturn engine.run(   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\engine\\engine.py&quot;,\nline 347, in run\nrun_result = self.run_accelerator(   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\engine\\engine.py&quot;,\nline 412, in run_accelerator\nreturn self.run_no_search(   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\engine\\engine.py&quot;,\nline 483, in run_no_search\nshould_prune, signal, model_ids = self._run_passes(   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\engine\\engine.py&quot;,\nline 887, in _run_passes\nmodel_config, model_id = self._run_pass(   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\engine\\engine.py&quot;,\nline 985, in _run_pass\noutput_model_config = host.run_pass(p, input_model_config, data_root, output_model_path, pass_search_point)   File\n&quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\systems\\local.py&quot;,\nline 32, in run_pass\noutput_model = the_pass.run(model, data_root, output_model_path, point)   File\n&quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\passes\\olive_pass.py&quot;,\nline 367, in run\noutput_model = self._run_for_config(model, data_root, config, output_model_path)   File\n&quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\passes\\onnx\\conversion.py&quot;,\nline 58, in _run_for_config\nreturn self._convert_model_on_device(model, data_root, config, output_model_path, &quot;cpu&quot;)   File\n&quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\passes\\onnx\\conversion.py&quot;,\nline 73, in convert_model_on_device\ncomponent_model = model.get_component(component_name)   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\model_init.py&quot;,\nline 696, in get_component\nuser_module_loader = UserModuleLoader(self.model_script, self.script_dir)   File\n&quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\common\\user_module_loader.py&quot;,\nline 22, in init\nself.user_module = import_user_module(user_script, script_dir)   File\n&quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\common\\import_lib.py&quot;,\nline 39, in import_user_module\nreturn import_module_from_file(user_script)   File &quot;C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\common\\import_lib.py&quot;,\nline 27, in import_module_from_file\nspec.loader.exec_module(new_module)   File &quot;&quot;, line 883, in exec_module   File\n&quot;&quot;, line 241, in\ncall_with_frames_removed   File &quot;C:\\OpenAI\\Olive\\examples\\whisper\\code\\user_script.py&quot;, line 10, in\n\nfrom olive.model import PyTorchModelHandler ImportError: cannot import name &#39;PyTorchModelHandler&#39; from &#39;olive.model&#39;\n(C:\\Users\\User\\anaconda3\\envs\\OliveEnv\\lib\\site-packages\\olive\\model_init.py)\n\nI have a Conda environment: OliveEnv\nPackages installed:\n\nI have tried three different versions of olive, 0.3.1, 0.3.2, 0.4.0\nA very good video, showing the use of Olive is MS Build:\nhttps://www.youtube.com/watch?v=7_0N1VL5ZmA\nhttps://www.youtube.com/watch?v=Qj-l0tGKPf8\n"", ""excerpt"": ""3.1.1\npyreadline3            3.4.1\npython-dateutil        2.8.2\npytz                   2024.1\nPyYAML                 6.0.1\nregex                  2023.12.25\nrequests               2.31.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Olive to convert Whisper Model to Onnx""}"

Medata preparing error with pip install sentence-transformers,Stack Overflow,N/A,"{""tags"": [""python"", ""metadata"", ""sentence-transformers""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77925631, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1707426888, ""creation_date"": 1706862979, ""body"": ""When I try to install sentence-transformers, i get this:\n\nMy packages:\n\nCan anyone help me?\nI have installed or updated:\n\n"", ""excerpt"": ""rapidfuzz                 3.6.1\n\nreferencing               0.33.0\n\nregex                     2023.12.25\n\nrequests                  2.31.0\n\nrequests-toolbelt         1.0.0\n\nrpds-py                   0.17.1\n\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Medata preparing error with pip install sentence-transformers""}"

when I use &quot;pip install xformers&quot; I met a problem That:,Stack Overflow,N/A,"{""tags"": [""pip""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77910754, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1706674571, ""creation_date"": 1706673898, ""body"": ""&quot;Could not build wheels for xformers, which is required to install pyproject.toml-based projects&quot;\nthe hole wrong text:\n\n\nupgrade pip , exactly it doesn&#39;t work;\ninstall xformer , I find that xformer is totally different to xformers&#128514;\npip list :\n\n\n"", ""excerpt"": ""2.8.2\npytz                          2023.3.post1\nPyYAML                        6.0.1\npyzmq                         25.1.0\nregex                         2023.10.3\nrequests                      2.31.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""when I use &quot;pip install xformers&quot; I met a problem That:""}"

Use torch-tensorrt with diffusers library,Stack Overflow,N/A,"{""tags"": [""python"", ""pytorch"", ""tensorrt"", ""diffusers""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77898803, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1706522143, ""creation_date"": 1706522143, ""body"": ""I&#39;m trying to use torch-tensorrt with the diffusers library, to enhance the inference time on my diffusion models. I tried with a tool example just to see if it works.\n\nBut when I run this code, I get the following error:\n\nThis error doesn&#39;t show if I remove the torch_tensorrt.compile. Do you know what&#39;s happening here ? Is there a way to use torch-tensorrt with diffusers ?\nHere are the full details on my setup.\nLibraries:\n\nGPU:\n\n"", ""excerpt"": ""11.7.91\npackaging                23.2\npillow                   10.2.0\npip                      23.3.2\nPyYAML                   6.0.1\nregex                    2023.12.25\nrequests                 2.31.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Use torch-tensorrt with diffusers library""}"

module &#39;numpy&#39; has no attribute &#39;_no_nep50_warning&#39;,Stack Overflow,N/A,"{""tags"": [""python"", ""embedding"", ""langchain""], ""question_score"": 3, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 77064579, ""item_type"": ""question"", ""score"": 3, ""last_activity_date"": 1706039863, ""creation_date"": 1694154072, ""body"": ""When load HuggingFaceEmbeddings, always shows error like below.\n\nI tried to downgrade numpy version to\n\nBut always same errors happen. and googles has no answer about this.\nIf anyone know how to fixing it this problem... please tell me any information,\nHeres my pip installed source code.\nnumpy versions 1.24.0 was trying to downgrade version last time.\n\nHeres my pip list below\n\n"", ""excerpt"": ""0.5.0\nrich                             13.5.2\nrpds-py                          0.10.2\nrpy2                             3.4.2\nrsa                              4.9\ns3transfer                       0.6.2\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""module &#39;numpy&#39; has no attribute &#39;_no_nep50_warning&#39;""}"

"Loading downloaded LLM to my code error, probably caused by path error",Stack Overflow,N/A,"{""tags"": [""tensorflow""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77863807, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1705979004, ""creation_date"": 1705979004, ""body"": ""I was trying to load a downloaded LLM to my code, but I went into a problem. When I try to use  to load the model:\n\nI get an error:\n\nHowever you may clearly find that the directory  is the right directory\n\nI have already search the web, and I found this post , but it didn&#39;t solve my problem. I guess it was caused by the absolute path .\nCould you tell me how to solve the problem, or where to find the solution? Thanks.\n"", ""excerpt"": ""cache_autogptq_cuda_256.cpp        model-00001-of-00008.<span class=\""highlight\"">safetensors</span>  model-00008-of-00008.<span class=\""highlight\"">safetensors</span>\n.. &hellip; configuration.json                 model-00004-of-00008.<span class=\""highlight\"">safetensors</span>  qwen.tiktoken\nLICENSE.md  configuration_qwen.py              model-00005-of-00008.<span class=\""highlight\"">safetensors</span>  qwen_generation_utils.py\nNOTICE.md &hellip; "", ""title"": ""Loading downloaded LLM to my code error, probably caused by path error""}"

UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xd5 in position 65: invalid continuation byte,Stack Overflow,N/A,"{""tags"": [""python"", ""windows"", ""pycharm"", ""anaconda"", ""decode""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77851395, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1705760657, ""creation_date"": 1705760657, ""body"": ""I&#39;m trying to read a geotiff with rasterio.read(),and this error came out.\nHere is the code in Python Console:\n\n\n\n\nThe more confusing thing is that opening a fake address, this UnicodeDecodeError still came out as same.So all I know about this is a environment problem. Does it ralate to Windows system?\nHere is my environment:\n\nThis code is a part from a paper that I try to reproduce.I am sure that environments was built as the papaer claimed.\nI tried to change different version of python(3.9/3.10) and rasterio(1.3.7/8/9), but it didn&#39;t work.\nI changed global and project encodeing in PyCharm-Settings-Editor-Code Style-File Encodings, FAILED.\nI used ,FAILED.\nIs there a solution that do not need to use Linux?\n"", ""excerpt"": ""- fiona==1.9.5\n  - fsspec==2023.12.2\n  - huggingface-hub==0.20.2\n  - mpmath==1.3.0\n  - networkx==3.2.1\n  - numpy==1.26.3\n  - pandas==2.1.4\n  - pillow==10.2.0\n  - pyyaml==6.0.1\n  - rasterio==1.3.8\n  - <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xd5 in position 65: invalid continuation byte""}"

KeyError: &#39;context&#39; with RAG chain and FewShotPromptTemplate,Stack Overflow,N/A,"{""tags"": [""python"", ""langchain"", ""large-language-model"", ""retrieval-augmented-generation""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77847789, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1705692740, ""creation_date"": 1705684679, ""body"": ""I am having trouble understanding why the chain cannot find the  passed through from  in the code below. I would like to implement a few-shot prompt so the prompt includes examples plus context from similar documents in the vector DB. Help would be really appreciated!!\n\nwhich produces:\n\nIn this case,  is the post_text. from my understanding,  expects the  and  variable names. I don&#39;t understand why the  is not being supplied to the template.\nSorry, I know it&#39;s a bit long but i wanted to include as much as i can for a workable example! Any help would be really appreciated :)\n"", ""excerpt"": ""1\n    }\npipeline_kwargs={\n        &quot;repetition_penalty&quot;:1.1\n    }\n\nfrom awq import AutoAWQForCausalLM\n\nmodel = AutoAWQForCausalLM.from_quantized(model_path, fuse_layer=True,trust_remote_code = False, <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""KeyError: &#39;context&#39; with RAG chain and FewShotPromptTemplate""}"

Loading shrads of llama 2 when inferencing it by huggingface takes too long,Stack Overflow,N/A,"{""tags"": [""python"", ""huggingface-transformers"", ""huggingface"", ""llama""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77831837, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1705488581, ""creation_date"": 1705488581, ""body"": ""It happens when I trying to inference llama2. The following is my code:\n\nAnd there is my environment:\n\nGPU:\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 3090        On  | 00000000:01:00.0 Off |                  N/A |\n| 44%   52C    P8              23W / 350W |     18MiB / 24576MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  NVIDIA GeForce RTX 3080        On  | 00000000:04:00.0 Off |                  N/A |\n|  0%   42C    P8              22W / 320W |    299MiB / 10240MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\nPython version:3.11.7\nThere is no other problems but the low speed. It takes a hour to load the check point shards into gpu.\nI don&#39;t why this happens and how to fix it. Just provide some thoughts and discuss about it!\n"", ""excerpt"": ""23.3.1\npsutil             5.9.7\npycparser          2.21\npyOpenSSL          23.2.0\nPySocks            1.7.1\nPyYAML             6.0.1\nregex              2023.12.25\nrequests           2.31.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Loading shrads of llama 2 when inferencing it by huggingface takes too long""}"

How to see sizes of installed pip packages?,Stack Overflow,N/A,"{""tags"": [""python"", ""linux"", ""debian"", ""pip""], ""question_score"": 84, ""is_accepted"": false, ""answer_id"": 77764029, ""is_answered"": false, ""question_id"": 34266159, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1704451532, ""creation_date"": 1704451532, ""body"": ""Building on @Tirtha and @AnsonH answers, here is my version:\nIt features:\n\nline showing the total space,\na line showing the space taken by all the small libraries,\na table-like formatting to display everything in decreasing order.\n\n\nRunning the script in python outputs:\n\n"", ""excerpt"": ""=============================\nkaleido 0.2.1                   253.34 MiB\ntorch 1.13.0                    232.95 MiB\nscipy 1.8.1                     93.77 MiB\npyarrow 10.0.0                  81.60 MiB\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""How to see sizes of installed pip packages?""}"

Is there a faster way to transfer data from local machine to AWS EC2?,Stack Overflow,N/A,"{""tags"": [""amazon-web-services"", ""amazon-ec2"", ""scp""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77731178, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1703860858, ""creation_date"": 1703842241, ""body"": ""I am trying to set up my very first EC2 instance for work. This work involved transferring a few large files to EC2 (around 5~10GB) every now and then. The problem is, the scp approach in the AWS document only gives me around 200~600KB/s (averaging~400), much lower than what I expected and will take me 5 hours just to upload one file.\n\nFollowing similar questions here in Stackoverflow, I also tried using rsync instead with not much luck. In fact, I think it even gives a somewhat lower average speed.\n\nIs this speed range to be expected ? Testing by basic google tool, I should have upload speed of 10Mbps, so around 1.2MB. If possible, I would love to get something over a MB. My machine is MacOS and EC2 is ubuntu. I am not the owner of this AWS account and someone else provides this EC2 instance to me, so I cannot set up an S3 and send data to it instead either.\nSorry for such a novice question and thanks you all in advance. I would really appreciate any suggestions for configurations or other approaches for faster upload speed.\n(For further information) I know almost next to nothing when it comes to networking so I have no idea how to check for bottleneck in the connection. If you need any further information please write the script to check it in the comment and I will update the question with the test results.\n"", ""excerpt"": ""In fact, I think it even gives a somewhat lower average speed.\nrsync -avz --progress -e &quot;ssh -i ${EC2_KEY}&quot; *.<span class=\""highlight\"">safetensors</span> &quot;${EC2_DNS}:/home/ubuntu/&quot;\n\nIs this speed range to be expected ? &hellip; "", ""title"": ""Is there a faster way to transfer data from local machine to AWS EC2?""}"

docker build error when building miniconda env on macbook pro m1 arm64,Stack Overflow,N/A,"{""tags"": [""docker"", ""arm64"", ""miniconda""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77660567, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1702561040, ""creation_date"": 1702561040, ""body"": ""im trying to dockerize a ML API service and i have my dependencies in a environment.yml file.\nthe docker file i have written is the following:\n\nand here is my environement.yml file:\n\nwhen i try a build with the   command, i have the following error occuring:\n\nto be sure the yaml file is actually working on my local machine (macbook pro m1 arm64), i tried to build the env again, which worked successfully. I&#39;m probably missing something, can you help me  please?\n"", ""excerpt"": ""0\n  - readline=8.2=h1a28f6b_0\n  - regex=2023.10.3=py311h80987f9_0\n  - requests=2.31.0=py311hca03da5_0\n  - requests-oauthlib=1.3.0=py_0\n  - responses=0.13.3=pyhd3eb1b0_0\n  - rsa=4.7.2=pyhd3eb1b0_1\n  - <span class=\""highlight\"">safetensors</span> &hellip; - pyyaml==6.0.1=py311h80987f9_0\n12.45   - re2==2022.04.01=hc377ac9_0\n12.45   - readline==8.2=h1a28f6b_0\n12.45   - regex==2023.10.3=py311h80987f9_0\n12.45   - requests==2.31.0=py311hca03da5_0\n12.45   - <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""docker build error when building miniconda env on macbook pro m1 arm64""}"

InvalidHeaderDeserialization Error After Finetune Training of LLM has Completed with Hugging Face Library,Stack Overflow,N/A,"{""tags"": [""python"", ""huggingface""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77602220, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1701719571, ""creation_date"": 1701719571, ""body"": ""I am trying to fine tune flan-t5-large to classify financial tweets. I&#39;m running into an error message which occurs immediately after the model completes training. I&#39;m unable to find existing posts in the context of the Hugging Face library. Given this, I&#39;m having a very hard time debugging and resolving this error message, and am hoping that someone more familiar with the Hugging Face library could help.\n\n\n\n"", ""excerpt"": ""adapters_weights = safe_load_file(filename, device=device)\n    271 else:\n    272     adapters_weights = torch.load(filename, map_location=torch.device(device))\n\nFile ~/Projects/llmtest1/lib/python3.10/site-packages/<span class=\""highlight\"">safetensors</span> &hellip; /torch.py:308, in load_file(filename, device)\n    285 &quot;&quot;&quot;\n    286 Loads a <span class=\""highlight\"">safetensors</span> file into torch format.\n    287 \n   (...)\n    305 ```\n    306 &quot;&quot;&quot;\n    307 result = {}\n--&gt; 308 with safe_open(filename &hellip; "", ""title"": ""InvalidHeaderDeserialization Error After Finetune Training of LLM has Completed with Hugging Face Library""}"

/usr/local/lib/python3.11/site-packages/ctransformers/lib/basic/libctransformers.so: cannot open shared object file: No such file or directory,Stack Overflow,N/A,"{""tags"": [""python"", ""docker"", ""large-language-model"", ""ctransformers""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77588996, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1701485601, ""creation_date"": 1701485306, ""body"": ""I&#39;m facing trouble dockerizing an LLM based application that I&#39;m working on. I have a python/chainlit app that runs fine when I start it through the terminal, however, it seems to fail when I try to run it through a docker container.\nMeta information:\nPython Version: 3.11.4\nDocker version: 24.0.6, build ed223bc\nOperating System: MacOS 13.6.2 (22G320)\nChip: Apple M1 Pro\nLocally, I run things as follows:\nCreate python virtual environment\n\n\nRun a pip install of all necessary libraries\n\nStart application\n\nThe requirements.txt file has the following:\n\nWith Docker, I run the following commands:\nBuild Docker image\n\nThe Docker File is as follows:\n\nStart Docker container\n\nThe image builds successfully, and the application even starts, but I get the following error log:\n\nI suspect it has something to do with the ctransformers library. Additionally, I ssh&#39;ed into the container and found that the file libctransformers.so is in fact present and not missing.\n"", ""excerpt"": ""python-socketio==5.10.0\nPyYAML==6.0.1\npyzmq==25.1.1\nqtconsole==5.5.1\nQtPy==2.4.1\nreferencing==0.31.1\nregex==2023.10.3\nrequests==2.31\nrfc3339-validator==0.1.4\nrfc3986-validator==0.1.1\nrpds-py==0.13.2\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""/usr/local/lib/python3.11/site-packages/ctransformers/lib/basic/libctransformers.so: cannot open shared object file: No such file or directory""}"

CondaValueError: could not parse &#39;abseil-cpp 20211102.0 h6b3803e_1 conda-forge&#39; in: requirements.txt,Stack Overflow,N/A,"{""tags"": [""github"", ""jupyter-notebook"", ""conda"", ""requirements.txt"", ""conda-forge""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77567323, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1701209127, ""creation_date"": 1701207864, ""body"": ""I&#39;m trying to run a github repository for the first time, but it has a lot of interdependent files (.py, .ipynb, requirements.txt and .json).\nRepo link\nSo firstly I&#39;ve tried to clone the repository on a jupyter notebook after having created an environment on Anaconda.Navigator and then have done:\n\nAfter this, firstly I&#39;ve sought to download all requirements by doing the following (in the modeling_and_return_estimation.ipynb notebook which I&#39;ve got in the enviroment after cloning) by doing the following:\n\nIt gave me this error:\n\n\nHow do i fix this issue?\n"", ""excerpt"": ""0.1.1              pyh9f0ad1d_0    conda-forge\nrpds-py                   0.8.10          py311h0563b04_0    conda-forge\nsacremoses                0.0.53             pyhd8ed1ab_0    conda-forge\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""CondaValueError: could not parse &#39;abseil-cpp 20211102.0 h6b3803e_1 conda-forge&#39; in: requirements.txt""}"

Segmentation Fault when Using SentenceTransformer Inside Docker Container,Stack Overflow,N/A,"{""tags"": [""python"", ""docker"", ""apple-m1"", ""sentence-transformers""], ""question_score"": 9, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 3, ""is_answered"": true, ""question_id"": 77290003, ""item_type"": ""question"", ""score"": 9, ""last_activity_date"": 1700577383, ""creation_date"": 1697222192, ""body"": ""Edit: After test on different machines it is Apple M1 M2 specific bug.\nI am trying to run a Flask application inside a Docker container on Apple Silicon M2 (could be an issue), where I use the SentenceTransformer model to encode sentences. However, when I call the encode method on the model, the application crashes with a segmentation fault.\nHere&#39;s the relevant code:\n\nSource: https://www.sbert.net/docs/quickstart.html\nafter using  and \nThe error traceback is:\n\nSome points:\nThe Docker container has ample memory allocated.\nI&#39;ve tried updating the libraries (torch, transformers, and sentence-transformers).\nThe same code works perfectly outside the Docker environment.\nHow can I resolve this segmentation fault when running the code inside Docker?\nHere is a pip list of actual versions.\n\nFor more clarification, here is the most important part of my Dockerfile.\n\n"", ""excerpt"": ""numpy                 1.26.0\npackaging             23.2\nPillow                10.0.1\npip                   23.2.1\nPyYAML                6.0.1\nregex                 2023.10.3\nrequests              2.31.0\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Segmentation Fault when Using SentenceTransformer Inside Docker Container""}"

Error when training LoRa Model Using kohya_ss,Stack Overflow,N/A,"{""tags"": [""python"", ""stable-diffusion""], ""question_score"": -3, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77506481, ""item_type"": ""question"", ""score"": -3, ""last_activity_date"": 1700567175, ""creation_date"": 1700302997, ""body"": ""I get the following output, when I try to train a LoRa Modell using kohya_ss:\n\nI tried reinstalling cuda, I have added it to PATH, I have reinstalled the package bitsandbytes.\n"", ""excerpt"": ""Modell kram/Nele/image&#39;, &#39;--resolution=512,512&#39;, &#39;--output_dir=E:/Homeworklol/Deepfakes/LoRa Modell kram/Nele/model&#39;, &#39;--logging_dir=E:/Homeworklol/Deepfakes/LoRa Modell kram/Nele/log&#39;, &#39;--save_model_as=<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Error when training LoRa Model Using kohya_ss""}"

Offline Execution of StableDiffusionPipeline from_single_file in Python,Stack Overflow,N/A,"{""tags"": [""python-3.x"", ""offline"", ""stable-diffusion""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 77423047, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1700320239, ""creation_date"": 1699117748, ""body"": ""I’m working with a Python script that uses the  from the diffusers module. Here’s the relevant part of my code:\n\nThe line  only works when I have an active internet connection. If I’m offline, I encounter the following error:\n\nGiven that my  model file is already stored locally on my computer, I would like my code to function even when I’m offline.\nHow can I achieve this?\nAny help would be appreciated. Thanks!\n"", ""excerpt"": ""Here’s the relevant part of my code:\nfrom diffusers import StableDiffusionPipeline\nMODEL_PATH = &#39;/home/mypath/Stable-diffusion/limitlessvision_v20.<span class=\""highlight\"">safetensors</span>&#39;\npipeline = StableDiffusionPipeline.from_single_file &hellip; (Caused by NewConnectionError(&#39;&lt;urllib3.connection.HTTPSConnection object at 0x7f27f1dc4b50&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;))\n\nGiven that my .<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Offline Execution of StableDiffusionPipeline from_single_file in Python""}"

Why is Stable Diffusion + Roop API Call throws httpException?,Stack Overflow,N/A,"{""tags"": [""python"", ""json"", ""rest"", ""stable-diffusion""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 2, ""is_answered"": true, ""question_id"": 76884450, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1700266752, ""creation_date"": 1691764001, ""body"": ""I tried to build an API call to a Runpod instance of Stable Diffusion with Roop Extension. The Webui working correctly, now i build a serverless endpoint based on this worker. There i can inference correctly without roop. Now i try to build a call with roop arguments like in this example from roop&#39;s git site.\nHere is my relevant code:\n\nAfter that i get a response:\n\nWhy is the minimal example failing?\n"", ""excerpt"": ""&quot;api_name&quot;: &quot;txt2img&quot;,\n    &quot;prompt&quot;: prompt,\n    &quot;restore_faces&quot;: True,\n    &quot;negative_prompt&quot;: negPrompt,\n    &quot;seed&quot;: -1,\n    &quot;override_settings&quot;: {\n        &quot;sd_model_checkpoint&quot;: &quot;dreamshaper_7.<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Why is Stable Diffusion + Roop API Call throws httpException?""}"

Getting an error &#39;no file named tf_model.h5 or pytorch_model.bin found in directory gpt2&#39;,Stack Overflow,N/A,"{""tags"": [""tensorflow"", ""text"", ""pytorch"", ""classification"", ""gpt-2""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 77440561, ""is_answered"": false, ""question_id"": 75662453, ""item_type"": ""answer"", ""score"": 2, ""last_activity_date"": 1699380915, ""creation_date"": 1699380915, ""body"": ""If your model directory doesn&#39;t have  or  files as the error suggests, but does have a  file or similar, first confirm you have  installed:\n\nOtherwise install it:\n\nIf errors persist, try:\n\nThings should work after resolving any dependency issues and restarting your kernel to reload modules. For reference, I was able to load a fine-tuned  and its corresponding  file with the following:\n\n\n"", ""excerpt"": ""importlib.util\nprint(importlib.util.find_spec(&quot;<span class=\""highlight\"">safetensors</span>&quot;) is not None)\n\nOtherwise install it:\n! &hellip; pip install <span class=\""highlight\"">safetensors</span>==0.4.0 \n!pip install tokenizers==0.14.1 \n! &hellip; "", ""title"": ""Getting an error &#39;no file named tf_model.h5 or pytorch_model.bin found in directory gpt2&#39;""}"

RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable while GPU utilization is 0% according to nvidia-smi,Stack Overflow,N/A,"{""tags"": [""python"", ""gpu"", ""nvidia"", ""large-language-model"", ""gradio""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 77435116, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1699369339, ""creation_date"": 1699321591, ""body"": ""I&#39;m trying to launch a gradio backend that uses the LLM calls from Facebook. But it tells me that GPUs are not available:\n\nIt does not appear that the GPU is being over-utilized.\n\nSide note, I&#39;m surprised that python3 takes up so much memory.\nI have the appropriate driver\n\nand I can load the model:\n\nI don&#39;t know if it&#39;s related, but I have to say that installing LLamaV2 was no picnic:\n\nUpdate\nWithout any reason but reboots I now have two different errors, after rebooting, sometime I get:\n\nLike if the driver wasn&#39;t activated at all.\nAnd some other time I have:\n\n"", ""excerpt"": ""(10 GB)\nUsername for &#39;https://huggingface.co&#39;: fatal: could not read Username for &#39;https://huggingface.co&#39;: Success\nError downloading object: model-00001-of-00002.<span class=\""highlight\"">safetensors</span> (66dec18): Smudge error: &hellip; Use `git lfs logs last` to view the log.\nerror: external filter &#39;git-lfs filter-process&#39; failed\nfatal: model-00001-of-00002.<span class=\""highlight\"">safetensors</span>: smudge filter lfs failed\n\nUpdate\nWithout any reason but reboots &hellip; "", ""title"": ""RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable while GPU utilization is 0% according to nvidia-smi""}"

When copying file from HugginFace to GoogleColab with wget I only get a small size,Stack Overflow,N/A,"{""tags"": [""google-colaboratory"", ""huggingface-hub""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 77411193, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1699107659, ""creation_date"": 1698941256, ""body"": ""I have identified my account in Google Colab by providing a token\n\nthen I would like to copy this 6.5 Go file :\n\nUnfortunately it gets only 53K\n\nIs there a way to overcome this limitation ?\n"", ""excerpt"": ""wget https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.<span class=\""highlight\"">safetensors</span>\n\nUnfortunately it gets only 53K\n\nIs there a way to overcome this limitation ? &hellip; "", ""title"": ""When copying file from HugginFace to GoogleColab with wget I only get a small size""}"

Can you help me with this? typing-extensions 4.6.0 - 4.8.0,Stack Overflow,N/A,"{""tags"": [""google-colaboratory""], ""question_score"": 0, ""is_accepted"": false, ""answer_id"": 77380335, ""is_answered"": false, ""question_id"": 77329717, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1698511160, ""creation_date"": 1698511160, ""body"": ""I solved changing\n\nto\n\nI was using a Colab notebook\n"", ""excerpt"": ""I solved changing\n%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort <span class=\""highlight\"">safetensors</span> xformers\n\nto\n%pip install -q typing-extensions==4.5.0 kaleido accelerate transformers ftfy &hellip; bitsandbytes==0.35.0 gradio natsort <span class=\""highlight\"">safetensors</span> xformers\n\nI was using a Colab notebook &hellip; "", ""title"": ""Can you help me with this? typing-extensions 4.6.0 - 4.8.0""}"

Conda SSL error,Stack Overflow,N/A,"{""tags"": [""python"", ""proxy"", ""anaconda"", ""conda""], ""question_score"": 15, ""is_accepted"": false, ""answer_id"": 77326557, ""is_answered"": false, ""question_id"": 31729076, ""item_type"": ""answer"", ""score"": 0, ""last_activity_date"": 1697743469, ""creation_date"": 1697743469, ""body"": ""If you have a VPN running, like Zscaler, and you can turn it off, that will solve this problem. You can turn back on VPN after the installation succeeds.\nI was having issue running update command:\n\nTurning off Zscaler solved it:\n\n"", ""excerpt"": ""pyodbc-4.0.39              |  py311h313beb8_0          69 KB\n    queuelib-1.6.2             |  py311hca03da5_0          36 KB\n    regex-2023.10.3            |  py311h80987f9_0         394 KB\n    <span class=\""highlight\"">safetensors</span> &hellip; 4.0.39-py311h313beb8_0 \n  queuelib                            1.5.0-py311hca03da5_0 --&gt; 1.6.2-py311hca03da5_0 \n  regex                            2022.7.9-py311h80987f9_0 --&gt; 2023.10.3-py311h80987f9_0 \n  <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Conda SSL error""}"

Import of transformers package throwing value_error,Stack Overflow,N/A,"{""tags"": [""python"", ""anaconda"", ""jupyter"", ""huggingface-transformers""], ""question_score"": 2, ""is_accepted"": false, ""answer_id"": 77295022, ""is_answered"": false, ""question_id"": 68997701, ""item_type"": ""answer"", ""score"": 1, ""last_activity_date"": 1697332450, ""creation_date"": 1697332450, ""body"": ""I ran into the same problem.\nHere is the version check script I created\n\nI found two issue\n\n with its latest version wasnt working, I lowered it to 21.0\n wasn&#39;t installed, installed it manually \n\nAfter that, everything worked well.\n"", ""excerpt"": ""version(&#39;regex&#39;))\nprint(version(&#39;requests&#39;))\nprint(version(&#39;packaging&#39;))\nprint(version(&#39;filelock&#39;))\nprint(version(&#39;numpy&#39;))\nprint(version(&#39;tokenizers&#39;))\nprint(version(&#39;huggingface-hub&#39;))\nprint(version(&#39;<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Import of transformers package throwing value_error""}"

Cannot install Anaconda on my computer: Failed to Extract Packages Error,Stack Overflow,N/A,"{""tags"": [""installation"", ""module"", ""package"", ""anaconda""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": true, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 77245371, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1696733178, ""creation_date"": 1696603338, ""body"": ""I am trying to install Anaconda on Win7. I have done this multiple times with no issue. The pc is a clone of others, lots of space, Win7, no issues, etc.\nI am running 202309-64 bit setup.exe downloaded from Anaconda’s website.\nIt almost finishes installing, then I receive the error message “Failed to load packages.” I have tried clicking retry to no avail, abort cancels installation, and Ignore takes me to the next error message &quot;Failed to link extracted packages to C:\\Users\\ME\\anaconda3. Please check logs.\nI have re-downloaded the installer, I have deleted all instances and files of anaconda and .condarc on my PC, and followed other troubleshooting steps including restarting my pc, checking disk space, removing all files of anything “conda”, installing as administrator with privileges to a different (Default) user account, etc. Always the same error occurs when I launch the installer.\nBelow is the log file I have. Please help!! Why won’t it install? How can their own installer not work properly, I am not doing anything odd, it should have the .dll file it is looking for no?\nskipping to the bottom of the file…\n\n"", ""excerpt"": ""rfc3986-validator-0.1.1-py311haa95532_0.conda\nExtract: rtree-1.0.1-py311h2eaa2aa_0.conda\nExtract: ruamel.yaml-0.17.21-py311h2bbff1b_0.conda\nExtract: ruamel_yaml-0.17.21-py311h2bbff1b_0.conda\nExtract: <span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Cannot install Anaconda on my computer: Failed to Extract Packages Error""}"

SageMaker complains that /opt/ml/model does not appear to have a file named config.json,Stack Overflow,N/A,"{""tags"": [""python"", ""huggingface-transformers"", ""amazon-sagemaker"", ""huggingface""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 76933625, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1693719125, ""creation_date"": 1692420470, ""body"": ""I am using a huggingface model alongside a custom pipeline to deploy my model onto SageMaker, my model.tar.gz structure looks like below:\n\nI deployed my model via\n\nHowever, when I try to invoke the model, here is my response\n\nAnother error is \nBut config.json is clearly in my model directory. Here is my inference.py code\n\nWhat am I doing wrong here? I should have followed all needed steps to deploy a huggingface model onto SageMaker.\n"", ""excerpt"": ""model.tar.gz structure looks like below:\n├── added_tokens.json\n├── code\n│   ├── inference.py\n│   ├── pipeline.py\n│   └── requirements.txt\n├── config.json\n├── generation_config.json\n├── model-00001-of-00002.<span class=\""highlight\"">safetensors</span> &hellip; ├── model-00002-of-00002.<span class=\""highlight\"">safetensors</span>\n├── model.safetensors.index.json\n├── special_tokens_map.json\n├── tokenizer_config.json\n├── tokenizer.json\n└── tokenizer.model\n\nI deployed my model via\nfrom sagemaker.huggingface.model &hellip; "", ""title"": ""SageMaker complains that /opt/ml/model does not appear to have a file named config.json""}"

Unable to download Huggingface API Transformers,Stack Overflow,N/A,"{""tags"": [""python"", ""tensorflow"", ""pip"", ""huggingface-transformers"", ""safe-tensors""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 76910031, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1692294477, ""creation_date"": 1692150845, ""body"": ""After downloading tensorflow and PyTorch, and running , I get this error:\n\nHere is a screenshot of the whole thing:\n\nNot sure how to fix this. I tried upgrading pip and running this command as an answer to a similar post said to: , but the same error keeps showing up.\nDon&#39;t understand the issue and dont know how to fix it.\n"", ""excerpt"": ""After downloading tensorflow and PyTorch, and running pip install -q transformers, I get this error:\nERROR: Failed building wheel for <span class=\""highlight\"">safetensors</span>\nERROR: Could not build wheels for <span class=\""highlight\"">safetensors</span>, which is &hellip; "", ""title"": ""Unable to download Huggingface API Transformers""}"

Copy installations from Google colab to a standard environment,Stack Overflow,N/A,"{""tags"": [""python"", ""installation"", ""pip"", ""google-colaboratory"", ""python-wheel""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 76896205, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1692002402, ""creation_date"": 1691985786, ""body"": ""I&#39;ve tried using &#39;pip freeze&#39; in the google colab as is suggested here. But the installation failed. I&#39;ve tried commenting out the failing lines after each failure for a bit, here are some of them:\n1.\n\n\n\n\n\n\n\n\n\nAt which point I gave up on this process.\nI&#39;m guessing the first two problems are with simply being outside a google colab, and with not having cuda installed respectively.\nAnyhow, is there a better way to do this? Maybe a way to outright download the environment in the colab as a virtual environment?\nAdding my requirements.txt according to the request:\n\n"", ""excerpt"": ""==0.1.7.post0\nqudida==0.0.4\nreferencing==0.30.2\nregex==2023.6.3\nrequests==2.31.0\nrequests-oauthlib==1.3.1\nrequirements-parser==0.5.0\nrich==13.5.2\nrpds-py==0.9.2\nrpy2==3.4.2\nrsa==4.9\ns3transfer==0.3.7\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Copy installations from Google colab to a standard environment""}"

Segmentation fault when converting llama2 model to huggingface,Stack Overflow,N/A,"{""tags"": [""pytorch"", ""nlp"", ""huggingface-transformers""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 76759381, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1691825655, ""creation_date"": 1690258091, ""body"": ""I want to convert the llama2 7B model to the huggingface formate, but I keep getting a segmentation fault error.\n\nI&#39;m using this https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py script to convert the model to the huggingface format. Here the content for completeness:\n\n"", ""excerpt"": ""parser.add_argument(\n        &quot;--output_dir&quot;,\n        help=&quot;Location to write HF model and tokenizer&quot;,\n    )\n    parser.add_argument(&quot;--safe_serialization&quot;, type=bool, help=&quot;Whether or not to save using `<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Segmentation fault when converting llama2 model to huggingface""}"

Error in deploy LLM model in sagemaker endpoint. pls provide the solution any one known,Stack Overflow,N/A,"{""tags"": [""amazon-web-services"", ""amazon-sagemaker"", ""huggingface-transformers"", ""large-language-model"", ""safe-tensors""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 76802666, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1690821092, ""creation_date"": 1690798837, ""body"": ""\nwe are trying to deploy falcon  model in  endpoint by following code provided in   model deploy option.\nplease provide the solution for this issue..\n"", ""excerpt"": ""/site-packages/text_generation_server/utils/convert.py&quot;, line 62, in convert_file\n    save_file(pt_state, str(sf_file), metadata={\n    &quot;format&quot;: &quot;pt&quot;\n})\n  File &quot;/opt/conda/lib/python3.9/site-packages/<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Error in deploy LLM model in sagemaker endpoint. pls provide the solution any one known""}"

"Stable Diffusion, Error loading script: hook.py",Stack Overflow,N/A,"{""tags"": [""python"", ""runtime-error"", ""stable-diffusion""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 76801528, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1690789248, ""creation_date"": 1690788671, ""body"": ""Seemingly out of nowhere my Stable Diffusion launcher has gotten an issue where it can&#39;t load a script.\nThe program seems to work as intended aside from that so I&#39;m unsure of the impact, but I&#39;d rather get it fixed than it being a problem down the line.\nthe following is the full cmd log of what happens on launch:\n\nAs you can see, immediately upon launching the program the same way I have done before, I now get met with\n\nI have tried doing a fresh install of Python, yet this still occurs, I have even tried a fresh reinstall of the program itself, same problem.\nI want to know if anyone who has either used this program or who are proficient enough with Python could help me on how to proceed with repairing this issue?\nWhat I tried:\nI simply launched the program using the provided .bat file for initiating the script, same way as I have always done it.\nWhat I expected:\na clean cmd launch log without any error messages\nWhat happened:\nI was met with an error message that wasn&#39;t there before.\n"", ""excerpt"": ""downloads\n2023-07-31 09:05:07,996 - ControlNet - INFO - ControlNet v1.1.234\nLoading weights [445ffb5195] from C:\\Stable Diffusion\\stable-diffusion-webui\\models\\Stable-diffusion\\epicrealism_pureEvolutionV4.<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""Stable Diffusion, Error loading script: hook.py""}"

libsentencepiece.so.0: cannot open shared object file: No such file or directory when creating BERTopic model,Stack Overflow,N/A,"{""tags"": [""conda"", ""bert-language-model"", ""sentencepiece""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 76690132, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1689932591, ""creation_date"": 1689357274, ""body"": ""I am trying to train a  Model in python. However, I get this error:\n\nI have tried to uninstall and reinstall , install , and set the  variable to the path to . None of these things have worked. Does anyone know why this is? If it matters, I am running this script on a remote server. I think perhaps it may have something to do with conflicts with the dependencies, but I am unsure.\nPython Code\n\nFull Traceback\n\nDependencies\n\n"", ""excerpt"": ""0.8.10           py38h0cc4f7c_0    conda-forge\ns2n                       1.3.46               h06160fa_0    conda-forge\nsacremoses                0.0.53             pyhd8ed1ab_0    conda-forge\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""libsentencepiece.so.0: cannot open shared object file: No such file or directory when creating BERTopic model""}"

Model.generate stop code execution without any error,Stack Overflow,N/A,"{""tags"": [""huggingface-transformers"", ""huggingface""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 76338717, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1685088075, ""creation_date"": 1685087373, ""body"": ""I am pretty new to HF, this is my first attempt to use a model. The problem is  kinda abrupt the script execution without any error. Here’s my code:\n\nThis code gives me no output except “before”. Also, I’ve tried other models with the same result. It looks the issue on my side… I’ll be very grateful for your help. Thanks!\nEnv:\n\nLogs:\n\n"", ""excerpt"": ""Env:\n- `transformers` version: 4.30.0.dev0\n- Platform: macOS-10.15.7-x86_64-i386-64bit\n- Python version: 3.9.16\n- Huggingface_hub version: 0.14.1\n- <span class=\""highlight\"">Safetensors</span> version: 0.3.1\n- PyTorch version (GPU?) &hellip; "", ""title"": ""Model.generate stop code execution without any error""}"

Stable Diffusion Webui ConnectTimeoutError while starting,Stack Overflow,N/A,"{""tags"": [""python"", ""huggingface"", ""stable-diffusion""], ""question_score"": 2, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": false, ""question_id"": 76229875, ""item_type"": ""question"", ""score"": 2, ""last_activity_date"": 1683827157, ""creation_date"": 1683823353, ""body"": ""I&#39;m trying to set up stable diffusion on a server so that users can access it via RDP and generate what they need. The server in place I hosted on-premise and doesn&#39;t have any internet connection. I installed all needed libs via whl files. I was able to set up everything fine for my user, meaning when I run the webui-user.bat everything works fine.\nAs soon as a different user connects to the server and runs the webui-user.bat that startup runs till a certain point and stops with ConnectionTimeoutError. Here is the console output from the part where it fails.\n\nI ensured Python is installed and set up globaly on the server not only for my user. I don&#39;t get why the problem seems to be user-specific.\nWhat I&#39;ve tried so far is:\n\nDowngrading the request lib to 1.27.1\nusing the offline version of stable diffusion web ui https://github.com/HotChocut/stable-diffusion-webui-offline-launch\n\n"", ""excerpt"": ""Loading weights [aba96b389d] from C:\\AutomaticStableDiff\\stable-diffusion-webui\\models\\Stable-diffusion\\mdjrny-v4.<span class=\""highlight\"">safetensors</span>\nCreating model from config: C:\\AutomaticStableDiff\\stable-diffusion-webui\\configs &hellip; "", ""title"": ""Stable Diffusion Webui ConnectTimeoutError while starting""}"

NVMe SSD read speed drops after multiple GBs read,Stack Overflow,N/A,"{""tags"": [""c++"", ""linux"", ""performance"", ""file"", ""nvme""], ""question_score"": 4, ""is_accepted"": false, ""answer_id"": 75784236, ""is_answered"": false, ""question_id"": 75776897, ""item_type"": ""answer"", ""score"": 3, ""last_activity_date"": 1679251555, ""creation_date"": 1679251555, ""body"": ""This is a partial answer showing a similar (smaller) effect and several hints to track the problem.\nI was not able to fully reproduce this issue on my NVMe SSE Samsung 980 Pro (1 TB), but I discovered an interesting similar effect that might explain the behavior of your SSE. Here is the results on my machine for the first ~30 GiB files:\n\nWhat we can see is that the speed seems to decrease over time. This effect is consistent over multiple runs. However, the speed of the first file is still ~3.3 GB/s even just after another run. This means my SSD is likely not responsible for the decrease in performance (otherwise, the speed is expected to decrease over subsequent runs). Note that data is not cached (check with  and lower-level profiling tools).\nIf did not download more data as I was running out of space (~4 GiB left) on this partition and this is actually an important point : the slowest files are the one I downloaded last! My first hypothesis was that the file system is fragmented when there is not a lot of remaining space so the last file as split in more small chunks than the first. If so, this causes the operating system (OS) to do more IO requests of small blocks than for the first files, resulting in a lower throughput.\nTo check that, I just removed the last file to make some space and copied the second file () and here is the result:\n\nAs we can see, the speed is consistent with the previous run for all the files except for the copied file. Note that the file system on the partition is Ext4. Other file systems can result in different behavior, especially FAT which tends to get quickly fragmented.\nThat being said, I tried to use the tool  to defragment the files and it did not significantly improve the situation. In fact, all files except the first (fastest) has been reported as files that do &quot;not need defragmentation&quot;. A report can be shown using . Copying files is also a simple way to automatically defragment them.\nI then tested again the same program on the same Linux system on a larger NTFS partition (800 GiB) and did not reproduced the effect. I downloaded more downloaded files and made many copy of the last file so not to wait for a while downloading it. The resulting directory toke 229 GiB. Results on an NTFS partition are actually more stable and surprisingly even better than the Ext4 partition. I run this 3 time to be sure this is reproducible (and it was). Here are the result of the last run:\n\nI advise you to try storing the files on a different partition with another file system so to check if the performance gap is coming from that. I expect the culprit to be the SSD driver, the firmware or the hardware itself.\nSo far, the SLC cache of my SSD did not impact the read timings, only writes, but it is not clear wether reads can be impacted by the SLC cache. The behaviour can be pretty different from one SSD to another. On my SSD, the firmware stores data in the SLC cache and do not move directly data to the TLC blocks. I guess this is for improving the life of the SSD by avoiding storing data systematically to TLC while they could be removed just later (TLC cells have a limited number of write which tends to be lower than the one of SLC cells, even on dynamic pseudo-SLC cache like on your SSD, which uses TLC cells to make an SLC cache). When the cache is closed to be saturated, it apparently start to move the most of the SLC cache content to TLC blocks. I guess this to be able to better sustain the next write bursts and delay the throughput switch due to the SLC-&gt;TLC block copy.\nNote that a good firmware should keep relagularly modified file blocks in the SLC cache so to reduce the wear of the SSD. This means a way to check the speed of the SLC vs TLC blocks is to hammer write a file and then measure the time to read it compared to a file writen once. However, this strategy is dangerous since it can significantly reduce the life of the SSD if the firmware does not actually store the hammer file content in the SLC cache. Because of this, I did not tried this on my SSD.\nNote that once a set of SLC/TLC pages belonging to a file is written, there is no reason for the read speed to change (except if the request are done less contiguously -- which is unexpected for a file left untouched and no other write done on the target SSD -- or if the firmware decide to surprisingly move the read pages -- which is possible when other writes are done meanwhile for example due to the wear levelling). Consequently, it would be interesting to read the files in a different order so to check whether the bad performance results are tied to specific files or just dependent of the time/heat. To check whether the heat is a problem, you can try to intensively use the SSD before running the program and check if this impact the performance of the benchmark. Note that 75&#176;C is pretty high for this SSD (it is advised not to exceed 70&#176;C). Besides this (and the impact of the FS), I am running out of ideas. I hope this helps.\n"", ""excerpt"": ""To check that, I just removed the last file to make some space and copied the second file (model_00002-of-00072.<span class=\""highlight\"">safetensors</span>) and here is the result:\n&quot;/tmp/bigfiles/model_00005-of-00072.<span class=\""highlight\"">safetensors</span>&quot;\n4932875509 &hellip; &quot;/tmp/bigfiles/model_00002-of-00072.<span class=\""highlight\"">safetensors</span>&quot;\n4932875549 bytes in 1.8347 seconds: 2.68866 billion bytes per second.   &lt;----------\n&quot;/tmp/bigfiles/model_00004-of-00072.<span class=\""highlight\"">safetensors</span>&quot;\n4932875557 bytes in &hellip; "", ""title"": ""NVMe SSD read speed drops after multiple GBs read""}"

How to pass a variable by value to another thread?,Stack Overflow,N/A,"{""tags"": [""multithreading"", ""rust"", ""closures"", ""borrow-checker"", ""pass-by-value""], ""question_score"": 0, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 1, ""is_answered"": true, ""question_id"": 75764911, ""item_type"": ""question"", ""score"": 0, ""last_activity_date"": 1679045096, ""creation_date"": 1679039079, ""body"": ""I need to launch multiple threads in Rust, where each thread receives its worker id (integer between 0 and 31 inclusively). How to do it in Rust?\nHere&#39;s what I have tried:\n\nThis gives a compilation error:\n\nI moved the other variables outside the scope, so that Rust knows that references to them will outlive the threads. However, what can I do with  variable?\n"", ""excerpt"": ""error: could not compile `<span class=\""highlight\"">safetensors</span>-python` due to previous error\n\nI moved the other variables outside the scope, so that Rust knows that references to them will outlive the threads. &hellip; "", ""title"": ""How to pass a variable by value to another thread?""}"

How to manually perform inference in BLOOM? (with manually loaded BLOOM blocks to VRAM),Stack Overflow,N/A,"{""tags"": [""python"", ""nlp"", ""artificial-intelligence"", ""huggingface-transformers"", ""bloom""], ""question_score"": 1, ""is_accepted"": false, ""has_accepted_answer"": false, ""answer_count"": 0, ""is_answered"": false, ""question_id"": 75436031, ""item_type"": ""question"", ""score"": 1, ""last_activity_date"": 1676292160, ""creation_date"": 1676292160, ""body"": ""Problem Introduction\nHello, I am a member of a project which partial goal is to distribute BLOOM model between several computers. The communication part between computers is not a problem for us, but we don&#39;t know how to manually perform inference. If we could know that, then we could create sort of a &quot;chain&quot; of computers, which hopefully will accelerate whole computation.\nIt is important to us to manually perform inference because if we&#39;ll succeed we&#39;ll start optimizing process of chain links selection based on graphics cards used in host PC.\nAssumtions for simplicity (will be changed in the future versions):\n\nPrompts coming to a distributed architecture are processed in series.\nAmount of computers and their parameters are not an issue.\nGPUs of all computers are the same.\nAll computers have one GPU.\nThere will be one BLOOM block per GPU.\n\n\nTechnology related part\nComputer specification:\n\nOS: Windows 10 Home\nProcessor: Intel i7-11700K\nPC RAM: 32GB\nGraphic Card: NVIDIA RTX 3060 12GB VRAM\n\nInstalled packages (pip freeze):\n\nRepository arrangement (as simple as it can be):\n\nCode (main.py):\n\nCode output:\n\nCode description:\nInitially I was working with the AutoTokenizer and AutoModelForCausalLM pipeline from the code example on bigscience/bloom code repository. I hoped for some kind of easy way to perform inference only on one block at the time. I didn&#39;t find one.\nThen I used debugger to track route of function calls to examine MVP of manual inference. I realised how complicated is AutoModelForCausalLM class in practice. By tinkering with code in jupyter notebooks and debugger I managed to define code above.\nIf somebody could help with definig manual inference I will be glad. Thank you in advance for your help.\n"", ""excerpt"": ""platformdirs==2.6.2\nprompt-toolkit==3.0.36\npsutil==5.9.4\npure-eval==0.2.2\nPygments==2.14.0\npython-dateutil==2.8.2\npytz==2022.7.1\npywin32==305\nPyYAML==6.0\npyzmq==25.0.0\nregex==2022.10.31\nrequests==2.28.2\n<span class=\""highlight\"">safetensors</span> &hellip; "", ""title"": ""How to manually perform inference in BLOOM? (with manually loaded BLOOM blocks to VRAM)""}"

